{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Adaboost [classifier] from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation of adaboost performs even better in terms of accuracy when compared with sklearn implementaion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "df = df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.8, random_state=None):\n",
    "    train_df = df.sample(frac=test_size, random_state=random_state)\n",
    "    test_df = df[~df.index.isin(train_df.index)]\n",
    "    return train_df.sort_index(), test_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_classification(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_regression(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.sum((y_true - y_pred)**2) / len(y_true)) # RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DecisionStump()\n",
    "- BaseBoostingAlgorithm()\n",
    "- AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Feature/Attribute Index to consider for splitting\n",
    "        self.decision_feature_index = None\n",
    "        # Exact value from Feature/Attribute to split on\n",
    "        self.decision_threshold_value = None\n",
    "        # Stump importance / weight\n",
    "        self.weight = None\n",
    "        # Stump error\n",
    "        self.error = None\n",
    "        # Left leaf value\n",
    "        self.left_leaf_value = None\n",
    "        # Right leaf value\n",
    "        self.right_leaf_value = None\n",
    "        # Stump decision compartor \n",
    "        self.decision_comparator = None\n",
    "        print(\"New Stump Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBoostingAlgorithm():\n",
    "    def __init__(self, n_learners):\n",
    "        self.n_learners = n_learners\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store all weak learners (Weak learner -> A decsion stump)\n",
    "        self.learners = []\n",
    "        # Identify each feature type in input X and store as list\n",
    "        self.feature_types = self._determine_type_of_feature(X)\n",
    "        # Concatenate input and output\n",
    "        self.data = np.concatenate((X, np.expand_dims(y, axis=1)), axis=1)\n",
    "        # Initialize weight for each example as 1/N (where N -> total number of examples)\n",
    "        self.sample_weight = np.full(len(self.data), np.divide(1, len(self.data)))        \n",
    "        print(self.feature_types)\n",
    "        print(self.ml_task)\n",
    "        \n",
    "        # Iterate and build learners\n",
    "        for i_boost in range(self.n_learners):\n",
    "            # Instantiate a new decision stump object\n",
    "            learner = DecisionStump()\n",
    "            # Find and Perform split over best feature \n",
    "            potential_splits = self._get_potential_splits(self.data)\n",
    "            \n",
    "            split_column_index, split_value, metric = self._determine_best_split(self.data, self.sample_weight, potential_splits, self.ml_task)\n",
    "            left_node_data, right_node_data = self._split_data(self.data, self.sample_weight, split_column_index, split_value)\n",
    "            print(f'split_column_index: {split_column_index}, split_value: {split_value}')\n",
    "            print(f'Change in overall_metric: {metric}')\n",
    "            # Compute Leaf values\n",
    "            left_leaf_value = self._create_leaf(left_node_data, self.ml_task)\n",
    "            right_leaf_value = self._create_leaf(right_node_data, self.ml_task)\n",
    "            print(f'Left leaf: {left_leaf_value}, Right leaf: {right_leaf_value}')\n",
    "\n",
    "            # Allocate the instantiated learner with our computed values\n",
    "            learner.decision_feature_index = split_column_index\n",
    "            learner.decision_threshold_value = split_value\n",
    "            learner.left_leaf_value = left_leaf_value\n",
    "            learner.right_leaf_value = right_leaf_value\n",
    "            learner.decision_comparator = self.feature_types[split_column_index]\n",
    "            \n",
    "            # Boosting step\n",
    "            self.sample_weight, learner = self.boost(i_boost,\n",
    "                                              self.data,\n",
    "                                              self.sample_weight,\n",
    "                                              learner)\n",
    "            # Early Termination\n",
    "            if self.sample_weight is None:\n",
    "                break\n",
    "            # Stop boosting since error is 0\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if learner.error == 0 or np.sum(self.sample_weight) <= 0:\n",
    "                self.learners.append(learner)\n",
    "                break\n",
    "            print(f'{i_boost}: Sample weight(sum) [Raw] {np.sum(self.sample_weight)}')\n",
    "            # Dont perform operations in below conditional block if we are on final learner\n",
    "            if not i_boost == self.n_learners - 1:\n",
    "                # Normalize\n",
    "                self.sample_weight /= np.sum(self.sample_weight)\n",
    "                \n",
    "                # Note(Alternative): Sample data(examples) based on sample_weight\n",
    "                # Construct new data set sample based on sample_weight\n",
    "#                 self.data = self._sample_data_by_weights(self.data, self.sample_weight)\n",
    "                # Reinitialize equal sample weights for the new data\n",
    "#                 self.sample_weight = np.full(len(self.data), (1 / len(self.data)))\n",
    "            print(f'{i_boost}: Sample weight(sum) [Normalized] {np.sum(self.sample_weight)}')\n",
    "            # Add this learner to our main list of learners\n",
    "            self.learners.append(learner)\n",
    "            print(f'Total stumps: {len(self.learners)}')\n",
    "            \n",
    "        return self  \n",
    "            \n",
    "    def stump_predict(self, data, learner):\n",
    "        \"\"\"\n",
    "        Computes prediction for the passed data examples w.r.t to the learner(descision stump) \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        feature_column = data[:, learner.decision_feature_index]\n",
    "        for value in feature_column:\n",
    "            if learner.decision_comparator == 'categorical':\n",
    "                if value == learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            else: # continuous\n",
    "                if value <= learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def _gini_sk(self, data):\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        label_column = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        _, value_indexes, counts = np.unique(label_column, return_counts=True, return_index=True)\n",
    "\n",
    "        class_weights = np.array([np.take(data_sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in value_indexes])\n",
    "        \n",
    "        cw = np.sum(class_weights**2)\n",
    "        wn = np.sum(data_sample_weight)**2\n",
    "        gini = 1.0 - (cw/wn)\n",
    "        #print(f\"cw: {cw}, wn: {wn} ---> 1.0 - (cw/wn) == {1.0} - {cw}/{wn}\")\n",
    "        return gini\n",
    "    \n",
    "    def _calculate_weighted_mse(self, data):\n",
    "        \"\"\"\n",
    "        Calculate weighted mean squared error\n",
    "        \"\"\"\n",
    "        actual_values = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        \n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "\n",
    "        else:\n",
    "            prediction = np.mean(actual_values)\n",
    "            # Not normalizing using sum of weighted mean, beacuse the sum of weighted mean is 1\n",
    "            weighted_mse = np.mean((data_sample_weight * (actual_values - prediction))**2)\n",
    "\n",
    "        return weighted_mse\n",
    "    \n",
    "    \n",
    "    def _calculate_weighted_overall_metric(self, data, left_node_data, right_node_data, metric_function):\n",
    "        \"\"\"\n",
    "        Generalized impurity metric, computes weighted overall\n",
    "        impurity/error w.r.t left and right nodes\n",
    "        \"\"\"\n",
    "        # Labels\n",
    "        left_label_column = left_node_data[:, -2]\n",
    "        right_label_column = right_node_data[:, -2]\n",
    "        parent_label_column = data[:, -2]\n",
    "        # Sample weights\n",
    "        left_sample_weight = left_node_data[:, -1]\n",
    "        right_sample_weight = right_node_data[:, -1]\n",
    "        parent_sample_weight = data[:, -1]\n",
    "        \n",
    "        if self.ml_task == 'classification':\n",
    "            _, left_value_indexes, left_counts = np.unique(left_label_column, return_counts=True, return_index=True)\n",
    "            _, right_value_indexes, right_counts = np.unique(right_label_column, return_counts=True, return_index=True)\n",
    "            \n",
    "            aggregated_left_class_weights = np.array([np.take(left_sample_weight, np.where(left_label_column == left_label_column[value_index])[0]).sum() for value_index in left_value_indexes])\n",
    "            aggregated_right_class_weights = np.array([np.take(right_sample_weight, np.where(right_label_column == right_label_column[value_index])[0]).sum() for value_index in right_value_indexes])\n",
    "            \n",
    "            weighted_prob_node_left = np.sum(aggregated_left_class_weights)\n",
    "            weighted_prob_node_right = np.sum(aggregated_right_class_weights)\n",
    "            \n",
    "        else:\n",
    "            total_parent_sample_weight = np.sum(np.sum(left_sample_weight), np.sum(right_sample_weight))\n",
    "            # Weighted probabilities of left and right node\n",
    "            weighted_prob_node_left = np.sum(left_sample_weight) / total_parent_sample_weight\n",
    "            weighted_prob_node_right = np.sum(right_sample_weight) / total_parent_sample_weight\n",
    "        \n",
    "        left_impurity = metric_function(left_node_data)\n",
    "        right_impurity = metric_function(right_node_data)\n",
    "\n",
    "        if left_impurity != None and right_impurity != None:\n",
    "            overall_metric =  weighted_prob_node_left * left_impurity + weighted_prob_node_right * right_impurity\n",
    "            \n",
    "            #print(f'weighted_prob_node_left * w_i(left_node_data)): {weighted_prob_node_left} * {left_impurity} = {(weighted_prob_node_left * left_impurity)}')\n",
    "            #print(f'weighted_prob_node_right * w_i(right_node_data)): {weighted_prob_node_right} * {right_impurity} = {(weighted_prob_node_right * right_impurity)}')\n",
    "            return overall_metric\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Alternative method to weighted loss: Which works by random sampling of example from a distribution based on sample_weight\n",
    "    def _sample_data_by_weights(self, data, sample_weight):\n",
    "        \"\"\"\n",
    "        Construct an new input, iteratively sampled over distribution \n",
    "        formed by passed sample_weight.\n",
    "\n",
    "        Note: \n",
    "        Learn more about this technique: https://youtu.be/LsK-xG1cLYA (Statquest)\n",
    "        \"\"\"\n",
    "        n_samples, _ = np.shape(data)\n",
    "        # Intialize array to hold sampled index  \n",
    "        sampled_indices = []\n",
    "        # Perform cumulative summation over sample_weight to create buckets\n",
    "        sample_weight_buckets = np.cumsum(sample_weight)\n",
    "        # Keeping sampling 'n_samples' times\n",
    "        for _ in range(n_samples):\n",
    "            # Generate a random number between 0 and 1\n",
    "            random_num = np.random.random_sample()\n",
    "            # Find the bucket Eg: weight buckets [0.33, 0.66, 0.99] and random number = 0.47\n",
    "            # then index 1 will be selected (since cumsum value is 0.66)\n",
    "            bucket_index = np.where(sample_weight_buckets > random_num)[0][0]\n",
    "\n",
    "            sampled_indices.append(bucket_index)\n",
    "        # finally construct weighted data using sampled_indexes\n",
    "        weighted_data = data[sampled_indices]\n",
    "\n",
    "        return weighted_data\n",
    "\n",
    "\n",
    "    def _get_potential_splits(self, data):\n",
    "        \"\"\"\n",
    "        Get all potential splits for each feature\n",
    "        Splits can be made on each unique value\n",
    "        Can essentially make a split at each unique value\n",
    "        \n",
    "        \"\"\"\n",
    "        potential_splits = {}\n",
    "        _, n_columns = data.shape\n",
    "        for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            potential_splits[column_index] = unique_values\n",
    "            \n",
    "        return potential_splits\n",
    "    \n",
    "    \n",
    "    def _determine_best_split(self, data, sample_weight, potential_splits, ml_task):\n",
    "        \"\"\"\n",
    "        Iterate over each column_index (as keys) in potential_split (dict)\n",
    "        Perform split(of examples) over each unique value and evaluate the split\n",
    "        Identify the best split and return its feature index and value\n",
    "        \"\"\"\n",
    "        # Stitch data with sample_weight towards the end\n",
    "        data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        \n",
    "        # Best minimum gini index to be updated iteratively\n",
    "        best_overall_metric = float('inf')\n",
    "        \n",
    "        for column_index in potential_splits:\n",
    "            #print(f\"COLUMN {column_index}\")\n",
    "            for value in potential_splits[column_index]:\n",
    "                #print(f'column_index: {column_index}, value: {value}')\n",
    "                left_node_data, right_node_data = self._split_data(data, None, split_column_index=column_index, split_value=value)\n",
    "\n",
    "                if ml_task == \"regression\":\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._calculate_weighted_mse)\n",
    "                else: # classification\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._gini_sk)\n",
    "\n",
    "                # If a lower overall_metric is achieved update the index and value with the current\n",
    "                if current_overall_metric != None and current_overall_metric <= best_overall_metric:\n",
    "                    best_overall_metric = current_overall_metric\n",
    "                    best_split_column_index = column_index\n",
    "                    best_split_value = value\n",
    "                #print(f'best_overall_metric: {best_overall_metric}')\n",
    "                #print('---')\n",
    "            #print(f'Debug [1]: Best: {best_overall_metric}, index: {best_split_column_index}, value: {best_split_value}')\n",
    "        return best_split_column_index, best_split_value, best_overall_metric\n",
    "    \n",
    "    \n",
    "    def _split_data(self, data, sample_weight, split_column_index, split_value):\n",
    "        \"\"\" \n",
    "        Split data(examples) based on best split_column_index and split_value\n",
    "        estimated using task specific splitting metric.\n",
    "        \"\"\"\n",
    "        # Stich sample_weight to data if passed as an argument or the assumption is, it has already been stiched/appended(in _determine_best_split())\n",
    "        if sample_weight is not None:\n",
    "            # Stitch data with sample_weight towards the end (axis=1)\n",
    "            data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        #else it is already appended \n",
    "        \n",
    "        # Get values(from feature column) for the passed split_column index\n",
    "        split_column_values = data[:, split_column_index]\n",
    "        \n",
    "        type_of_feature = self.feature_types[split_column_index]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            left_node_data = data[split_column_values <= split_value]\n",
    "            right_node_data = data[split_column_values >  split_value]\n",
    "\n",
    "        # feature is categorical   \n",
    "        else:\n",
    "            left_node_data = data[split_column_values == split_value]\n",
    "            right_node_data = data[split_column_values != split_value]\n",
    "        return left_node_data, right_node_data\n",
    "    \n",
    "    \n",
    "    def _create_leaf(self, data, ml_task):\n",
    "        \"\"\"\n",
    "        Create leaf node, with leaf value based on ml_task\n",
    "        for,\n",
    "        Classfication: consider majority vote\n",
    "        Regression: consider the mean value\n",
    "        \"\"\"\n",
    "        label_column = data[:, -2]\n",
    "        sample_weight = data[:, -1]\n",
    "        \n",
    "        if ml_task == \"regression\":\n",
    "            leaf = np.mean(label_column)\n",
    "\n",
    "        # classfication    \n",
    "        else:  \n",
    "            # Decide leaf value based on sum of weights for each class in the node\n",
    "            unique_classes, unique_cls_start_indices, counts_unique_classes = np.unique(label_column, return_counts=True, return_index=True)\n",
    "            unique_class_aggregated_weights = np.array([np.take(sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in unique_cls_start_indices])\n",
    "            index = unique_class_aggregated_weights.argmax()\n",
    "            leaf = unique_classes[index]\n",
    "\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    def _determine_type_of_feature(self, X):\n",
    "        \"\"\"\n",
    "        Determine, if the feature is categorical or continuous\n",
    "        \"\"\"\n",
    "        feature_types = []\n",
    "        n_unique_values_treshold = 15 # Threshold for a numeric feature to be categorical\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        for feature_i in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature_i])\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "\n",
    "        return feature_types\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier(BaseBoostingAlgorithm):\n",
    "    def __init__(self, n_learners=20):\n",
    "        # Set total number of weak learners\n",
    "        super().__init__(n_learners)\n",
    "        self.ml_task = \"classification\"\n",
    "        self.classes = None\n",
    "        self.n_classes = None\n",
    "        \n",
    "    def boost(self, i_boost, data, sample_weight, learner):\n",
    "        \"\"\"\n",
    "        Compute learner importance and error, along with boosted weights for each example \n",
    "        \"\"\"\n",
    "        print(f'Boost Called')\n",
    "        \n",
    "        # If its first boost initialize number of classes(n_classes)\n",
    "        if i_boost == 0:\n",
    "            self.classes = np.unique(data[:, -1])\n",
    "            self.n_classes = self.classes.size\n",
    "            \n",
    "        # Perform predictions\n",
    "        preds = self.stump_predict(data, learner)\n",
    "        \n",
    "        # Incorrectly classified examples\n",
    "        incorrect = preds != data[:, -1]\n",
    "\n",
    "        # Learner Error\n",
    "        learner_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        # Stop if classification is perfect\n",
    "        if learner_error <= 0:\n",
    "            learner.weight = 1\n",
    "            learner.error = 0\n",
    "            return sample_weight, learner\n",
    "        print(f'Learner error: {learner_error}')\n",
    "        \n",
    "        # Learner weight\n",
    "        learner_weight = (np.log((1. - learner_error) / (learner_error)) +\n",
    "                        np.log(self.n_classes - 1.))\n",
    "        print(f'Learner weight: {learner_weight}')\n",
    "        \n",
    "        # Boost sample_weight for each each sample\n",
    "        # Dont boost sample_weight if we are on final learner\n",
    "        if not i_boost == self.n_learners - 1:\n",
    "            # Boost only positive weights\n",
    "            sample_weight *= np.exp(learner_weight * incorrect *\n",
    "                                    ((sample_weight > 0) | (learner_weight < 0)))\n",
    "\n",
    "        # Allocate learner its computed weight and error\n",
    "        learner.weight = learner_weight\n",
    "        learner.error = learner_error\n",
    "\n",
    "        # Finally return sample weights and boosted learner\n",
    "        return sample_weight, learner\n",
    "         \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for X.\n",
    "        \"\"\"\n",
    "        print(f'Predict called')\n",
    "        \n",
    "        # Get activated matrix for with respect to each learner [get vote of each learner]\n",
    "        # Add each activated matrix (matrix addition) [get overall vote of all leaners]\n",
    "        # return the overall matrix\n",
    "        # Argmax is used over each row of overall matrix to figure our the class\n",
    "        classes = self.classes[:, np.newaxis]\n",
    "        pred = sum((self.stump_predict(X, learner) == classes).T * learner.weight\n",
    "                   for learner in self.learners)\n",
    "        # Normalize \n",
    "        learner_weights = sum(learner.weight for learner in self.learners)\n",
    "        pred /= learner_weights\n",
    "        \n",
    "        # If its binary classification obatin the form [-, +], convienient to select classes with np.take() \n",
    "        # Eg(binary): classes =  [[c1], [c2]] and pred = [True, False, True], below output: [[c2], [c1], [c2]]\n",
    "        if self.n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return classes.take(pred > 0, axis=0)\n",
    "        # Finds index of column with max value, and uses this index to select class from classes\n",
    "\n",
    "        return classes.take(np.argmax(pred, axis=1), axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['continuous', 'continuous', 'continuous', 'continuous']\n",
      "classification\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.33333333333333404\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.3333333333333332\n",
      "Learner weight: 1.386294361119891\n",
      "0: Sample weight(sum) [Raw] 2.0\n",
      "0: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 1\n",
      "New Stump Created!\n",
      "split_column_index: 2, split_value: 4.7\n",
      "Change in overall_metric: 0.21404997642621418\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.18000000000000002\n",
      "Learner weight: 2.2094946699280333\n",
      "1: Sample weight(sum) [Raw] 2.4600000000000004\n",
      "1: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 2\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 1.6\n",
      "Change in overall_metric: 0.20526378968831266\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.11412225233363443\n",
      "Learner weight: 2.742455876638902\n",
      "2: Sample weight(sum) [Raw] 2.657633242999097\n",
      "2: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 3\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.2880802448962008\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.23700484356904267\n",
      "Learner weight: 1.8623182858387413\n",
      "3: Sample weight(sum) [Raw] 2.2889854692928724\n",
      "3: Sample weight(sum) [Normalized] 0.9999999999999996\n",
      "Total stumps: 4\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.2586206170546941\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.1604277516136026\n",
      "Learner weight: 2.3481960190707705\n",
      "4: Sample weight(sum) [Raw] 2.5187167451591916\n",
      "4: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 5\n",
      "New Stump Created!\n",
      "split_column_index: 2, split_value: 5.0\n",
      "Change in overall_metric: 0.26038936405161456\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.14913685791266568\n",
      "Learner weight: 2.43453408223676\n",
      "5: Sample weight(sum) [Raw] 2.5525894262620024\n",
      "5: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 6\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.33906099956051833\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.2955678163250568\n",
      "Learner weight: 1.5616409382400107\n",
      "6: Sample weight(sum) [Raw] 2.1132965510248294\n",
      "6: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 7\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.29344323143123613\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.18812451488510812\n",
      "Learner weight: 2.1553901086927425\n",
      "7: Sample weight(sum) [Raw] 2.4356264553446767\n",
      "7: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 8\n",
      "New Stump Created!\n",
      "split_column_index: 2, split_value: 4.4\n",
      "Change in overall_metric: 0.3609285531634814\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.24460463933167675\n",
      "Learner weight: 1.8207452591756645\n",
      "8: Sample weight(sum) [Raw] 2.2661860820049697\n",
      "8: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 9\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.3816953472246771\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.2941800198847062\n",
      "Learner weight: 1.5683155073815593\n",
      "9: Sample weight(sum) [Raw] 2.117459940345882\n",
      "9: Sample weight(sum) [Normalized] 0.9999999999999999\n",
      "Total stumps: 10\n",
      "New Stump Created!\n",
      "split_column_index: 1, split_value: 2.6\n",
      "Change in overall_metric: 0.3411101023212601\n",
      "Left leaf: Iris-virginica, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.27922896298769534\n",
      "Learner weight: 1.6414366031839984\n",
      "10: Sample weight(sum) [Raw] 2.162313111036914\n",
      "10: Sample weight(sum) [Normalized] 0.9999999999999998\n",
      "Total stumps: 11\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.2461860504878137\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.15101356499178484\n",
      "Learner weight: 2.4198207219698564\n",
      "11: Sample weight(sum) [Raw] 2.5469593050246457\n",
      "11: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 12\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.3755464954957733\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.26139880027561585\n",
      "Learner weight: 1.7318580957309724\n",
      "12: Sample weight(sum) [Raw] 2.215803599173152\n",
      "12: Sample weight(sum) [Normalized] 0.9999999999999998\n",
      "Total stumps: 13\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 1.4\n",
      "Change in overall_metric: 0.39580268274700614\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.26800433681827185\n",
      "Learner weight: 1.697918607350923\n",
      "13: Sample weight(sum) [Raw] 2.195986989545184\n",
      "13: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 14\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.42045153637334615\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.3254443432442076\n",
      "Learner weight: 1.4220099098478411\n",
      "14: Sample weight(sum) [Raw] 2.023666970267377\n",
      "14: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 15\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.40749843045260414\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.2934277153048024\n",
      "Learner weight: 1.5719413674719886\n",
      "15: Sample weight(sum) [Raw] 2.119716854085593\n",
      "15: Sample weight(sum) [Normalized] 1.0000000000000002\n",
      "Total stumps: 16\n",
      "New Stump Created!\n",
      "split_column_index: 2, split_value: 5.1\n",
      "Change in overall_metric: 0.3567549691331884\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.2351985535243699\n",
      "Learner weight: 1.8723333675834857\n",
      "16: Sample weight(sum) [Raw] 2.2944043394268903\n",
      "16: Sample weight(sum) [Normalized] 0.9999999999999998\n",
      "Total stumps: 17\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.4027532533478423\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.29056197951281365\n",
      "Learner weight: 1.5858034085967052\n",
      "17: Sample weight(sum) [Raw] 2.128314061461558\n",
      "17: Sample weight(sum) [Normalized] 1.0000000000000004\n",
      "Total stumps: 18\n",
      "New Stump Created!\n",
      "split_column_index: 1, split_value: 2.8\n",
      "Change in overall_metric: 0.4070819579422973\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.33333333333333326\n",
      "Learner weight: 1.386294361119891\n",
      "18: Sample weight(sum) [Raw] 2.0000000000000013\n",
      "18: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 19\n",
      "New Stump Created!\n",
      "split_column_index: 1, split_value: 3.0\n",
      "Change in overall_metric: 0.4289200990045776\n",
      "Left leaf: Iris-virginica, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.2964586270949753\n",
      "Learner weight: 1.5573661944055233\n",
      "19: Sample weight(sum) [Raw] 1.0\n",
      "19: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 20\n"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(df.iloc[:, :-1].values, df.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict called\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_classification(df.iloc[:, -1].values, predictions[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = np.array([4, 1, 1, 2, 3, 2, 3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = 1/len(tff)\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tww = np.array([tw]*len(tff))\n",
    "tww1 = np.array([0.1, 0.1, 0.1, 0.25, 0.04, 0.25, 0.04, 0.04, 0.04, 0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tww1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.1\n",
      "1 0.1\n",
      "1 0.1\n",
      "2 0.25\n",
      "3 0.04\n",
      "2 0.25\n",
      "3 0.04\n",
      "3 0.04\n",
      "3 0.04\n",
      "3 0.04\n"
     ]
    }
   ],
   "source": [
    "for tz1, tzw2 in zip(tff, tww1):\n",
    "    print(tz1, tzw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unq_clses, counts_unq_clses = np.unique(tff, return_counts=True)\n",
    "# idx = counts_unq_clses.argmax()\n",
    "# lf = unq_clses[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.5 0.2 0.1]\n"
     ]
    }
   ],
   "source": [
    "unq_clses, unq_start_indices, counts_unq_clses = np.unique(tff, return_counts=True, return_index=True)\n",
    "# Sum their weights \n",
    "cls_weights = np.array([np.take(tww1, np.where(tff == tff[value_index])[0]).sum() for value_index in unq_start_indices])\n",
    "print(cls_weights)\n",
    "idx = cls_weights.argmax()\n",
    "lf = unq_clses[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_clses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_unq_clses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
