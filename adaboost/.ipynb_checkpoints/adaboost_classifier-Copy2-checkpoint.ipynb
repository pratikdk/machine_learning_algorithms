{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Adaboost [Regressor] from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My implementation of adaboost performs even better in terms of accuracy when compared with sklearn implementaion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data [Regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/marklvl/bike-sharing-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = pd.read_csv(\"Bike.csv\", parse_dates=[\"dteday\"])\n",
    "reg_df = reg_df.drop([\"instant\", \"casual\", \"registered\"], axis=1)\n",
    "reg_df = reg_df.rename({\"dteday\": \"date\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  season  yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0 2011-01-01       1   0     1        0        6           0           2   \n",
       "1 2011-01-02       1   0     1        0        0           0           2   \n",
       "2 2011-01-03       1   0     1        0        1           1           1   \n",
       "\n",
       "       temp     atemp       hum  windspeed   cnt  \n",
       "0  0.344167  0.363625  0.805833   0.160446   985  \n",
       "1  0.363478  0.353739  0.696087   0.248539   801  \n",
       "2  0.196364  0.189405  0.437273   0.248309  1349  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_X_df = reg_df.iloc[:, :-1] # Input raw df\n",
    "reg_y_df = reg_df.iloc[:, -1] # Output raw df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive new features from date column\n",
    "date_column = reg_X_df.date\n",
    "\n",
    "reg_X_df[\"day_of_year\"] = date_column.dt.dayofyear\n",
    "reg_X_df[\"day_of_month\"] = date_column.dt.day\n",
    "reg_X_df[\"quarter\"] = date_column.dt.quarter\n",
    "reg_X_df[\"week\"] = date_column.dt.week\n",
    "reg_X_df[\"is_month_end\"] = date_column.dt.is_month_end\n",
    "reg_X_df[\"is_month_start\"] = date_column.dt.is_month_start\n",
    "reg_X_df[\"is_quarter_end\"] = date_column.dt.is_quarter_end\n",
    "reg_X_df[\"is_quarter_start\"] = date_column.dt.is_quarter_start\n",
    "reg_X_df[\"is_year_end\"] = date_column.dt.is_year_end\n",
    "reg_X_df[\"is_year_start\"] = date_column.dt.is_year_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>week</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>is_year_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  season  yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0 2011-01-01       1   0     1        0        6           0           2   \n",
       "1 2011-01-02       1   0     1        0        0           0           2   \n",
       "2 2011-01-03       1   0     1        0        1           1           1   \n",
       "\n",
       "       temp     atemp  ...  day_of_year  day_of_month  quarter  week  \\\n",
       "0  0.344167  0.363625  ...            1             1        1    52   \n",
       "1  0.363478  0.353739  ...            2             2        1    52   \n",
       "2  0.196364  0.189405  ...            3             3        1     1   \n",
       "\n",
       "   is_month_end  is_month_start  is_quarter_end  is_quarter_start  \\\n",
       "0         False            True           False              True   \n",
       "1         False           False           False             False   \n",
       "2         False           False           False             False   \n",
       "\n",
       "   is_year_end  is_year_start  \n",
       "0        False           True  \n",
       "1        False          False  \n",
       "2        False          False  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_X_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     985\n",
       "1     801\n",
       "2    1349\n",
       "Name: cnt, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_y_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.8, random_state=None):\n",
    "    train_df = df.sample(frac=test_size, random_state=random_state)\n",
    "    test_df = df[~df.index.isin(train_df.index)]\n",
    "    return train_df.sort_index(), test_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_classification(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_regression(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.sum((y_true - y_pred)**2) / len(y_true)) # RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y, y_pred):\n",
    "    \"\"\"\n",
    "    R2 Score\n",
    "    How much(%) of the total variation in y is explained by variation in x(fitted line)\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(y)\n",
    "    SE_total_variation = np.sum((y - mean_y)**2) # Unexplained max possible variation in y wrt->Mean\n",
    "    SE_line_variation = np.sum((y - y_pred)**2) # Unexplained variation in y wrt -> fitted line\n",
    "    r2_score = 1 - (SE_line_variation / SE_total_variation) # Expalined = 1 - Unexplained\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DecisionStump()\n",
    "- BaseBoostingAlgorithm()\n",
    "- AdaBoostClassifier()\n",
    "- AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Feature/Attribute Index to consider for splitting\n",
    "        self.decision_feature_index = None\n",
    "        # Exact value from Feature/Attribute to split on\n",
    "        self.decision_threshold_value = None\n",
    "        # Stump importance / weight\n",
    "        self.weight = None\n",
    "        # Stump error\n",
    "        self.error = None\n",
    "        # Left leaf value\n",
    "        self.left_leaf_value = None\n",
    "        # Right leaf value\n",
    "        self.right_leaf_value = None\n",
    "        # Stump decision compartor \n",
    "        self.decision_comparator = None\n",
    "        print(\"New Stump Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBoostingAlgorithm():\n",
    "    def __init__(self, n_learners):\n",
    "        self.n_learners = n_learners\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store all weak learners (Weak learner -> A decsion stump)\n",
    "        self.learners = []\n",
    "        # Identify each feature type in input X and store as list\n",
    "        self.feature_types = self._determine_type_of_feature(X)\n",
    "        # Concatenate input and output\n",
    "        self.data = np.concatenate((X, np.expand_dims(y, axis=1)), axis=1)\n",
    "        # Initialize weight for each example as 1/N (where N -> total number of examples)\n",
    "        self.sample_weight = np.full(len(self.data), np.divide(1, len(self.data)))        \n",
    "        print(self.feature_types)\n",
    "        print(self.ml_task)\n",
    "        \n",
    "        # Iterate and build learners\n",
    "        for i_boost in range(self.n_learners):\n",
    "            # Instantiate a new decision stump object\n",
    "            learner = DecisionStump()\n",
    "            # Find and Perform split over best feature \n",
    "            potential_splits = self._get_potential_splits(self.data)\n",
    "            \n",
    "            split_column_index, split_value, metric = self._determine_best_split(self.data, self.sample_weight, potential_splits, self.ml_task)\n",
    "            left_node_data, right_node_data = self._split_data(self.data, self.sample_weight, split_column_index, split_value)\n",
    "            print(f'split_column_index: {split_column_index}, split_value: {split_value}')\n",
    "            print(f'Change in overall_metric: {metric}')\n",
    "            # Compute Leaf values\n",
    "            left_leaf_value = self._create_leaf(left_node_data, self.ml_task)\n",
    "            right_leaf_value = self._create_leaf(right_node_data, self.ml_task)\n",
    "            print(f'Left leaf: {left_leaf_value}, Right leaf: {right_leaf_value}')\n",
    "\n",
    "            # Allocate the instantiated learner with our computed values\n",
    "            learner.decision_feature_index = split_column_index\n",
    "            learner.decision_threshold_value = split_value\n",
    "            learner.left_leaf_value = left_leaf_value\n",
    "            learner.right_leaf_value = right_leaf_value\n",
    "            learner.decision_comparator = self.feature_types[split_column_index]\n",
    "            \n",
    "            # Boosting step\n",
    "            self.sample_weight, learner = self.boost(i_boost,\n",
    "                                              self.data,\n",
    "                                              self.sample_weight,\n",
    "                                              learner)\n",
    "            # Early Termination\n",
    "            if self.sample_weight is None:\n",
    "                break\n",
    "            # Stop boosting since error is 0\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if learner.error == 0 or np.sum(self.sample_weight) <= 0:\n",
    "                self.learners.append(learner)\n",
    "                break\n",
    "            print(f'{i_boost}: Sample weight(sum) [Raw] {np.sum(self.sample_weight)}')\n",
    "            # Dont perform operations in below conditional block if we are on final learner\n",
    "            if not i_boost == self.n_learners - 1:\n",
    "                # Normalize\n",
    "                self.sample_weight /= np.sum(self.sample_weight)\n",
    "                \n",
    "                # Note(Alternative): Sample data(examples) based on sample_weight\n",
    "                # Construct new data set sample based on sample_weight\n",
    "#                 self.data = self._sample_data_by_weights(self.data, self.sample_weight)\n",
    "                # Reinitialize equal sample weights for the new data\n",
    "#                 self.sample_weight = np.full(len(self.data), (1 / len(self.data)))\n",
    "            print(f'{i_boost}: Sample weight(sum) [Normalized] {np.sum(self.sample_weight)}')\n",
    "            # Add this learner to our main list of learners\n",
    "            self.learners.append(learner)\n",
    "            print(f'Total stumps: {len(self.learners)}')\n",
    "            \n",
    "        return self  \n",
    "            \n",
    "    def stump_predict(self, data, learner):\n",
    "        \"\"\"\n",
    "        Computes prediction for the passed data examples w.r.t to the learner(descision stump) \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        feature_column = data[:, learner.decision_feature_index]\n",
    "        for value in feature_column:\n",
    "            if learner.decision_comparator == 'categorical':\n",
    "                if value == learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            else: # continuous\n",
    "                if value <= learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def _gini_sk(self, data):\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        label_column = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        _, value_indexes, counts = np.unique(label_column, return_counts=True, return_index=True)\n",
    "\n",
    "        class_weights = np.array([np.take(data_sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in value_indexes])\n",
    "        \n",
    "        cw = np.sum(class_weights**2)\n",
    "        wn = np.sum(data_sample_weight)**2\n",
    "        gini = 1.0 - (cw/wn)\n",
    "        #print(f\"cw: {cw}, wn: {wn} ---> 1.0 - (cw/wn) == {1.0} - {cw}/{wn}\")\n",
    "        return gini\n",
    "    \n",
    "    def _calculate_weighted_mse(self, data):\n",
    "        \"\"\"\n",
    "        Calculate weighted mean squared error\n",
    "        \"\"\"\n",
    "        actual_values = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        \n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "\n",
    "        else:\n",
    "            prediction = np.mean(actual_values)\n",
    "            # Not normalizing using sum of weighted mean, beacuse the sum of weighted mean is 1\n",
    "            weighted_mse = np.mean((data_sample_weight * (actual_values - prediction))**2)\n",
    "\n",
    "        return weighted_mse\n",
    "    \n",
    "    \n",
    "    def _calculate_weighted_overall_metric(self, data, left_node_data, right_node_data, metric_function):\n",
    "        \"\"\"\n",
    "        Generalized impurity metric, computes weighted overall\n",
    "        impurity/error w.r.t left and right nodes\n",
    "        \"\"\"\n",
    "        # Labels\n",
    "        left_label_column = left_node_data[:, -2]\n",
    "        right_label_column = right_node_data[:, -2]\n",
    "        parent_label_column = data[:, -2]\n",
    "        # Sample weights\n",
    "        left_sample_weight = left_node_data[:, -1]\n",
    "        right_sample_weight = right_node_data[:, -1]\n",
    "        parent_sample_weight = data[:, -1]\n",
    "        \n",
    "        if self.ml_task == 'classification':\n",
    "            _, left_value_indexes, left_counts = np.unique(left_label_column, return_counts=True, return_index=True)\n",
    "            _, right_value_indexes, right_counts = np.unique(right_label_column, return_counts=True, return_index=True)\n",
    "            \n",
    "            aggregated_left_class_weights = np.array([np.take(left_sample_weight, np.where(left_label_column == left_label_column[value_index])[0]).sum() for value_index in left_value_indexes])\n",
    "            aggregated_right_class_weights = np.array([np.take(right_sample_weight, np.where(right_label_column == right_label_column[value_index])[0]).sum() for value_index in right_value_indexes])\n",
    "            \n",
    "            weighted_prob_node_left = np.sum(aggregated_left_class_weights)\n",
    "            weighted_prob_node_right = np.sum(aggregated_right_class_weights)\n",
    "            \n",
    "        else:\n",
    "            total_parent_sample_weight = np.sum(np.sum(left_sample_weight), np.sum(right_sample_weight))\n",
    "            # Weighted probabilities of left and right node\n",
    "            weighted_prob_node_left = np.sum(left_sample_weight) / total_parent_sample_weight\n",
    "            weighted_prob_node_right = np.sum(right_sample_weight) / total_parent_sample_weight\n",
    "        \n",
    "        left_impurity = metric_function(left_node_data)\n",
    "        right_impurity = metric_function(right_node_data)\n",
    "\n",
    "        if left_impurity != None and right_impurity != None:\n",
    "            overall_metric =  weighted_prob_node_left * left_impurity + weighted_prob_node_right * right_impurity\n",
    "            \n",
    "            #print(f'weighted_prob_node_left * w_i(left_node_data)): {weighted_prob_node_left} * {left_impurity} = {(weighted_prob_node_left * left_impurity)}')\n",
    "            #print(f'weighted_prob_node_right * w_i(right_node_data)): {weighted_prob_node_right} * {right_impurity} = {(weighted_prob_node_right * right_impurity)}')\n",
    "            return overall_metric\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Alternative method to weighted loss: Which works by random sampling of example from a distribution based on sample_weight\n",
    "    def _sample_data_by_weights(self, data, sample_weight):\n",
    "        \"\"\"\n",
    "        Construct an new input, iteratively sampled over distribution \n",
    "        formed by passed sample_weight.\n",
    "\n",
    "        Note: \n",
    "        Learn more about this technique: https://youtu.be/LsK-xG1cLYA (Statquest)\n",
    "        \"\"\"\n",
    "        n_samples, _ = np.shape(data)\n",
    "        # Intialize array to hold sampled index  \n",
    "        sampled_indices = []\n",
    "        # Perform cumulative summation over sample_weight to create buckets\n",
    "        sample_weight_buckets = np.cumsum(sample_weight)\n",
    "        # Keeping sampling 'n_samples' times\n",
    "        for _ in range(n_samples):\n",
    "            # Generate a random number between 0 and 1\n",
    "            random_num = np.random.random_sample()\n",
    "            # Find the bucket Eg: weight buckets [0.33, 0.66, 0.99] and random number = 0.47\n",
    "            # then index 1 will be selected (since cumsum value is 0.66)\n",
    "            bucket_index = np.where(sample_weight_buckets > random_num)[0][0]\n",
    "\n",
    "            sampled_indices.append(bucket_index)\n",
    "        # finally construct weighted data using sampled_indexes\n",
    "        weighted_data = data[sampled_indices]\n",
    "\n",
    "        return weighted_data\n",
    "\n",
    "\n",
    "    def _get_potential_splits(self, data):\n",
    "        \"\"\"\n",
    "        Get all potential splits for each feature\n",
    "        Splits can be made on each unique value\n",
    "        Can essentially make a split at each unique value\n",
    "        \n",
    "        \"\"\"\n",
    "        potential_splits = {}\n",
    "        _, n_columns = data.shape\n",
    "        for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            potential_splits[column_index] = unique_values\n",
    "            \n",
    "        return potential_splits\n",
    "    \n",
    "    \n",
    "    def _determine_best_split(self, data, sample_weight, potential_splits, ml_task):\n",
    "        \"\"\"\n",
    "        Iterate over each column_index (as keys) in potential_split (dict)\n",
    "        Perform split(of examples) over each unique value and evaluate the split\n",
    "        Identify the best split and return its feature index and value\n",
    "        \"\"\"\n",
    "        # Stitch data with sample_weight towards the end\n",
    "        data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        \n",
    "        # Best minimum gini index to be updated iteratively\n",
    "        best_overall_metric = float('inf')\n",
    "        \n",
    "        for column_index in potential_splits:\n",
    "            #print(f\"COLUMN {column_index}\")\n",
    "            for value in potential_splits[column_index]:\n",
    "                #print(f'column_index: {column_index}, value: {value}')\n",
    "                left_node_data, right_node_data = self._split_data(data, None, split_column_index=column_index, split_value=value)\n",
    "\n",
    "                if ml_task == \"regression\":\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._calculate_weighted_mse)\n",
    "                else: # classification\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._gini_sk)\n",
    "\n",
    "                # If a lower overall_metric is achieved update the index and value with the current\n",
    "                if current_overall_metric != None and current_overall_metric <= best_overall_metric:\n",
    "                    best_overall_metric = current_overall_metric\n",
    "                    best_split_column_index = column_index\n",
    "                    best_split_value = value\n",
    "                #print(f'best_overall_metric: {best_overall_metric}')\n",
    "                #print('---')\n",
    "            #print(f'Debug [1]: Best: {best_overall_metric}, index: {best_split_column_index}, value: {best_split_value}')\n",
    "        return best_split_column_index, best_split_value, best_overall_metric\n",
    "    \n",
    "    \n",
    "    def _split_data(self, data, sample_weight, split_column_index, split_value):\n",
    "        \"\"\" \n",
    "        Split data(examples) based on best split_column_index and split_value\n",
    "        estimated using task specific splitting metric.\n",
    "        \"\"\"\n",
    "        # Stich sample_weight to data if passed as an argument or the assumption is, it has already been stiched/appended(in _determine_best_split())\n",
    "        if sample_weight is not None:\n",
    "            # Stitch data with sample_weight towards the end (axis=1)\n",
    "            data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        #else it is already appended \n",
    "        \n",
    "        # Get values(from feature column) for the passed split_column index\n",
    "        split_column_values = data[:, split_column_index]\n",
    "        \n",
    "        type_of_feature = self.feature_types[split_column_index]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            left_node_data = data[split_column_values <= split_value]\n",
    "            right_node_data = data[split_column_values >  split_value]\n",
    "\n",
    "        # feature is categorical   \n",
    "        else:\n",
    "            left_node_data = data[split_column_values == split_value]\n",
    "            right_node_data = data[split_column_values != split_value]\n",
    "        return left_node_data, right_node_data\n",
    "    \n",
    "    \n",
    "    def _create_leaf(self, data, ml_task):\n",
    "        \"\"\"\n",
    "        Create leaf node, with leaf value based on ml_task\n",
    "        for,\n",
    "        Classfication: consider majority vote\n",
    "        Regression: consider the mean value\n",
    "        \"\"\"\n",
    "        label_column = data[:, -2]\n",
    "        sample_weight = data[:, -1]\n",
    "        \n",
    "        if ml_task == \"regression\":\n",
    "            leaf = np.mean(label_column)\n",
    "\n",
    "        # classfication    \n",
    "        else:  \n",
    "            # Decide leaf value based on sum of weights for each class in the node\n",
    "            unique_classes, unique_cls_start_indices, counts_unique_classes = np.unique(label_column, return_counts=True, return_index=True)\n",
    "            unique_class_aggregated_weights = np.array([np.take(sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in unique_cls_start_indices])\n",
    "            index = unique_class_aggregated_weights.argmax()\n",
    "            leaf = unique_classes[index]\n",
    "\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    def _determine_type_of_feature(self, X):\n",
    "        \"\"\"\n",
    "        Determine, if the feature is categorical or continuous\n",
    "        \"\"\"\n",
    "        feature_types = []\n",
    "        n_unique_values_treshold = 15 # Threshold for a numeric feature to be categorical\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        for feature_i in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature_i])\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "\n",
    "        return feature_types\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier(BaseBoostingAlgorithm):\n",
    "    def __init__(self, n_learners=20):\n",
    "        # Set total number of weak learners\n",
    "        super().__init__(n_learners)\n",
    "        self.ml_task = \"classification\"\n",
    "        self.classes = None\n",
    "        self.n_classes = None\n",
    "        \n",
    "    def boost(self, i_boost, data, sample_weight, learner):\n",
    "        \"\"\"\n",
    "        Compute learner importance and error, along with boosted weights for each example \n",
    "        \"\"\"\n",
    "        print(f'Boost Called')\n",
    "        \n",
    "        # If its first boost initialize number of classes(n_classes)\n",
    "        if i_boost == 0:\n",
    "            self.classes = np.unique(data[:, -1])\n",
    "            self.n_classes = self.classes.size\n",
    "            \n",
    "        # Perform predictions\n",
    "        preds = self.stump_predict(data, learner)\n",
    "        \n",
    "        # Incorrectly classified examples\n",
    "        incorrect = preds != data[:, -1]\n",
    "\n",
    "        # Learner Error\n",
    "        learner_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        # Stop if classification is perfect\n",
    "        if learner_error <= 0:\n",
    "            learner.weight = 1\n",
    "            learner.error = 0\n",
    "            return sample_weight, learner\n",
    "        print(f'Learner error: {learner_error}')\n",
    "        \n",
    "        # Learner weight\n",
    "        learner_weight = (np.log((1. - learner_error) / (learner_error)) +\n",
    "                        np.log(self.n_classes - 1.))\n",
    "        print(f'Learner weight: {learner_weight}')\n",
    "        \n",
    "        # Boost sample_weight for each each sample\n",
    "        # Dont boost sample_weight if we are on final learner\n",
    "        if not i_boost == self.n_learners - 1:\n",
    "            # Boost only positive weights\n",
    "            sample_weight *= np.exp(learner_weight * incorrect *\n",
    "                                    ((sample_weight > 0) | (learner_weight < 0)))\n",
    "\n",
    "        # Allocate learner its computed weight and error\n",
    "        learner.weight = learner_weight\n",
    "        learner.error = learner_error\n",
    "\n",
    "        # Finally return sample weights and boosted learner\n",
    "        return sample_weight, learner\n",
    "         \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for X.\n",
    "        \"\"\"\n",
    "        print(f'Predict called')\n",
    "        \n",
    "        # Get activated matrix for with respect to each learner [get vote of each learner]\n",
    "        # Add each activated matrix (matrix addition) [get overall vote of all leaners]\n",
    "        # return the overall matrix\n",
    "        # Argmax is used over each row of overall matrix to figure our the class\n",
    "        classes = self.classes[:, np.newaxis]\n",
    "        pred = sum((self.stump_predict(X, learner) == classes).T * learner.weight\n",
    "                   for learner in self.learners)\n",
    "        # Normalize \n",
    "        learner_weights = sum(learner.weight for learner in self.learners)\n",
    "        pred /= learner_weights\n",
    "        \n",
    "        # If its binary classification obatin the form [-, +], convienient to select classes with np.take() \n",
    "        # Eg(binary): classes =  [[c1], [c2]] and pred = [True, False, True], below output: [[c2], [c1], [c2]]\n",
    "        if self.n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return classes.take(pred > 0, axis=0)\n",
    "        # Finds index of column with max value, and uses this index to select class from classes\n",
    "\n",
    "        return classes.take(np.argmax(pred, axis=1), axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = AdaBoostClassifier(20)\n",
    "# clf = clf.fit(df.iloc[:, :-1].values, df.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = clf.predict(df.iloc[:, :-1].values)\n",
    "# accuracy_score_classification(df.iloc[:, -1].values, predictions[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = np.array([4, 1, 1, 2, 3, 2, 3, 3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw = 1/len(tff)\n",
    "tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tww = np.array([tw]*len(tff))\n",
    "tww1 = np.array([0.1, 0.1, 0.1, 0.25, 0.04, 0.25, 0.04, 0.04, 0.04, 0.04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tww1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.1\n",
      "1 0.1\n",
      "1 0.1\n",
      "2 0.25\n",
      "3 0.04\n",
      "2 0.25\n",
      "3 0.04\n",
      "3 0.04\n",
      "3 0.04\n",
      "3 0.04\n"
     ]
    }
   ],
   "source": [
    "for tz1, tzw2 in zip(tff, tww1):\n",
    "    print(tz1, tzw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unq_clses, counts_unq_clses = np.unique(tff, return_counts=True)\n",
    "# idx = counts_unq_clses.argmax()\n",
    "# lf = unq_clses[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2 0.5 0.2 0.1]\n"
     ]
    }
   ],
   "source": [
    "unq_clses, unq_start_indices, counts_unq_clses = np.unique(tff, return_counts=True, return_index=True)\n",
    "# Sum their weights \n",
    "cls_weights = np.array([np.take(tww1, np.where(tff == tff[value_index])[0]).sum() for value_index in unq_start_indices])\n",
    "print(cls_weights)\n",
    "idx = cls_weights.argmax()\n",
    "lf = unq_clses[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unq_clses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_unq_clses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
