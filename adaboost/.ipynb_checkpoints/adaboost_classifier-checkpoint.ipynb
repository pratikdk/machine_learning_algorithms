{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost [classifier + regressor] from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "df = df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.8, random_state=None):\n",
    "    train_df = df.sample(frac=test_size, random_state=random_state)\n",
    "    test_df = df[~df.index.isin(train_df.index)]\n",
    "    return train_df.sort_index(), test_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_classification(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_regression(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.sum((y_true - y_pred)**2) / len(y_true)) # RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DecisionStump()\n",
    "- BaseBoostingAlgorithm()\n",
    "- AdaBoostClassifier()\n",
    "- AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Feature/Attribute Index to consider for splitting\n",
    "        self.decision_feature_index = None\n",
    "        # Exact value from Feature/Attribute to split on\n",
    "        self.decision_threshold_value = None\n",
    "        # Stump importance / weight\n",
    "        self.weight = None\n",
    "        # Stump error\n",
    "        self.error = None\n",
    "        # Left leaf value\n",
    "        self.left_leaf_value = None\n",
    "        # Right leaf value\n",
    "        self.right_leaf_value = None\n",
    "        # Stump decision compartor \n",
    "        self.decision_comparator = None\n",
    "        print(\"New Stump Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBoostingAlgorithm():\n",
    "    def __init__(self, n_learners):\n",
    "        self.n_learners = n_learners\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store all weak learners (Weak learner -> A decsion stump)\n",
    "        self.learners = []\n",
    "        # Identify each feature type in input X and store as list\n",
    "        self.feature_types = self._determine_type_of_feature(X)\n",
    "        # Concatenate input and output\n",
    "        self.data = np.concatenate((X, np.expand_dims(y, axis=1)), axis=1)\n",
    "        # Initialize weight for each example as 1/N (where N -> total number of examples)\n",
    "        self.sample_weight = np.full(len(self.data), np.divide(1, len(self.data)))        \n",
    "        print(self.feature_types)\n",
    "        print(self.ml_task)\n",
    "        \n",
    "        # Iterate and build learners\n",
    "        for i_boost in range(self.n_learners):\n",
    "            # Instantiate a new decision stump object\n",
    "            learner = DecisionStump()\n",
    "            # Find and Perform split over best feature \n",
    "            potential_splits = self._get_potential_splits(self.data)\n",
    "            \n",
    "            split_column_index, split_value, metric = self._determine_best_split(self.data, self.sample_weight, potential_splits, self.ml_task)\n",
    "            left_node_data, right_node_data = self._split_data(self.data, split_column_index, split_value)\n",
    "            print(f'split_column_index: {split_column_index}, split_value: {split_value}')\n",
    "            print(f'Change in overall_metric: {metric}')\n",
    "            # Compute Leaf values\n",
    "            left_leaf_value = self._create_leaf(left_node_data, self.ml_task)\n",
    "            right_leaf_value = self._create_leaf(right_node_data, self.ml_task)\n",
    "            print(f'Left leaf: {left_leaf_value}, Right leaf: {right_leaf_value}')\n",
    "\n",
    "            # Allocate the instantiated learner with our computed values\n",
    "            learner.decision_feature_index = split_column_index\n",
    "            learner.decision_threshold_value = split_value\n",
    "            learner.left_leaf_value = left_leaf_value\n",
    "            learner.right_leaf_value = right_leaf_value\n",
    "            learner.decision_comparator = self.feature_types[split_column_index]\n",
    "            \n",
    "            # Boosting step\n",
    "            self.sample_weight, learner = self.boost(i_boost,\n",
    "                                              self.data,\n",
    "                                              self.sample_weight,\n",
    "                                              learner)\n",
    "            # Early Termination\n",
    "            if self.sample_weight is None:\n",
    "                break\n",
    "            # Stop boosting since error is 0\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if learner.error == 0 or np.sum(self.sample_weight) <= 0:\n",
    "                self.learners.append(learner)\n",
    "                break\n",
    "            print(f'{i_boost}: Sample weight(sum) [Raw] {np.sum(self.sample_weight)}')\n",
    "            # Dont perform operations in below conditional block if we are on final learner\n",
    "            if not i_boost == self.n_learners - 1:\n",
    "                # Normalize\n",
    "                self.sample_weight /= np.sum(self.sample_weight)\n",
    "                # Construct new data set sample based on sample_weight\n",
    "#                 self.data = self._sample_data_by_weights(self.data, self.sample_weight)\n",
    "                # Reinitialize equal sample weights for the new data\n",
    "#                 self.sample_weight = np.full(len(self.data), (1 / len(self.data)))\n",
    "            print(f'{i_boost}: Sample weight(sum) [Normalized] {np.sum(self.sample_weight)}')\n",
    "            # Add this learner to our main list of learners\n",
    "            self.learners.append(learner)\n",
    "            print(f'Total stumps: {len(self.learners)}')\n",
    "            \n",
    "        return self  \n",
    "            \n",
    "    def stump_predict(self, data, learner):\n",
    "        \"\"\"\n",
    "        Computes prediction for the passed data examples w.r.t to the learner(descision stump) \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        feature_column = data[:, learner.decision_feature_index]\n",
    "        for value in feature_column:\n",
    "            if learner.decision_comparator == 'categorical':\n",
    "                if value == learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            else: # continuous\n",
    "                if value <= learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def _gini_sk(self, data):\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        label_column = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        _, value_indexes, counts = np.unique(label_column, return_counts=True, return_index=True)\n",
    "\n",
    "        class_weights = np.array([np.take(data_sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in value_indexes])\n",
    "        \n",
    "        cw = np.sum(class_weights**2)\n",
    "        wn = np.sum(data_sample_weight)**2\n",
    "        gini = 1.0 - (cw/wn)\n",
    "        #print(f\"cw: {cw}, wn: {wn} ---> 1.0 - (cw/wn) == {1.0} - {cw}/{wn}\")\n",
    "        return gini\n",
    "    \n",
    "    def _calculate_weighted_gini_index(self, data):\n",
    "        \"\"\"\n",
    "        Calculate weighted gini index\n",
    "        \"\"\"\n",
    "        label_column = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        #_, counts = np.unique(label_column, return_counts=True)\n",
    "        _, value_indexes, counts = np.unique(label_column, return_counts=True, return_index=True)\n",
    "        # Get summed weights for each class\n",
    "        class_weights = np.array([np.take(data_sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in value_indexes])\n",
    "    \n",
    "        weighted_classes = counts * class_weights\n",
    "        normalized_weighted_classes = weighted_classes / sum(weighted_classes)\n",
    "        \n",
    "        #class_counts_squared = np.sum(counts**2)\n",
    "        #class_weights_squared = np.sum(class_weights)**2\n",
    "        \n",
    "        weighted_gini_impurity = -(1 + sum(normalized_weighted_classes**2))\n",
    "        #print(f'class_weights_squared: {class_weights_squared}')\n",
    "        #weighted_gini_impurity = 1.0 - np.divide(class_counts_squared, class_weights_squared)\n",
    "\n",
    "        return weighted_gini_impurity\n",
    "    \n",
    "    def _calculate_weighted_mse(self, data):\n",
    "        \"\"\"\n",
    "        Calculate weighted mean squared error\n",
    "        \"\"\"\n",
    "        actual_values = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        \n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "\n",
    "        else:\n",
    "            prediction = np.mean(actual_values)\n",
    "            # Not normalizing using sum of weighted mean, beacuse the sum of weighted mean is 1\n",
    "            weighted_mse = np.mean((data_sample_weight * (actual_values - prediction))**2)\n",
    "\n",
    "        return weighted_mse\n",
    "    \n",
    "    \n",
    "    def _calculate_weighted_overall_metric(self, data, left_node_data, right_node_data, metric_function):\n",
    "        \"\"\"\n",
    "        Generalized impurity metric, computes weighted overall\n",
    "        impurity/error w.r.t left and right nodes\n",
    "        \"\"\"\n",
    "        # Labels\n",
    "        left_label_column = left_node_data[:, -2]\n",
    "        right_label_column = right_node_data[:, -2]\n",
    "        parent_label_column = data[:, -2]\n",
    "        # Sample weights\n",
    "        left_sample_weight = left_node_data[:, -1]\n",
    "        right_sample_weight = right_node_data[:, -1]\n",
    "        parent_sample_weight = data[:, -1]\n",
    "        \n",
    "        if self.ml_task == 'classification':\n",
    "            _, left_value_indexes, left_counts = np.unique(left_label_column, return_counts=True, return_index=True)\n",
    "            _, right_value_indexes, right_counts = np.unique(right_label_column, return_counts=True, return_index=True)\n",
    "            _, parent_value_indexes, parent_counts = np.unique(parent_label_column, return_counts=True, return_index=True)\n",
    "            \n",
    "            left_class_weights = np.array([np.take(left_sample_weight, np.where(left_label_column == left_label_column[value_index])[0]).sum() for value_index in left_value_indexes])\n",
    "            right_class_weights = np.array([np.take(right_sample_weight, np.where(right_label_column == right_label_column[value_index])[0]).sum() for value_index in right_value_indexes])\n",
    "            parent_class_weights = np.array([np.take(parent_sample_weight, np.where(parent_label_column == parent_label_column[value_index])[0]).sum() for value_index in parent_value_indexes])\n",
    "            \n",
    "            # class count * class weight, for respective classes\n",
    "            left_weighted_classes = left_counts * left_class_weights\n",
    "            right_weighted_classes = right_counts * right_class_weights\n",
    "            parent_weighted_classes = parent_counts * parent_class_weights\n",
    "            \n",
    "#             weighted_prob_node_left = np.divide(np.sum(left_weighted_classes), np.sum(parent_weighted_classes))\n",
    "#             weighted_prob_node_right = np.divide(np.sum(right_weighted_classes), np.sum(parent_weighted_classes))\n",
    "            \n",
    "            #weighted_prob_node_left = np.sum(left_weighted_classes / np.sum(parent_class_weights))\n",
    "            #weighted_prob_node_right = np.sum(right_weighted_classes / np.sum(parent_class_weights))\n",
    "            \n",
    "            weighted_prob_node_left = np.sum(left_class_weights)# / np.sum(parent_weighted_classes)\n",
    "            weighted_prob_node_right = np.sum(right_class_weights)# / np.sum(parent_weighted_classes)\n",
    "            \n",
    "        else:\n",
    "            total_parent_sample_weight = np.sum(np.sum(left_sample_weight), np.sum(right_sample_weight))\n",
    "            # Weighted probabilities of left and right node\n",
    "            weighted_prob_node_left = np.sum(left_sample_weight) / total_parent_sample_weight\n",
    "            weighted_prob_node_right = np.sum(right_sample_weight) / total_parent_sample_weight\n",
    "        \n",
    "        left_impurity = metric_function(left_node_data)\n",
    "        right_impurity = metric_function(right_node_data)\n",
    "\n",
    "        if left_impurity != None and right_impurity != None:\n",
    "            overall_metric =  weighted_prob_node_left * left_impurity + weighted_prob_node_right * right_impurity\n",
    "            \n",
    "            #print(f'weighted_prob_node_left * w_i(left_node_data)): {weighted_prob_node_left} * {left_impurity} = {(weighted_prob_node_left * left_impurity)}')\n",
    "            #print(f'weighted_prob_node_right * w_i(right_node_data)): {weighted_prob_node_right} * {right_impurity} = {(weighted_prob_node_right * right_impurity)}')\n",
    "            return overall_metric\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def _sample_data_by_weights(self, data, sample_weight):\n",
    "        \"\"\"\n",
    "        Construct an new input, iteratively sampled over distribution \n",
    "        formed by passed sample_weight.\n",
    "\n",
    "        Note: \n",
    "        Learn more about this technique: https://youtu.be/LsK-xG1cLYA (Statquest)\n",
    "        \"\"\"\n",
    "        n_samples, _ = np.shape(data)\n",
    "        # Intialize array to hold sampled index  \n",
    "        sampled_indices = []\n",
    "        # Perform cumulative summation over sample_weight to create buckets\n",
    "        sample_weight_buckets = np.cumsum(sample_weight)\n",
    "        # Keeping sampling 'n_samples' times\n",
    "        for _ in range(n_samples):\n",
    "            # Generate a random number between 0 and 1\n",
    "            random_num = np.random.random_sample()\n",
    "            # Find the bucket Eg: weight buckets [0.33, 0.66, 0.99] and random number = 0.47\n",
    "            # then index 1 will be selected (since cumsum value is 0.66)\n",
    "            bucket_index = np.where(sample_weight_buckets > random_num)[0][0]\n",
    "\n",
    "            sampled_indices.append(bucket_index)\n",
    "        # finally construct weighted data using sampled_indexes\n",
    "        weighted_data = data[sampled_indices]\n",
    "\n",
    "        return weighted_data\n",
    "\n",
    "\n",
    "    def _get_potential_splits(self, data):\n",
    "        \"\"\"\n",
    "        Get all potential splits for each feature\n",
    "        Splits can be made on each unique value\n",
    "        Can essentially make a split at each unique value\n",
    "        \n",
    "        \"\"\"\n",
    "        potential_splits = {}\n",
    "        _, n_columns = data.shape\n",
    "        for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            potential_splits[column_index] = unique_values\n",
    "            \n",
    "        return potential_splits\n",
    "    \n",
    "    \n",
    "    def _determine_best_split(self, data, sample_weight, potential_splits, ml_task):\n",
    "        \"\"\"\n",
    "        Iterate over each column_index (as keys) in potential_split (dict)\n",
    "        Perform split(of examples) over each unique value and evaluate the split\n",
    "        Identify the best split and return its feature index and value\n",
    "        \"\"\"\n",
    "        # Stitch data with sample_weight towards the end\n",
    "        data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        \n",
    "        # Best minimum gini index to be updated iteratively\n",
    "        best_overall_metric = float('inf')\n",
    "        \n",
    "        for column_index in potential_splits:\n",
    "            #print(f\"COLUMN {column_index}\")\n",
    "            for value in potential_splits[column_index]:\n",
    "                #print(f'column_index: {column_index}, value: {value}')\n",
    "                left_node_data, right_node_data = self._split_data(data, split_column_index=column_index, split_value=value)\n",
    "\n",
    "                if ml_task == \"regression\":\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._calculate_weighted_mse)\n",
    "                else: # classification\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._gini_sk)\n",
    "\n",
    "                # If a lower overall_metric is achieved update the index and value with the current\n",
    "                if current_overall_metric != None and current_overall_metric <= best_overall_metric:\n",
    "                    best_overall_metric = current_overall_metric\n",
    "                    best_split_column_index = column_index\n",
    "                    best_split_value = value\n",
    "                #print(f'best_overall_metric: {best_overall_metric}')\n",
    "                #print('---')\n",
    "            #print(f'Debug [1]: Best: {best_overall_metric}, index: {best_split_column_index}, value: {best_split_value}')\n",
    "        return best_split_column_index, best_split_value, best_overall_metric\n",
    "    \n",
    "    \n",
    "    def _split_data(self, data, split_column_index, split_value):\n",
    "        \"\"\" \n",
    "        Split data(examples) based on best split_column_index and split_value\n",
    "        estimated using task specific splitting metric.\n",
    "        \"\"\"\n",
    "        # Get values(from feature column) for the passed split_column index\n",
    "        split_column_values = data[:, split_column_index]\n",
    "\n",
    "        type_of_feature = self.feature_types[split_column_index]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            left_node_data = data[split_column_values <= split_value]\n",
    "            right_node_data = data[split_column_values >  split_value]\n",
    "\n",
    "        # feature is categorical   \n",
    "        else:\n",
    "            left_node_data = data[split_column_values == split_value]\n",
    "            right_node_data = data[split_column_values != split_value]\n",
    "        return left_node_data, right_node_data\n",
    "    \n",
    "    \n",
    "    def _create_leaf(self, data, ml_task):\n",
    "        \"\"\"\n",
    "        Create leaf node, with leaf value based on ml_task\n",
    "        for,\n",
    "        Classfication: consider majority vote\n",
    "        Regression: consider the mean value\n",
    "        \"\"\"\n",
    "        label_column = data[:, -1]\n",
    "        if ml_task == \"regression\":\n",
    "            leaf = np.mean(label_column)\n",
    "\n",
    "        # classfication    \n",
    "        else:\n",
    "            unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "            index = counts_unique_classes.argmax()\n",
    "            leaf = unique_classes[index]\n",
    "\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    def _determine_type_of_feature(self, X):\n",
    "        \"\"\"\n",
    "        Determine, if the feature is categorical or continuous\n",
    "        \"\"\"\n",
    "        feature_types = []\n",
    "        n_unique_values_treshold = 15 # Threshold for a numeric feature to be categorical\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        for feature_i in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature_i])\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "\n",
    "        return feature_types\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier(BaseBoostingAlgorithm):\n",
    "    def __init__(self, n_learners=20):\n",
    "        # Set total number of weak learners\n",
    "        super().__init__(n_learners)\n",
    "        self.ml_task = \"classification\"\n",
    "        self.classes = None\n",
    "        self.n_classes = None\n",
    "        \n",
    "    def boost(self, i_boost, data, sample_weight, learner):\n",
    "        \"\"\"\n",
    "        Compute learner importance and error, along with boosted weights for each example \n",
    "        \"\"\"\n",
    "        print(f'Boost Called')\n",
    "        \n",
    "        # If its first boost initialize number of classes(n_classes)\n",
    "        if i_boost == 0:\n",
    "            self.classes = np.unique(data[:, -1])\n",
    "            self.n_classes = self.classes.size\n",
    "            \n",
    "        # Perform predictions\n",
    "        preds = self.stump_predict(data, learner)\n",
    "        \n",
    "        # Incorrectly classified examples\n",
    "        incorrect = preds != data[:, -1]\n",
    "\n",
    "        # Learner Error\n",
    "        learner_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        # Stop if classification is perfect\n",
    "        if learner_error <= 0:\n",
    "            learner.weight = 1\n",
    "            learner.error = 0\n",
    "            return sample_weight, learner\n",
    "        print(f'Learner error: {learner_error}')\n",
    "        \n",
    "        # Learner weight\n",
    "        learner_weight = (np.log((1. - learner_error) / (learner_error)) +\n",
    "                        np.log(self.n_classes - 1.))\n",
    "        print(f'Learner weight: {learner_weight}')\n",
    "        \n",
    "        # Boost sample_weight for each each sample\n",
    "        # Dont boost sample_weight if we are on final learner\n",
    "        if not i_boost == self.n_learners - 1:\n",
    "        # Boost only positive weights\n",
    "            sample_weight *= np.exp(learner_weight * incorrect *\n",
    "                                    ((sample_weight > 0) | (learner_weight < 0)))\n",
    "#             sample_weight *= np.exp(-learner_weight * np.where(incorrect, -1, 1))\n",
    "\n",
    "        # Allocate learner its computed weight and error\n",
    "        learner.weight = learner_weight\n",
    "        learner.error = learner_error\n",
    "\n",
    "        # Finally return sample weights and boosted learner\n",
    "        return sample_weight, learner\n",
    "         \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for X.\n",
    "        \"\"\"\n",
    "        print(f'Predict called')\n",
    "        \n",
    "        # Get activated matrix for with respect to each learner [get vote of each learner]\n",
    "        # Add each activated matrix (matrix addition) [get overall vote of all leaners]\n",
    "        # return the overall matrix\n",
    "        # Argmax is used over each row of overall matrix to figure our the class\n",
    "        classes = self.classes[:, np.newaxis]\n",
    "        pred = sum((self.stump_predict(X, learner) == classes).T * learner.weight\n",
    "                   for learner in self.learners)\n",
    "        # Normalize \n",
    "        learner_weights = sum(learner.weight for learner in self.learners)\n",
    "        pred /= learner_weights\n",
    "        \n",
    "        # If its binary classification obatin the form [-, +], convienient to select classes with np.take() \n",
    "        # Eg(binary): classes =  [[c1], [c2]] and pred = [True, False, True], below output: [[c2], [c1], [c2]]\n",
    "        if self.n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return classes.take(pred > 0, axis=0)\n",
    "        # Finds index of column with max value, and uses this index to select class from classes\n",
    "\n",
    "        return classes.take(np.argmax(pred, axis=1), axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['continuous', 'continuous', 'continuous', 'continuous']\n",
      "classification\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Change in overall_metric: 0.33333333333333404\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.3333333333333332\n",
      "Learner weight: 1.386294361119891\n",
      "0: Sample weight(sum) [Raw] 2.0\n",
      "0: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 1\n",
      "New Stump Created!\n",
      "split_column_index: 2, split_value: 4.7\n",
      "Change in overall_metric: 0.21404997642621418\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.18000000000000002\n",
      "Learner weight: 2.2094946699280333\n",
      "1: Sample weight(sum) [Raw] 2.4600000000000004\n",
      "1: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 2\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 1.6\n",
      "Change in overall_metric: 0.20526378968831266\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.6389641674194517\n",
      "Learner weight: 0.12227601641353214\n",
      "2: Sample weight(sum) [Raw] 1.0831074977416448\n",
      "2: Sample weight(sum) [Normalized] 0.9999999999999997\n",
      "Total stumps: 3\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 1.6\n",
      "Change in overall_metric: 0.20079028936870236\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.6666666666666667\n",
      "Learner weight: -3.3306690738754696e-16\n",
      "3: Sample weight(sum) [Raw] 0.9999999999999996\n",
      "3: Sample weight(sum) [Normalized] 0.9999999999999998\n",
      "Total stumps: 4\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 1.6\n",
      "Change in overall_metric: 0.20079028936870208\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.6666666666666669\n",
      "Learner weight: -7.771561172376096e-16\n",
      "4: Sample weight(sum) [Raw] 0.9999999999999998\n",
      "4: Sample weight(sum) [Normalized] 0.9999999999999998\n",
      "Total stumps: 5\n"
     ]
    }
   ],
   "source": [
    "clf = clf.fit(df.iloc[:, :-1].values, df.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict called\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_classification(df.iloc[:, -1].values, predictions[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
