{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost [classifier + regressor] from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Iris.csv\")\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "df = df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types of lables\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20d485c6518>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency plot\n",
    "df['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.8, random_state=None):\n",
    "    train_df = df.sample(frac=test_size, random_state=random_state)\n",
    "    test_df = df[~df.index.isin(train_df.index)]\n",
    "    return train_df.sort_index(), test_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_classification(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_regression(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.sum((y_true - y_pred)**2) / len(y_true)) # RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DecisionStump()\n",
    "- BaseBoostingAlgorithm()\n",
    "- AdaBoostClassifier()\n",
    "- AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Feature/Attribute Index to consider for splitting\n",
    "        self.decision_feature_index = None\n",
    "        # Exact value from Feature/Attribute to split on\n",
    "        self.decision_threshold_value = None\n",
    "        # Stump importance / weight\n",
    "        self.weight = None\n",
    "        # Stump error\n",
    "        self.error = None\n",
    "        # Left leaf value\n",
    "        self.left_leaf_value = None\n",
    "        # Right leaf value\n",
    "        self.right_leaf_value = None\n",
    "        # Stump decision compartor \n",
    "        self.decision_comparator = None\n",
    "        print(\"New Stump Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBoostingAlgorithm():\n",
    "    def __init__(self, n_learners):\n",
    "        self.n_learners = n_learners\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store all weak learners (Weak learner -> A decsion stump)\n",
    "        self.learners = []\n",
    "        # Identify each feature type in input X and store as list\n",
    "        self.feature_types = self._determine_type_of_feature(X)\n",
    "        # Concatenate input and output\n",
    "        self.data = np.concatenate((X, np.expand_dims(y, axis=1)), axis=1)\n",
    "        # Initialize weight for each example as 1/N (where N -> total number of examples)\n",
    "        self.sample_weight = np.full(len(self.data), (1 / len(self.data)))\n",
    "        \n",
    "        print(self.learners)\n",
    "        print(self.feature_types)\n",
    "        print(self.ml_task)\n",
    "        \n",
    "        # Iterate and build learners\n",
    "        for i_boost in range(self.n_learners):\n",
    "            # Instantiate a new decision stump object\n",
    "            learner = DecisionStump()\n",
    "\n",
    "            # Find and Perform split over best feature \n",
    "            potential_splits = self._get_potential_splits(self.data)\n",
    "            split_column_index, split_value = self._determine_best_split(self.data, potential_splits, self.ml_task)\n",
    "            left_node_data, right_node_data = self._split_data(self.data, split_column_index, split_value)\n",
    "            print(f'split_column_index: {split_column_index}, split_value: {split_value}')\n",
    "\n",
    "            # Compute Leaf values\n",
    "            left_leaf_value = self._create_leaf(left_node_data, self.ml_task)\n",
    "            right_leaf_value = self._create_leaf(right_node_data, self.ml_task)\n",
    "            print(f'Left leaf: {left_leaf_value}, Right leaf: {right_leaf_value}')\n",
    "\n",
    "            # Allocate the instantiated learner with our computed values\n",
    "            learner.decision_feature_index = split_column_index\n",
    "            learner.decision_threshold_value = split_value\n",
    "            learner.left_leaf_value = left_leaf_value\n",
    "            learner.right_leaf_value = right_leaf_value\n",
    "            learner.decision_comparator = self.feature_types[split_column_index]\n",
    "            \n",
    "            # Boosting step\n",
    "            self.sample_weight, learner = self.boost(i_boost,\n",
    "                                              self.data,\n",
    "                                              self.sample_weight,\n",
    "                                              learner)\n",
    "            # Early Termination\n",
    "            if self.sample_weight is None:\n",
    "                break\n",
    "            # Stop boosting since error is 0\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if learner.error == 0 or np.sum(self.sample_weight) <= 0:\n",
    "                self.learners.append(learner)\n",
    "                break\n",
    "            print(f'{i_boost}: Sample weight(sum) {np.sum(self.sample_weight)}')\n",
    "            # Dont perform operations in below conditional block if we are on final learner\n",
    "            if not i_boost == self.n_learners - 1:\n",
    "                # Normalize\n",
    "                self.sample_weight /= np.sum(self.sample_weight)\n",
    "                # Construct new data set sample based on sample_weight\n",
    "                self.data = self._sample_data_by_weights(self.data, self.sample_weight)\n",
    "                # Reinitialize equal sample weights for the new data\n",
    "                self.sample_weight = np.full(len(self.data), (1 / len(self.data)))\n",
    "            \n",
    "            # Add this learner to our main list of learners\n",
    "            self.learners.append(learner)\n",
    "            print(f'Total stumps: {len(self.learners)}')\n",
    "            \n",
    "        return self  \n",
    "            \n",
    "    def stump_predict(self, data, learner):\n",
    "        \"\"\"\n",
    "        Computes prediction for the passed data examples w.r.t to the learner(descision stump) \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        feature_column = data[:, learner.decision_feature_index]\n",
    "        for value in feature_column:\n",
    "            if learner.decision_comparator == 'categorical':\n",
    "                if value == learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            else: # continuous\n",
    "                if value <= learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "    def _sample_data_by_weights(self, data, sample_weight):\n",
    "        \"\"\"\n",
    "        Construct an new input, iteratively sampled over distribution \n",
    "        formed by passed sample_weight.\n",
    "\n",
    "        Note: \n",
    "        Learn more about this technique: https://youtu.be/LsK-xG1cLYA (Statquest)\n",
    "        \"\"\"\n",
    "        n_samples, _ = np.shape(data)\n",
    "        # Intialize array to hold sampled index  \n",
    "        sampled_indices = []\n",
    "        # Perform cumulative summation over sample_weight to create buckets\n",
    "        sample_weight_buckets = np.cumsum(sample_weight)\n",
    "        # Keeping sampling 'n_samples' times\n",
    "        for _ in range(n_samples):\n",
    "            # Generate a random number between 0 and 1\n",
    "            random_num = np.random.random_sample()\n",
    "            # Find the bucket Eg: weight buckets [0.33, 0.66, 0.99] and random number = 0.47\n",
    "            # then index 1 will be selected (since cumsum value is 0.66)\n",
    "            bucket_index = np.where(sample_weight_buckets > random_num)[0][0]\n",
    "\n",
    "            sampled_indices.append(bucket_index)\n",
    "        # finally construct weighted data using sampled_indexes\n",
    "        weighted_data = data[sampled_indices]\n",
    "\n",
    "        return weighted_data\n",
    "\n",
    "\n",
    "    def _get_potential_splits(self, data):\n",
    "        \"\"\"\n",
    "        Get all potential splits for each feature\n",
    "        Splits can be made on each unique value\n",
    "        Can essentially make a split at each unique value\n",
    "        \n",
    "        \"\"\"\n",
    "        potential_splits = {}\n",
    "        _, n_columns = data.shape\n",
    "        for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            potential_splits[column_index] = unique_values\n",
    "\n",
    "        return potential_splits\n",
    "    \n",
    "    \n",
    "    def _calculate_gini_index(self, data):\n",
    "        \"\"\"\n",
    "        Calculate gini index\n",
    "        \"\"\"\n",
    "        label_column = data[:, -1]\n",
    "        _, counts = np.unique(label_column, return_counts=True)\n",
    "\n",
    "        probabilities = counts / counts.sum()\n",
    "        gini_impurity = - (1 + sum(probabilities**2))\n",
    "\n",
    "        return gini_impurity\n",
    "    \n",
    "    \n",
    "    def _calculate_mse(self, data):\n",
    "        \"\"\"\n",
    "        Calculate mean squared error\n",
    "        \"\"\"\n",
    "        actual_values = data[:, -1]\n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "\n",
    "        else:\n",
    "            prediction = np.mean(actual_values)\n",
    "            mse = np.mean((actual_values - prediction) **2)\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    \n",
    "    def _calculate_overall_metric(self, left_node_data, right_node_data, metric_function):\n",
    "        \"\"\"\n",
    "        Generalized impurity metric, computes weighted overall\n",
    "        impurity/error w.r.t left and right nodes\n",
    "        \"\"\"\n",
    "        n = len(left_node_data) + len(right_node_data)\n",
    "        # Probabilities of left and right node\n",
    "        prob_node_left = len(left_node_data) / n\n",
    "        prob_node_right = len(right_node_data) / n\n",
    "\n",
    "        overall_metric =  (prob_node_left * metric_function(left_node_data) \n",
    "                         + prob_node_right * metric_function(right_node_data))\n",
    "        \n",
    "        print(f'prob_node_left * w_i(left_node_data)): {prob_node_left} * {metric_function(left_node_data)} = {(prob_node_left * metric_function(left_node_data))}')\n",
    "        print(f'prob_node_right * w_i(right_node_data)): {prob_node_right} * {metric_function(right_node_data)} = {(prob_node_right * metric_function(right_node_data))}')\n",
    "        \n",
    "\n",
    "        return overall_metric\n",
    "    \n",
    "    def _determine_best_split(self, data, potential_splits, ml_task):\n",
    "        \"\"\"\n",
    "        Iterate over each column_index (as keys) in potential_split (dict)\n",
    "        Perform split(of examples) over each unique value and evaluate the split\n",
    "        Identify the best split and return its feature index and value\n",
    "        \"\"\"\n",
    "        \n",
    "        # Best minimum gini index to be updated iteratively\n",
    "        best_overall_metric = float('inf')\n",
    "        \n",
    "        for column_index in potential_splits:\n",
    "            for value in potential_splits[column_index]:\n",
    "                left_node_data, right_node_data = self._split_data(data, split_column_index=column_index, split_value=value)\n",
    "\n",
    "                if ml_task == \"regression\":\n",
    "                    current_overall_metric = self._calculate_overall_metric(left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._calculate_mse)\n",
    "                else: # classification\n",
    "                    current_overall_metric = self._calculate_overall_metric(left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._calculate_gini_index)\n",
    "                \n",
    "                # If a lower overall_metric is achieved update the index and value with the current\n",
    "                #print(column_index, value, current_overall_metric)\n",
    "                if current_overall_metric <= best_overall_metric:\n",
    "                    best_overall_metric = current_overall_metric\n",
    "                    best_split_column_index = column_index\n",
    "                    best_split_value = value\n",
    "\n",
    "        return best_split_column_index, best_split_value\n",
    "    \n",
    "    \n",
    "    def _split_data(self, data, split_column_index, split_value):\n",
    "        \"\"\" \n",
    "        Split data(examples) based on best split_column_index and split_value\n",
    "        estimated using task specific splitting metric.\n",
    "        \"\"\"\n",
    "        # Get values(from feature column) for the passed split_column index\n",
    "        split_column_values = data[:, split_column_index]\n",
    "\n",
    "        type_of_feature = self.feature_types[split_column_index]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            left_node_data = data[split_column_values <= split_value]\n",
    "            right_node_data = data[split_column_values >  split_value]\n",
    "\n",
    "        # feature is categorical   \n",
    "        else:\n",
    "            left_node_data = data[split_column_values == split_value]\n",
    "            right_node_data = data[split_column_values != split_value]\n",
    "\n",
    "        return left_node_data, right_node_data\n",
    "    \n",
    "    \n",
    "    def _create_leaf(self, data, ml_task):\n",
    "        \"\"\"\n",
    "        Create leaf node, with leaf value based on ml_task\n",
    "        for,\n",
    "        Classfication: consider majority vote\n",
    "        Regression: consider the mean value\n",
    "        \"\"\"\n",
    "        label_column = data[:, -1]\n",
    "        if ml_task == \"regression\":\n",
    "            leaf = np.mean(label_column)\n",
    "\n",
    "        # classfication    \n",
    "        else:\n",
    "            unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "            index = counts_unique_classes.argmax()\n",
    "            leaf = unique_classes[index]\n",
    "\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    def _determine_type_of_feature(self, X):\n",
    "        \"\"\"\n",
    "        Determine, if the feature is categorical or continuous\n",
    "        \"\"\"\n",
    "        feature_types = []\n",
    "        n_unique_values_treshold = 15 # Threshold for a numeric feature to be categorical\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        for feature_i in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature_i])\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "\n",
    "        return feature_types\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier(BaseBoostingAlgorithm):\n",
    "    def __init__(self, n_learners=20):\n",
    "        # Set total number of weak learners\n",
    "        super().__init__(n_learners)\n",
    "        self.ml_task = \"classification\"\n",
    "        self.classes = None\n",
    "        self.n_classes = None\n",
    "        \n",
    "    def boost(self, i_boost, data, sample_weight, learner):\n",
    "        \"\"\"\n",
    "        Compute learner importance and error, along with boosted weights for each example \n",
    "        \"\"\"\n",
    "        print(f'Boost Called')\n",
    "        \n",
    "        # If its first boost initialize number of classes(n_classes)\n",
    "        if i_boost == 0:\n",
    "            self.classes = np.unique(data[:, -1])\n",
    "            self.n_classes = self.classes.size\n",
    "        \n",
    "        # Perform predictions\n",
    "        preds = self.stump_predict(self.data, learner)\n",
    "        \n",
    "        # Incorrectly classified examples\n",
    "        incorrect = preds != data[:, -1]\n",
    "        \n",
    "        # Learner Error\n",
    "        learner_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        # Stop if classification is perfect\n",
    "        if learner_error <= 0:\n",
    "            learner.weight = 1\n",
    "            learner.error = 0\n",
    "            return sample_weight, learner\n",
    "        print(f'Learner error: {learner_error}')\n",
    "        \n",
    "        # Learner weight\n",
    "        learner_weight = (np.log((1 - learner_error) / learner_error) +\n",
    "                        np.log(self.n_classes - 1))\n",
    "        print(f'Learner weight: {learner_weight}')\n",
    "        \n",
    "        # Boost sample_weight for each each sample\n",
    "        # Dont boost sample_weight if we are on final learner\n",
    "        if not i_boost == self.n_learners - 1:\n",
    "        # Boost only positive weights\n",
    "        #print(f'#### Sample Weights before')\n",
    "        #print(sample_weight)\n",
    "            sample_weight *= np.exp(learner_weight * incorrect *\n",
    "                                    ((sample_weight > 0) |\n",
    "                                     (learner_weight < 0)))\n",
    "        \n",
    "        #print(f'#### Sample Weights After')\n",
    "        #print(sample_weight)\n",
    "        # Allocate learner its computed weight and error\n",
    "        learner.weight = learner_weight\n",
    "        learner.error = learner_error\n",
    "        \n",
    "        # Finally return sample weights and boosted learner\n",
    "        return sample_weight, learner\n",
    "         \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for X.\n",
    "        \"\"\"\n",
    "        print(f'Predict called')\n",
    "        \n",
    "        # Get activated matrix for with respect to each learner [get vote of each learner]\n",
    "        # Add each activated matrix (matrix addition) [get overall vote of all leaners]\n",
    "        # return the overall matrix\n",
    "        # Argmax is used over each row of overall matrix to figure our the class\n",
    "        classes = self.classes[:, np.newaxis]\n",
    "        pred = sum((self.stump_predict(X, learner) == classes).T * learner.weight\n",
    "                   for learner in self.learners)\n",
    "        # Normalize \n",
    "        learner_weights = sum(learner.weight for learner in self.learners)\n",
    "        pred /= learner_weights\n",
    "        \n",
    "        # If its binary classification obatin the form [-, +], convienient to select classes with np.take() \n",
    "        # Eg(binary): classes =  [[c1], [c2]] and pred = [True, False, True], below output: [[c2], [c1], [c2]]\n",
    "        if self.n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return classes.take(pred > 0, axis=0)\n",
    "        # Finds index of column with max value, and uses this index to select class from classes\n",
    "        #print(f'Classes ({classes.shape}): {classes}')\n",
    "        #print(f'Classes ({pred.shape}): {pred}')\n",
    "        #print(f'Classes element ({pred[0].shape}): {pred}')\n",
    "        return classes.take(np.argmax(pred, axis=1), axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i for i in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.71828183,  7.3890561 , 20.08553692])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['a'],\n",
       "       ['b'],\n",
       "       ['c']], dtype='<U1')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgg = np.array(['a', 'b', 'c'])\n",
    "xgg = xgg[:, np.newaxis]\n",
    "xgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'b'], dtype='<U1')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgp = np.array(['a', 'b', 'b'])\n",
    "xgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False],\n",
       "       [False,  True,  True],\n",
       "       [False, False, False]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xgp == xgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xgp == xgg).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 2, 0],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((xgp == xgg).T * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 0],\n",
       "       [0, 4, 0],\n",
       "       [0, 4, 0]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([((xgp == xgg).T * 2), ((xgp == xgg).T * 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((xgp == xgg) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((xgp == xgg).T * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [0, 2],\n",
       "       [0, 2]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtt = np.array([[2, 0, 0], [0, 2, 2]]).T\n",
    "vtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtt[:, 0] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2,  0],\n",
       "       [ 0,  2],\n",
       "       [ 0,  2]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vtt.sum(axis=1) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = np.array([['class1'], ['class2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['class1'],\n",
       "       ['class2'],\n",
       "       ['class1'],\n",
       "       ['class2'],\n",
       "       ['class2']], dtype='<U6')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs.take([False, True, False, True, True], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostRegressor():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/4 * True * (1/2 > 0) * (-1 < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['continuous', 'continuous', 'continuous', 'continuous']\n",
      "classification\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.3333333333333332\n",
      "Learner weight: 1.386294361119891\n",
      "0: Sample weight(sum) 2.0\n",
      "Total stumps: 1\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 1.4\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.15999999999999998\n",
      "Learner weight: 2.3513752571634776\n",
      "1: Sample weight(sum) 2.5199999999999996\n",
      "Total stumps: 2\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.11333333333333331\n",
      "Learner weight: 2.7502829647254834\n",
      "2: Sample weight(sum) 2.66\n",
      "Total stumps: 3\n",
      "New Stump Created!\n",
      "split_column_index: 3, split_value: 0.6\n",
      "Left leaf: Iris-setosa, Right leaf: Iris-versicolor\n",
      "Boost Called\n",
      "Learner error: 0.13999999999999999\n",
      "Learner weight: 2.5084371471981943\n",
      "3: Sample weight(sum) 2.579999999999999\n",
      "Total stumps: 4\n",
      "New Stump Created!\n",
      "split_column_index: 2, split_value: 4.9\n",
      "Left leaf: Iris-versicolor, Right leaf: Iris-virginica\n",
      "Boost Called\n",
      "Learner error: 0.1333333333333333\n",
      "Learner weight: 2.564949357461537\n",
      "4: Sample weight(sum) 1.0000000000000002\n",
      "Total stumps: 5\n"
     ]
    }
   ],
   "source": [
    "wdd = abc.fit(df.iloc[:, :-1].values, df.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict called\n"
     ]
    }
   ],
   "source": [
    "predicc = wdd.predict(df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_classification(df.iloc[:, -1].values, predicc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=1, random_state=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(df.iloc[:, :-1].values, df.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_pred = clf.predict(df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_classification(df.iloc[:, -1].values, sk_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdddf['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object),\n",
       " array([50, 50, 50], dtype=int64))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df.values[:, -1], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object),\n",
       " array([30, 21, 99], dtype=int64))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(wdd[:, -1], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e4dd538415fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwdd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py\u001b[0m in \u001b[0;36mhistogram\u001b[1;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[0;32m    778\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ravel_and_check_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[0mbin_edges\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniform_bins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_bin_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m     \u001b[1;31m# Histogram is an integer or a float array depending on the weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py\u001b[0m in \u001b[0;36m_get_bin_edges\u001b[1;34m(a, bins, range, weights)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'`bins` must be positive, when an integer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m         \u001b[0mfirst_edge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_outer_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\histograms.py\u001b[0m in \u001b[0;36m_get_outer_edges\u001b[1;34m(a, range)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mfirst_edge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_edge\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_edge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             raise ValueError(\n\u001b[0;32m    315\u001b[0m                 \"autodetected range of [{}, {}] is not finite\".format(first_edge, last_edge))\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "np.histogram(wdd[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df.values[:, -1]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = df.values[:, -1][:3] == ['Iris-setos', 'Iris-setosa', 'Iris-setosa']\n",
    "np.mean(np.average(ft, weights=np.array([0.2, 0.4, 0.4]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_digits()\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABC():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        print('Instantiate:',  name)\n",
    "    def print_myname(self, myname):\n",
    "        print(\"My name is\", myname)\n",
    "        self.predict()\n",
    "    def hello1(self):\n",
    "        print('h1')\n",
    "    def hello2(self):\n",
    "        self.hello1()\n",
    "        \n",
    "    def a2(self, name):\n",
    "        print('a2', name)\n",
    "    def a3(self, name):\n",
    "        print('a3', name)\n",
    "    def az(self, m_func):\n",
    "        m_func('pratik')\n",
    "        \n",
    "    def func_mix(self):\n",
    "        self.az(m_func=self.a2)\n",
    "        self.az(m_func=self.a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(ABC):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "    def something_about_me(self, name):\n",
    "        self.print_myname(name)\n",
    "    def predict(self):\n",
    "        print('predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiate: A\n"
     ]
    }
   ],
   "source": [
    "stored = A('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is pratik\n",
      "predicted\n"
     ]
    }
   ],
   "source": [
    "stored.something_about_me('pratik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is pratik\n",
      "predicted\n"
     ]
    }
   ],
   "source": [
    "stored.print_myname('pratik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2 pratik\n",
      "a3 pratik\n"
     ]
    }
   ],
   "source": [
    "stored.func_mix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a2() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-321-7373c228accd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstored\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: a2() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "stored.a2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-edfbb22029d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "np.unique(X[:, feature_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = np.array([\n",
    "       [2, 4, 1, 1, 0],\n",
    "       [5, 2, 1, 0, 0],\n",
    "       [0, 8, 0, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([2, 0], dtype=int64))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(tf[:, 2], return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a5a9ae7ac93f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "np.concatenate((df.iloc[:, :-1].values, df.iloc[:, -1].values), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate((df.iloc[:, :-1].values, np.expand_dims(df.iloc[:, -1].values, axis=1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, :-1].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, -1].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.expand_dims(df.iloc[:, -1].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33, 0.66, 0.99])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt = np.cumsum(np.array([0.33, 0.33, 0.33]))\n",
    "vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47607097125121944"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ide = np.random.random_sample()\n",
    "ide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  2,  1],\n",
       "       [22,  1,  2,  1],\n",
       "       [33,  1,  2,  1],\n",
       "       [44,  1,  2,  1],\n",
       "       [55,  1,  2,  1]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idr = np.array([[1, 1, 2, 1], [22, 1, 2, 1], [33, 1, 2, 1], [44, 1, 2, 1], [55, 1, 2, 1]])\n",
    "idr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "idd = [0, 0, 1, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  2,  1],\n",
       "       [ 1,  1,  2,  1],\n",
       "       [22,  1,  2,  1],\n",
       "       [33,  1,  2,  1],\n",
       "       [33,  1,  2,  1]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idr[idd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = np.array([0.4, 0.1, 0.1, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4, 0.5, 0.6, 0.8, 1. ])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_by_weight(data, sample_weight):\n",
    "    \"\"\"\n",
    "    Construct an new input, iteratively sampled over distribution \n",
    "    formed by passed sample_weight.\n",
    "    \n",
    "    Note: \n",
    "    Learn more about this technique: https://youtu.be/LsK-xG1cLYA (Statquest)\n",
    "    \"\"\"\n",
    "    n_samples, _ = np.shape(data)\n",
    "    # Intialize array to hold sampled index  \n",
    "    sampled_indices = []\n",
    "    # Perform cumulative summation over sample_weight to create buckets\n",
    "    sample_weight_buckets = np.cumsum(sample_weight)\n",
    "    # Keeping sampling 'n_samples' times\n",
    "    for _ in range(n_samples):\n",
    "        # Generate a random number between 0 and 1\n",
    "        random_num = np.random.random_sample()\n",
    "        # Find the bucket Eg: weight buckets [0.33, 0.66, 0.99] and random number = 0.47\n",
    "        # then index 1 will be selected (since cumsum value is 0.66)\n",
    "        bucket_index = np.where(sample_weight_buckets > random_num)[0][0]\n",
    "        \n",
    "        sampled_indices.append(bucket_index)\n",
    "    # finally construct weighted data using sampled_indexes\n",
    "    weighted_data = data[sampled_indices]\n",
    "    \n",
    "    return weighted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  2,  1],\n",
       "       [ 1,  1,  2,  1],\n",
       "       [ 1,  1,  2,  1],\n",
       "       [ 1,  1,  2,  1],\n",
       "       [55,  1,  2,  1]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data_by_weight(idr, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(vt > ide)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf = [np.random.random_sample() for i in range(100)] \n",
    "stfr = [random.uniform(0, 1) for i in range(100)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14.,  9., 11.,  6., 15.,  6., 14.,  7.,  7., 11.]),\n",
       " array([0.00696925, 0.10549445, 0.20401964, 0.30254484, 0.40107003,\n",
       "        0.49959522, 0.59812042, 0.69664561, 0.79517081, 0.893696  ,\n",
       "        0.9922212 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(stf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12., 14., 10.,  8.,  5., 10.,  6., 12., 10., 13.]),\n",
       " array([9.60159013e-04, 9.95832402e-02, 1.98206321e-01, 2.96829402e-01,\n",
       "        3.95452484e-01, 4.94075565e-01, 5.92698646e-01, 6.91321727e-01,\n",
       "        7.89944808e-01, 8.88567889e-01, 9.87190971e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(stfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
