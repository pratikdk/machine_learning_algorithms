{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Adaboost [Regressor] from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data [Regression]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/marklvl/bike-sharing-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = pd.read_csv(\"Bike.csv\", parse_dates=[\"dteday\"])\n",
    "reg_df = reg_df.drop([\"instant\", \"casual\", \"registered\"], axis=1)\n",
    "reg_df = reg_df.rename({\"dteday\": \"date\"}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  season  yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0 2011-01-01       1   0     1        0        6           0           2   \n",
       "1 2011-01-02       1   0     1        0        0           0           2   \n",
       "2 2011-01-03       1   0     1        0        1           1           1   \n",
       "\n",
       "       temp     atemp       hum  windspeed   cnt  \n",
       "0  0.344167  0.363625  0.805833   0.160446   985  \n",
       "1  0.363478  0.353739  0.696087   0.248539   801  \n",
       "2  0.196364  0.189405  0.437273   0.248309  1349  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_X_df = reg_df.iloc[:, :-1] # Input raw df\n",
    "reg_y_df = reg_df.iloc[:, -1] # Output raw df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive new features from date column\n",
    "date_column = reg_X_df.date\n",
    "\n",
    "reg_X_df[\"day_of_year\"] = date_column.dt.dayofyear\n",
    "reg_X_df[\"day_of_month\"] = date_column.dt.day\n",
    "reg_X_df[\"quarter\"] = date_column.dt.quarter\n",
    "reg_X_df[\"week\"] = date_column.dt.week\n",
    "reg_X_df[\"is_month_end\"] = date_column.dt.is_month_end\n",
    "reg_X_df[\"is_month_start\"] = date_column.dt.is_month_start\n",
    "reg_X_df[\"is_quarter_end\"] = date_column.dt.is_quarter_end\n",
    "reg_X_df[\"is_quarter_start\"] = date_column.dt.is_quarter_start\n",
    "reg_X_df[\"is_year_end\"] = date_column.dt.is_year_end\n",
    "reg_X_df[\"is_year_start\"] = date_column.dt.is_year_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>quarter</th>\n",
       "      <th>week</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_quarter_end</th>\n",
       "      <th>is_quarter_start</th>\n",
       "      <th>is_year_end</th>\n",
       "      <th>is_year_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  season  yr  mnth  holiday  weekday  workingday  weathersit  \\\n",
       "0 2011-01-01       1   0     1        0        6           0           2   \n",
       "1 2011-01-02       1   0     1        0        0           0           2   \n",
       "2 2011-01-03       1   0     1        0        1           1           1   \n",
       "\n",
       "       temp     atemp  ...  day_of_year  day_of_month  quarter  week  \\\n",
       "0  0.344167  0.363625  ...            1             1        1    52   \n",
       "1  0.363478  0.353739  ...            2             2        1    52   \n",
       "2  0.196364  0.189405  ...            3             3        1     1   \n",
       "\n",
       "   is_month_end  is_month_start  is_quarter_end  is_quarter_start  \\\n",
       "0         False            True           False              True   \n",
       "1         False           False           False             False   \n",
       "2         False           False           False             False   \n",
       "\n",
       "   is_year_end  is_year_start  \n",
       "0        False           True  \n",
       "1        False          False  \n",
       "2        False          False  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_X_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     985\n",
       "1     801\n",
       "2    1349\n",
       "Name: cnt, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_y_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.8, random_state=None):\n",
    "    train_df = df.sample(frac=test_size, random_state=random_state)\n",
    "    test_df = df[~df.index.isin(train_df.index)]\n",
    "    return train_df.sort_index(), test_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_classification(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_regression(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.sum((y_true - y_pred)**2) / len(y_true)) # RMSE\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y, y_pred):\n",
    "    \"\"\"\n",
    "    R2 Score\n",
    "    How much(%) of the total variation in y is explained by variation in x(fitted line)\n",
    "    \"\"\"\n",
    "    mean_y = np.mean(y)\n",
    "    SE_total_variation = np.sum((y - mean_y)**2) # Unexplained max possible variation in y wrt->Mean\n",
    "    SE_line_variation = np.sum((y - y_pred)**2) # Unexplained variation in y wrt -> fitted line\n",
    "    r2_score = 1 - (SE_line_variation / SE_total_variation) # Expalined = 1 - Unexplained\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DecisionStump()\n",
    "- BaseBoostingAlgorithm()\n",
    "- AdaBoostClassifier()\n",
    "- AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Feature/Attribute Index to consider for splitting\n",
    "        self.decision_feature_index = None\n",
    "        # Exact value from Feature/Attribute to split on\n",
    "        self.decision_threshold_value = None\n",
    "        # Stump importance / weight\n",
    "        self.weight = None\n",
    "        # Stump error\n",
    "        self.error = None\n",
    "        # Left leaf value\n",
    "        self.left_leaf_value = None\n",
    "        # Right leaf value\n",
    "        self.right_leaf_value = None\n",
    "        # Stump decision compartor \n",
    "        self.decision_comparator = None\n",
    "        print(\"New Stump Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBoostingAlgorithm():\n",
    "    def __init__(self, n_learners):\n",
    "        self.n_learners = n_learners\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Store all weak learners (Weak learner -> A decsion stump)\n",
    "        self.learners = []\n",
    "        # Identify each feature type in input X and store as list\n",
    "        self.feature_types = self._determine_type_of_feature(X)\n",
    "        # Concatenate input and output\n",
    "        self.data = np.concatenate((X, np.expand_dims(y, axis=1)), axis=1)\n",
    "        # Initialize weight for each example as 1/N (where N -> total number of examples)\n",
    "        self.sample_weight = np.full(len(self.data), np.divide(1, len(self.data)))        \n",
    "        print(self.feature_types)\n",
    "        print(self.ml_task)\n",
    "        #print(self._get_potential_splits(self.data))\n",
    "        # Iterate and build learners\n",
    "        for i_boost in range(self.n_learners):\n",
    "            # Instantiate a new decision stump object\n",
    "            learner = DecisionStump()\n",
    "            # Find and Perform split over best feature \n",
    "            potential_splits = self._get_potential_splits(self.data)\n",
    "            split_column_index, split_value, metric = self._determine_best_split(self.data, self.sample_weight, potential_splits, self.ml_task)\n",
    "            left_node_data, right_node_data = self._split_data(self.data, self.sample_weight, split_column_index, split_value)\n",
    "            print(f'split_column_index: {split_column_index}, split_value: {split_value}')\n",
    "            print(f'Change in overall_metric: {metric}')\n",
    "            # Compute Leaf values\n",
    "            left_leaf_value = self._create_leaf(left_node_data, self.ml_task)\n",
    "            right_leaf_value = self._create_leaf(right_node_data, self.ml_task)\n",
    "            print(f'Left leaf: {left_leaf_value}, Right leaf: {right_leaf_value}')\n",
    "\n",
    "            # Allocate the instantiated learner with our computed values\n",
    "            learner.decision_feature_index = split_column_index\n",
    "            learner.decision_threshold_value = split_value\n",
    "            learner.left_leaf_value = left_leaf_value\n",
    "            learner.right_leaf_value = right_leaf_value\n",
    "            learner.decision_comparator = self.feature_types[split_column_index]\n",
    "            \n",
    "            print(self.sample_weight[:5])\n",
    "            # Boosting step\n",
    "            self.sample_weight, learner = self.boost(i_boost,\n",
    "                                              self.data,\n",
    "                                              self.sample_weight,\n",
    "                                              learner)\n",
    "            # Early Termination\n",
    "            if self.sample_weight is None:\n",
    "                break\n",
    "            # Stop boosting since error is 0\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if learner.error == 0 or np.sum(self.sample_weight) <= 0:\n",
    "                self.learners.append(learner)\n",
    "                break\n",
    "            print(f'{i_boost}: Sample weight(sum) [Raw] {np.sum(self.sample_weight)}')\n",
    "            # Dont perform operations in below conditional block if we are on final learner\n",
    "            if not i_boost == self.n_learners - 1:\n",
    "                # Normalize\n",
    "                self.sample_weight /= np.sum(self.sample_weight)\n",
    "                \n",
    "                # Note(Alternative): Sample data(examples) based on sample_weight\n",
    "                # Construct new data set sample based on sample_weight\n",
    "#                 self.data = self._sample_data_by_weights(self.data, self.sample_weight)\n",
    "                # Reinitialize equal sample weights for the new data\n",
    "#                 self.sample_weight = np.full(len(self.data), (1 / len(self.data)))\n",
    "            print(f'{i_boost}: Sample weight(sum) [Normalized] {np.sum(self.sample_weight)}')\n",
    "            # Add this learner to our main list of learners\n",
    "            self.learners.append(learner)\n",
    "            print(f'Total stumps: {len(self.learners)}')\n",
    "            \n",
    "        return self  \n",
    "            \n",
    "    def stump_predict(self, data, learner):\n",
    "        \"\"\"\n",
    "        Computes prediction for the passed data examples w.r.t to the learner(descision stump) \n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        feature_column = data[:, learner.decision_feature_index]\n",
    "        for value in feature_column:\n",
    "            if learner.decision_comparator == 'categorical':\n",
    "                if value == learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            else: # continuous\n",
    "                if value <= learner.decision_threshold_value: # Left node\n",
    "                    pred = learner.left_leaf_value\n",
    "                else: # right node\n",
    "                    pred = learner.right_leaf_value\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "\n",
    "    def _gini_sk(self, data):\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        label_column = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        _, value_indexes, counts = np.unique(label_column, return_counts=True, return_index=True)\n",
    "\n",
    "        class_weights = np.array([np.take(data_sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in value_indexes])\n",
    "        \n",
    "        cw = np.sum(class_weights**2)\n",
    "        wn = np.sum(data_sample_weight)**2\n",
    "        gini = 1.0 - (cw/wn)\n",
    "        #print(f\"cw: {cw}, wn: {wn} ---> 1.0 - (cw/wn) == {1.0} - {cw}/{wn}\")\n",
    "        return gini\n",
    "    \n",
    "    def _calculate_weighted_mse(self, data):\n",
    "        \"\"\"\n",
    "        Calculate weighted mean squared error\n",
    "        \"\"\"\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        actual_values = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        \n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "\n",
    "        else:\n",
    "            #prediction = np.mean(actual_values)\n",
    "            prediction = np.average(actual_values, weights=data_sample_weight)\n",
    "            # ! Not normalizing using sum of weighted mean, beacuse the sum of weighted mean is 1\n",
    "            #mse = np.mean((data_sample_weight * (actual_values - prediction))**2)\n",
    "            #mse = np.average((actual_values - prediction)**2, weights=data_sample_weight)\n",
    "            mse = np.mean((actual_values - prediction)**2)\n",
    "\n",
    "        return mse\n",
    "    \n",
    "    \n",
    "    def _mse_sk(self, data):\n",
    "        \"\"\"\n",
    "        Calculate weighted mean squared error\n",
    "        \"\"\"\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        \n",
    "        actual_values = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        \n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "\n",
    "        else:\n",
    "            weighted_y = data_sample_weight * actual_values\n",
    "            sum_total = np.sum(weighted_y)\n",
    "            squared_sum_total = np.sum(weighted_y * actual_values) # w * y * y (or w * y^2)\n",
    "            weights_total = np.sum(data_sample_weight)\n",
    "            \n",
    "            impurity = (squared_sum_total / weights_total) - (sum_total / weights_total)**2\n",
    "            \n",
    "#             #prediction = np.mean(actual_values)\n",
    "#             prediction = np.average(actual_values, weights=data_sample_weight)\n",
    "#             # ! Not normalizing using sum of weighted mean, beacuse the sum of weighted mean is 1\n",
    "#             #mse = np.mean((data_sample_weight * (actual_values - prediction))**2)\n",
    "#             #mse = np.average((actual_values - prediction)**2, weights=data_sample_weight)\n",
    "#             mse = np.mean((actual_values - prediction)**2)\n",
    "\n",
    "        return impurity\n",
    "\n",
    "    def _mse_var(self, data):\n",
    "        if len(data) <= 0:\n",
    "            return None\n",
    "        actual_values = data[:, -2]\n",
    "        data_sample_weight =  data[:, -1]\n",
    "        \n",
    "        if len(actual_values) == 0:   # empty data\n",
    "            mse = 0\n",
    "        else:\n",
    "            mean_prediction = np.average(actual_values, weights=data_sample_weight)\n",
    "            max_dissipation = np.sum((actual_values - mean_prediction)**2)\n",
    "            normalizer = (len(data) - 1)\n",
    "            #print(f\"max_dissipation: {max_dissipation}\")\n",
    "            #print(f\"normalizer: {normalizer}\")\n",
    "            if normalizer == 0:\n",
    "                return None\n",
    "            mse = max_dissipation / normalizer\n",
    "#             weighted_y = data_sample_weight * actual_values\n",
    "#             sum_total = np.sum(weighted_y)\n",
    "#             squared_sum_total = np.sum(weighted_y * actual_values)\n",
    "#             mse = squared_sum_total / np.sum(data_sample_weight)\n",
    "#             mse -= (sum_total / np.sum(data_sample_weight))**2\n",
    "        \n",
    "        return mse\n",
    "        \n",
    "    \n",
    "    def _calculate_weighted_overall_metric(self, data, left_node_data, right_node_data, metric_function):\n",
    "        \"\"\"\n",
    "        Generalized impurity metric, computes weighted overall\n",
    "        impurity/error w.r.t left and right nodes\n",
    "        \"\"\"\n",
    "        # Labels\n",
    "        left_label_column = left_node_data[:, -2]\n",
    "        right_label_column = right_node_data[:, -2]\n",
    "        parent_label_column = data[:, -2]\n",
    "        # Sample weights\n",
    "        left_sample_weight = left_node_data[:, -1]\n",
    "        right_sample_weight = right_node_data[:, -1]\n",
    "        parent_sample_weight = data[:, -1]\n",
    "        \n",
    "        if self.ml_task == 'classification':\n",
    "            _, left_value_indexes, left_counts = np.unique(left_label_column, return_counts=True, return_index=True)\n",
    "            _, right_value_indexes, right_counts = np.unique(right_label_column, return_counts=True, return_index=True)\n",
    "            \n",
    "            aggregated_left_class_weights = np.array([np.take(left_sample_weight, np.where(left_label_column == left_label_column[value_index])[0]).sum() for value_index in left_value_indexes])\n",
    "            aggregated_right_class_weights = np.array([np.take(right_sample_weight, np.where(right_label_column == right_label_column[value_index])[0]).sum() for value_index in right_value_indexes])\n",
    "            \n",
    "            weighted_prob_node_left = np.sum(aggregated_left_class_weights)\n",
    "            weighted_prob_node_right = np.sum(aggregated_right_class_weights)\n",
    "            \n",
    "        else:\n",
    "            #total_parent_sample_weight = np.sum(np.sum(left_sample_weight), np.sum(right_sample_weight))\n",
    "            # Weighted probabilities of left and right node\n",
    "            weighted_prob_node_left = np.sum(left_sample_weight)# / total_parent_sample_weight\n",
    "            weighted_prob_node_right = np.sum(right_sample_weight)# / total_parent_sample_weight\n",
    "        \n",
    "        left_impurity = metric_function(left_node_data)\n",
    "        right_impurity = metric_function(right_node_data)\n",
    "        \n",
    "        if left_impurity != None and right_impurity != None:\n",
    "            if self.ml_task == 'classification': \n",
    "                overall_metric =  weighted_prob_node_left * left_impurity + weighted_prob_node_right * right_impurity\n",
    "            else:\n",
    "                overall_metric = left_impurity + right_impurity\n",
    "            #print(f'weighted_prob_node_left * w_i(left_node_data)): {weighted_prob_node_left} * {left_impurity} = {(weighted_prob_node_left * left_impurity)}')\n",
    "            #print(f'weighted_prob_node_right * w_i(right_node_data)): {weighted_prob_node_right} * {right_impurity} = {(weighted_prob_node_right * right_impurity)}')\n",
    "            return overall_metric\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Alternative method to weighted loss: Which works by random sampling of example from a distribution based on sample_weight\n",
    "    def _sample_data_by_weights(self, data, sample_weight):\n",
    "        \"\"\"\n",
    "        Construct an new input, iteratively sampled over distribution \n",
    "        formed by passed sample_weight.\n",
    "\n",
    "        Note: \n",
    "        Learn more about this technique: https://youtu.be/LsK-xG1cLYA (Statquest)\n",
    "        \"\"\"\n",
    "        n_samples, _ = np.shape(data)\n",
    "        # Intialize array to hold sampled index  \n",
    "        sampled_indices = []\n",
    "        # Perform cumulative summation over sample_weight to create buckets\n",
    "        sample_weight_buckets = np.cumsum(sample_weight)\n",
    "        # Keeping sampling 'n_samples' times\n",
    "        for _ in range(n_samples):\n",
    "            # Generate a random number between 0 and 1\n",
    "            random_num = np.random.random_sample()\n",
    "            # Find the bucket Eg: weight buckets [0.33, 0.66, 0.99] and random number = 0.47\n",
    "            # then index 1 will be selected (since cumsum value is 0.66)\n",
    "            bucket_index = np.where(sample_weight_buckets > random_num)[0][0]\n",
    "\n",
    "            sampled_indices.append(bucket_index)\n",
    "        # finally construct weighted data using sampled_indexes\n",
    "        weighted_data = data[sampled_indices]\n",
    "\n",
    "        return weighted_data\n",
    "\n",
    "\n",
    "    def _get_potential_splits(self, data):\n",
    "        \"\"\"\n",
    "        Get all potential splits for each feature\n",
    "        Splits can be made on each unique value\n",
    "        Can essentially make a split at each unique value\n",
    "        \n",
    "        \"\"\"\n",
    "        potential_splits = {}\n",
    "        _, n_columns = data.shape\n",
    "        for column_index in range(n_columns - 1): # excluding the last column which is the label\n",
    "            values = data[:, column_index]\n",
    "            unique_values = np.unique(values)\n",
    "\n",
    "            potential_splits[column_index] = unique_values\n",
    "            \n",
    "        return potential_splits\n",
    "    \n",
    "    \n",
    "    def _determine_best_split(self, data, sample_weight, potential_splits, ml_task):\n",
    "        \"\"\"\n",
    "        Iterate over each column_index (as keys) in potential_split (dict)\n",
    "        Perform split(of examples) over each unique value and evaluate the split\n",
    "        Identify the best split and return its feature index and value\n",
    "        \"\"\"\n",
    "        # Stitch data with sample_weight towards the end\n",
    "        data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        \n",
    "        # Best minimum gini index to be updated iteratively\n",
    "        best_overall_metric = float('inf')\n",
    "        \n",
    "        for column_index in potential_splits:\n",
    "            #print(f\"COLUMN {column_index}\")\n",
    "            for value in potential_splits[column_index]:\n",
    "                #print(f'column_index: {column_index}, value: {value}')\n",
    "                left_node_data, right_node_data = self._split_data(data, None, split_column_index=column_index, split_value=value)\n",
    "\n",
    "                if ml_task == \"regression\":\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._mse_var)\n",
    "                else: # classification\n",
    "                    current_overall_metric = self._calculate_weighted_overall_metric(data, left_node_data, right_node_data,\n",
    "                                                                                     metric_function=self._gini_sk)\n",
    "\n",
    "                # If a lower overall_metric is achieved update the index and value with the current\n",
    "                if current_overall_metric != None and current_overall_metric <= best_overall_metric:\n",
    "                    best_overall_metric = current_overall_metric\n",
    "                    best_split_column_index = column_index\n",
    "                    best_split_value = value\n",
    "                #print(f'best_overall_metric: {best_overall_metric}')\n",
    "                #print('---')\n",
    "            #print(f'Debug [1]: Best: {best_overall_metric}, index: {best_split_column_index}, value: {best_split_value}')\n",
    "        return best_split_column_index, best_split_value, best_overall_metric\n",
    "    \n",
    "    \n",
    "    def _split_data(self, data, sample_weight, split_column_index, split_value):\n",
    "        \"\"\" \n",
    "        Split data(examples) based on best split_column_index and split_value\n",
    "        estimated using task specific splitting metric.\n",
    "        \"\"\"\n",
    "        # Stich sample_weight to data if passed as an argument or the assumption is, it has already been stiched/appended(in _determine_best_split())\n",
    "        if sample_weight is not None:\n",
    "            # Stitch data with sample_weight towards the end (axis=1)\n",
    "            data = np.concatenate((data, np.expand_dims(sample_weight, axis=1)), axis=1)\n",
    "        #else it is already appended \n",
    "        \n",
    "        # Get values(from feature column) for the passed split_column index\n",
    "        split_column_values = data[:, split_column_index]\n",
    "        \n",
    "        type_of_feature = self.feature_types[split_column_index]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            left_node_data = data[split_column_values <= split_value]\n",
    "            right_node_data = data[split_column_values >  split_value]\n",
    "\n",
    "        # feature is categorical   \n",
    "        else:\n",
    "            left_node_data = data[split_column_values == split_value]\n",
    "            right_node_data = data[split_column_values != split_value]\n",
    "        return left_node_data, right_node_data\n",
    "    \n",
    "    \n",
    "    def _create_leaf(self, data, ml_task):\n",
    "        \"\"\"\n",
    "        Create leaf node, with leaf value based on ml_task\n",
    "        for,\n",
    "        Classfication: consider majority vote\n",
    "        Regression: consider the mean value\n",
    "        \"\"\"\n",
    "        label_column = data[:, -2]\n",
    "        sample_weight = data[:, -1]\n",
    "        \n",
    "        if ml_task == \"regression\":\n",
    "            #leaf = np.mean(label_column)\n",
    "            #print(label_column)\n",
    "            leaf = np.average(label_column, weights=sample_weight)\n",
    "\n",
    "        # classfication    \n",
    "        else:  \n",
    "            # Decide leaf value based on sum of weights for each class in the node\n",
    "            unique_classes, unique_cls_start_indices, counts_unique_classes = np.unique(label_column, return_counts=True, return_index=True)\n",
    "            unique_class_aggregated_weights = np.array([np.take(sample_weight, np.where(label_column == label_column[value_index])[0]).sum() for value_index in unique_cls_start_indices])\n",
    "            index = unique_class_aggregated_weights.argmax()\n",
    "            leaf = unique_classes[index]\n",
    "\n",
    "        return leaf\n",
    "    \n",
    "    \n",
    "    def _determine_type_of_feature(self, X):\n",
    "        \"\"\"\n",
    "        Determine, if the feature is categorical or continuous\n",
    "        \"\"\"\n",
    "        feature_types = []\n",
    "        n_unique_values_treshold = 15 # Threshold for a numeric feature to be categorical\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        \n",
    "        for feature_i in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature_i])\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "\n",
    "        return feature_types\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier(BaseBoostingAlgorithm):\n",
    "    def __init__(self, n_learners=20):\n",
    "        # Set total number of weak learners\n",
    "        super().__init__(n_learners)\n",
    "        self.ml_task = \"classification\"\n",
    "        self.classes = None\n",
    "        self.n_classes = None\n",
    "        \n",
    "    def boost(self, i_boost, data, sample_weight, learner):\n",
    "        \"\"\"\n",
    "        Compute learner importance and error, along with boosted weights for each example \n",
    "        \"\"\"\n",
    "        print(f'Boost Called')\n",
    "        \n",
    "        # If its first boost initialize number of classes(n_classes)\n",
    "        if i_boost == 0:\n",
    "            self.classes = np.unique(data[:, -1])\n",
    "            self.n_classes = self.classes.size\n",
    "            \n",
    "        # Perform predictions\n",
    "        preds = self.stump_predict(data, learner)\n",
    "        \n",
    "        # Incorrectly classified examples\n",
    "        incorrect = preds != data[:, -1]\n",
    "\n",
    "        # Learner Error\n",
    "        learner_error = np.mean(np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        # Stop if classification is perfect\n",
    "        if learner_error <= 0:\n",
    "            learner.weight = 1\n",
    "            learner.error = 0\n",
    "            return sample_weight, learner\n",
    "        print(f'Learner error: {learner_error}')\n",
    "        \n",
    "        # Learner weight\n",
    "        learner_weight = (np.log((1. - learner_error) / (learner_error)) +\n",
    "                        np.log(self.n_classes - 1.))\n",
    "        print(f'Learner weight: {learner_weight}')\n",
    "        \n",
    "        # Boost sample_weight for each each sample\n",
    "        # Dont boost sample_weight if we are on final learner\n",
    "        if not i_boost == self.n_learners - 1:\n",
    "            # Boost only positive weights\n",
    "            sample_weight *= np.exp(learner_weight * incorrect *\n",
    "                                    ((sample_weight > 0) | (learner_weight < 0)))\n",
    "\n",
    "        # Allocate learner its computed weight and error\n",
    "        learner.weight = learner_weight\n",
    "        learner.error = learner_error\n",
    "\n",
    "        # Finally return sample weights and boosted learner\n",
    "        return sample_weight, learner\n",
    "         \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for X.\n",
    "        \"\"\"\n",
    "        print(f'Predict called')\n",
    "        \n",
    "        # Get activated matrix for with respect to each learner [get vote of each learner]\n",
    "        # Add each activated matrix (matrix addition) [get overall vote of all leaners]\n",
    "        # return the overall matrix\n",
    "        # Argmax is used over each row of overall matrix to figure our the class\n",
    "        classes = self.classes[:, np.newaxis]\n",
    "        pred = sum((self.stump_predict(X, learner) == classes).T * learner.weight\n",
    "                   for learner in self.learners)\n",
    "        # Normalize \n",
    "        learner_weights = sum(learner.weight for learner in self.learners)\n",
    "        pred /= learner_weights\n",
    "        \n",
    "        # If its binary classification obatin the form [-, +], convienient to select classes with np.take() \n",
    "        # Eg(binary): classes =  [[c1], [c2]] and pred = [True, False, True], below output: [[c2], [c1], [c2]]\n",
    "        if self.n_classes == 2:\n",
    "            pred[:, 0] *= -1\n",
    "            pred = pred.sum(axis=1)\n",
    "            return classes.take(pred > 0, axis=0)\n",
    "        # Finds index of column with max value, and uses this index to select class from classes\n",
    "\n",
    "        return classes.take(np.argmax(pred, axis=1), axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostRegressor(BaseBoostingAlgorithm):\n",
    "    def __init__(self, n_learners=20):\n",
    "        # Set total number of weak learners\n",
    "        super().__init__(n_learners)\n",
    "        self.ml_task = \"regression\"\n",
    "        \n",
    "    def boost(self, i_boost, data, sample_weight, learner):\n",
    "        \"\"\"\n",
    "        Compute learner importance and error, along with boosted weights for each example \n",
    "        \"\"\"\n",
    "        print(f'Boost Called')\n",
    "\n",
    "        # Perform predictions\n",
    "        preds = self.stump_predict(data, learner)\n",
    "        \n",
    "        error_vect = np.abs(preds - data[:, -1]) # Absolute residual\n",
    "        error_max = error_vect.max() # Max error value for current split\n",
    "        \n",
    "        # Normalize error vector with max error\n",
    "        if error_max != 0.:\n",
    "            error_vect /= error_max\n",
    "        # Calculate the average loss \n",
    "        # sum(Sample_weight * residual) # error value between 0<->1\n",
    "        learner_error = (sample_weight * error_vect).sum() \n",
    "        # Return if error is 0 or less # Nothing to improve\n",
    "        mse_error = accuracy_score_regression(reg_y_df.values, preds)\n",
    "        print(f\"mse_error: {mse_error}\")\n",
    "        if learner_error <= 0:\n",
    "            learner.weight = 1\n",
    "            learner.error = 0\n",
    "            return sample_weight, learner\n",
    "        print(f'Learner error: {learner_error}')\n",
    "        # Eg: Learner error\n",
    "        # learner_error = 0 (no error-> sum of residuals = 0)\n",
    "        # learner_error = 0.5 (average error -> (sum of residuals)/(error_max*n))\n",
    "        # Learner_error = 1 ((sum of residuals) == (error_max*n))\n",
    "        \n",
    "        # AdaBoost.R2\n",
    "        # Eg: Beta\n",
    "        # beta 1 or greater -> when learner error >= 0.5 and close to 1 (high error)\n",
    "        # beta less than 1 -> when learner error < 0.5 and close to 0 (low error)\n",
    "        beta = learner_error / (1. - learner_error)\n",
    "        print(f'Beta: {beta}')\n",
    "        # Eg: learner_weight\n",
    "        # beta 1 or greater -> learner_weight is less/close to 0 (because of high error)\n",
    "        # beta less than 1 -> learner_weight is more/away from 0 (bcause of low error) \n",
    "        learner_weight = np.log(1. / beta)\n",
    "        print(f'Learner weight: {learner_weight}')\n",
    "        \n",
    "        if not i_boost == self.n_learners - 1:\n",
    "            # raise beta using Normazlize error for each respective example(vector)\n",
    "            sample_weight = sample_weight * np.power(beta, (1. - error_vect))\n",
    "        \n",
    "        # Allocate learner its computed weight and error\n",
    "        learner.weight = learner_weight\n",
    "        learner.error = learner_error\n",
    "\n",
    "        # Finally return sample weights and boosted learner\n",
    "        return sample_weight, learner\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        - Predictions are taken from each learner\n",
    "        - These Predictions are ordered/sorted according to their magnitude to obtain their sorted indices\n",
    "        - Also Learner weights are sorted according to previously sorted predictions\n",
    "          for performing cummulutive sum over previously sorted learner weights for each example is performed\n",
    "          for these respective rolling sum, weights >=0.5 are marked as true \n",
    "        - Index is noted when first 0.5 crossing is obtained, this index is \n",
    "          used to select a prediction from our orginal unsorted predictions \n",
    "        \"\"\"\n",
    "        print(f'Predict called')\n",
    "        # Get predictions for each each example w.r.t each learner\n",
    "        preds = np.array([self.stump_predict(X, learner) for learner in self.learners]).T\n",
    "        \n",
    "        # sort predictions(given by learners) for each example\n",
    "        preds_sorted_idx = np.argsort(preds, axis=1)\n",
    "        \n",
    "        # Get weights of all the learners \n",
    "        learner_weights = np.array([learner.weight for learner in self.learners])\n",
    "        # Order learner weights according to preds_sorted_idx and perform cumsum()\n",
    "        weight_cdf = np.cumsum(learner_weights[preds_sorted_idx], axis=1)\n",
    "        # for each example's prediction weights_cdf, when in crosses 0.5(inc) mark as true\n",
    "        thresholded_weights = (weight_cdf >= 0.5) * weight_cdf[:, -1][:, np.newaxis]\n",
    "        median_weight_idx = thresholded_weights.argmax(axis=1)\n",
    "        \n",
    "        median_leaners = preds_sorted_idx[np.arange(len(X)), median_weight_idx]\n",
    "        \n",
    "        return preds[np.arange(len(X)), median_leaners]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " var = \\sum_i^n (y_i - y_bar) ** 2\n",
    "     = (\\sum_i^n y_i ** 2) - n_samples * y_bar ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = AdaBoostClassifier(20)\n",
    "# clf = clf.fit(df.iloc[:, :-1].values, df.iloc[:, -1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = clf.predict(df.iloc[:, :-1].values)\n",
    "# accuracy_score_classification(df.iloc[:, -1].values, predictions[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['categorical', 'categorical', 'categorical', 'categorical', 'categorical', 'categorical', 'categorical', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'continuous', 'categorical', 'continuous', 'categorical', 'categorical', 'categorical', 'categorical', 'categorical']\n",
      "regression\n",
      "New Stump Created!\n",
      "split_column_index: 8, split_value: 0.238461\n",
      "Change in overall_metric: 3720239.552559671\n",
      "Left leaf: 1592.833333333333, Right leaf: 4736.581979320478\n",
      "[0.00136799 0.00136799 0.00136799 0.00136799 0.00136799]\n",
      "Boost Called\n",
      "mse_error: 1752.5706950971967\n",
      "Learner error: 0.29591362726202547\n",
      "Beta: 0.42028029332723593\n",
      "Learner weight: 0.8668334252289138\n",
      "0: Sample weight(sum) [Raw] 0.55394478461488\n",
      "0: Sample weight(sum) [Normalized] 1.0\n",
      "Total stumps: 1\n",
      "New Stump Created!\n",
      "split_column_index: 8, split_value: 0.238461\n",
      "Change in overall_metric: 3720751.6701198043\n",
      "Left leaf: 1611.68314633051, Right leaf: 4724.339580272245\n",
      "[0.0020688083857496467 0.0021399948610897196 0.0010854887990750278\n",
      " 0.0010437998353995846 0.0010392677068203752]\n",
      "Boost Called\n",
      "mse_error: 1752.6177831246794\n",
      "Learner error: 0.34274142005568403\n",
      "Beta: 0.5214711994854774\n",
      "Learner weight: 0.6511012323645599\n",
      "1: Sample weight(sum) [Raw] 0.6596476148977332\n",
      "1: Sample weight(sum) [Normalized] 0.9999999999999996\n",
      "Total stumps: 2\n",
      "New Stump Created!\n",
      "split_column_index: 8, split_value: 0.238461\n",
      "Change in overall_metric: 3722305.634812942\n",
      "Left leaf: 1627.5438509166136, Right leaf: 4707.645995277368\n",
      "[0.00274472852954742 0.0029124366203078407 0.0008898971590251252\n",
      " 0.0008308510879191103 0.0008229023756289577]\n",
      "Boost Called\n",
      "mse_error: 1752.8172986254565\n",
      "Learner error: 0.3808185147506091\n",
      "Beta: 0.6150353714100881\n",
      "Learner weight: 0.48607549834156577\n",
      "2: Sample weight(sum) [Raw] 0.7451766586910394\n",
      "2: Sample weight(sum) [Normalized] 0.9999999999999992\n",
      "Total stumps: 3\n",
      "New Stump Created!\n",
      "split_column_index: 7, split_value: 0.23333299999999998\n",
      "Change in overall_metric: 3722485.664987906\n",
      "Left leaf: 1628.5071692032311, Right leaf: 4741.520215240761\n",
      "[0.003333148238072199 0.0036049676281340475 0.0007560138608699625\n",
      " 0.0006904257101492741 0.0006811301189713603]\n",
      "Boost Called\n",
      "mse_error: 1749.607753819535\n",
      "Learner error: 0.39802941632623046\n",
      "Beta: 0.6612107420550263\n",
      "Learner weight: 0.4136826668650201\n",
      "3: Sample weight(sum) [Raw] 0.7833786912133677\n",
      "3: Sample weight(sum) [Normalized] 0.9999999999999996\n",
      "Total stumps: 4\n",
      "New Stump Created!\n",
      "split_column_index: 7, split_value: 0.23333299999999998\n",
      "Change in overall_metric: 3723110.7436694787\n",
      "Left leaf: 1641.3729845944247, Right leaf: 4722.103321957005\n",
      "[0.003910424637595013 0.004298085709361414 0.0006539401906973555\n",
      " 0.0005861609199005641 0.0005763461899437564]\n",
      "Boost Called\n",
      "mse_error: 1749.7265293247633\n",
      "Learner error: 0.42342608137324855\n",
      "Beta: 0.7343829953004793\n",
      "Learner weight: 0.3087245944912046\n",
      "4: Sample weight(sum) [Raw] 0.8392340669936896\n",
      "4: Sample weight(sum) [Normalized] 0.9999999999999991\n",
      "Total stumps: 5\n",
      "New Stump Created!\n",
      "split_column_index: 7, split_value: 0.23333299999999998\n",
      "Change in overall_metric: 3724368.410836301\n",
      "Left leaf: 1651.1778935124446, Right leaf: 4706.641616867531\n",
      "[0.0043739097593824255 0.004865974643006624 0.0005833348297877028\n",
      " 0.0005156091301685769 0.0005057118838361812]\n",
      "Boost Called\n",
      "mse_error: 1749.9681163172331\n",
      "Learner error: 0.44262344370224227\n",
      "Beta: 0.7941192335792953\n",
      "Learner weight: 0.23052166077323394\n",
      "5: Sample weight(sum) [Raw] 0.9999999999999991\n",
      "5: Sample weight(sum) [Normalized] 0.9999999999999991\n",
      "Total stumps: 6\n"
     ]
    }
   ],
   "source": [
    "reg = AdaBoostRegressor(6)\n",
    "reg = reg.fit(reg_X_df.iloc[:, 1:-1].values, reg_y_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict called\n"
     ]
    }
   ],
   "source": [
    "predictions = reg.predict(reg_X_df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1748.6008254818623"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_regression(reg_y_df.values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1737979154665379"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(reg_y_df.values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
