{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch [Multilayer Perceptron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to plot neural network\n",
    "import matplotlib.pyplot as plt\n",
    "import draw_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data [Classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = pd.read_csv(\"Iris.csv\")\n",
    "clf_df = clf_df.drop(\"Id\", axis=1)\n",
    "clf_df = clf_df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_X_df = clf_df.iloc[:, :-1]\n",
    "clf_y_df = clf_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_X_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Iris-setosa\n",
       "1    Iris-setosa\n",
       "2    Iris-setosa\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_y_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "\n",
    "Input: **(Done)**\n",
    "- batch (as seperate argument) full/minibatch **(Done)**\n",
    "- single/batch/minibatch generate indices (random) **(Done)**\n",
    "- sample using indices **(Done)**\n",
    "\n",
    "Output: **(Done)**\n",
    "- output labelencoder **(Done)**\n",
    "- save actual labels **(Done)**\n",
    "- one hot encoding of labels **(Done)**\n",
    "\n",
    "Weight Initializations:\n",
    "- Random **(Done)**\n",
    "- Zeros **(Done)**\n",
    "- Xavier **(Done)**\n",
    "- He **(Done)**\n",
    "\n",
    "Activation functions:\n",
    "- sigmoid **(Done)**\n",
    "- sigmoid derivative **(Done)**\n",
    "- relu **(Done)**\n",
    "- relu derivative **(Done)**\n",
    "- tanh **(Done)**\n",
    "- tanh derivative **(Done)**\n",
    "- softmax **(Done)**\n",
    "- softmax derivative **(Done)**\n",
    "\n",
    "Loss computation:\n",
    "- binary_crossentropy **(Done)**\n",
    "- binary_crossentropy derivative **(Done)**\n",
    "- categorical_crossentropy **(Done)**\n",
    "- categorical_crossentropy derivative **(Done)**\n",
    "\n",
    "Forward propagation:\n",
    "- iterative forward pass **(Done)**\n",
    "\n",
    "Back propagation:\n",
    "- chain builder - Parameter respective,for each training example, negative gradient **(Done)**\n",
    "\n",
    "Parameter update:\n",
    "- average derivative respective to each parameter for all training examples **(Done)**\n",
    "- update old weights by adding **(Done)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliLayerPerceptronClassifer():\n",
    "    def __init__(self, random_state=False):\n",
    "        # RNG seed\n",
    "        if not random_state:\n",
    "            np.random.seed(20)\n",
    "        # network config vars\n",
    "        self.layers_config = []\n",
    "        self.network_structure = None\n",
    "        self.loss_function = None\n",
    "        self.optimizer = \"default\"\n",
    "        self.parameter_initializer = \"random\"\n",
    "        self.history = []\n",
    "        self.input_scaling_values = {}\n",
    "        self.batch_size = 0\n",
    "        #self.a_validator = []\n",
    "        \n",
    "        self.supported_network_settings = {\n",
    "            \"parameter_initializer\": [\"random\", \"zeros\", \"xavier\", \"he\"],\n",
    "            \"activation\": [\"sigmoid\", \"tanh\", \"relu\", \"softmax\", \"linear\"],\n",
    "            \"loss\": [\"binary_crossentropy\", \"categorical_crossentropy\"],\n",
    "            \"optimizer\": [\"default\",\n",
    "                         \"momentum\",\n",
    "                         \"nag\",\n",
    "                         \"adagrad\",\n",
    "                         \"adadelta\",\n",
    "                         \"rmsprop\",\n",
    "                         \"adam\",\n",
    "                         \"adamax\",\n",
    "                         \"nadam\",\n",
    "                         \"amsgrad\"]\n",
    "        }\n",
    "         \n",
    "        \n",
    "    '''\n",
    "    Setup training input and output(generate onehot for binary/multiclass/multilabel target)\n",
    "    '''\n",
    "    \n",
    "    def generate_batch_indices(self, input_examples_count, batch_size, final_batch_threshold=0.5, random=True):\n",
    "        row_indices = np.arange(start=0, stop=input_examples_count) # array([0, 1, ....., len(input_examples_count)-1 ])\n",
    "\n",
    "        if random:\n",
    "            np.random.shuffle(row_indices)\n",
    "\n",
    "        # Validate batch sizes\n",
    "        if batch_size: # 'Stochastic | mini-batch' batch sizes\n",
    "            if (batch_size < 1) | (batch_size > len(row_indices)): # Batch size should be between (0, len(row_indices)]\n",
    "                # raise an error \n",
    "                raise Exception(f\"Batch size violated the input size bounds: Batch size should be between (0, {len(data)}]\")\n",
    "        else:\n",
    "            batch_size = len(row_indices) # entire dataset as batch\n",
    "\n",
    "        # Validate if batch_size evenly splits data if not then mark batch_evenly_divides_input as false\n",
    "        batch_evenly_divides_input = False\n",
    "        if (len(row_indices) % batch_size) == 0:\n",
    "            batch_evenly_divides_input = True    \n",
    "        batches_count = int(len(row_indices)/batch_size) # Number of batches \n",
    "        batchable_rows_count = batches_count * batch_size # Number of rows which will be addressed by batches\n",
    "\n",
    "        # Store batch indices\n",
    "        batch_indices = np.stack(np.array_split(row_indices[:batchable_rows_count], batches_count), axis=0).tolist()\n",
    "        \n",
    "        # if some rows remain(batch_size didnt evenly divide len(row_indices)), make one final batch of input using\n",
    "        # remaining rows + randomly sampled rows from row_indices until size of new final batch equals batch_size\n",
    "        if not batch_evenly_divides_input:\n",
    "            remaining_row_indices = row_indices[batchable_rows_count:]\n",
    "            # If remining rows count are less than 50% (final_batch_threshold) of batch_size discard creation of final batch\n",
    "            if len(remaining_row_indices) > (batch_size * final_batch_threshold):\n",
    "                # compute how many more rows are required\n",
    "                required_row_indices_count = batch_size - len(remaining_row_indices)\n",
    "                # randomly sample about 'required_row_indices_count' rows from row_indices without replacement(to avoid selecting a row multiple times)\n",
    "                sampled_rows_indices = row_indices[np.random.choice(len(row_indices), required_row_indices_count, replace=False)]\n",
    "                # Stack remaining rows and sampled_rows within a single batch\n",
    "                final_batch = np.concatenate([remaining_row_indices, sampled_rows_indices]).tolist()\n",
    "                # Finally stack final batch to data_collated\n",
    "                batch_indices.append(final_batch)\n",
    "\n",
    "        return batch_indices\n",
    "    \n",
    "\n",
    "    def standardizer(self, x, isTrain=True): # Normalize with mean 0 and std-dev of 1, also called z-score\n",
    "        if isTrain:\n",
    "            x_mean = np.mean(x, axis=0)\n",
    "            x_stdev = np.std(x, axis=0)\n",
    "            # Save mean and standard deviation\n",
    "            self.input_scaling_values['standardizer_mean'] = x_mean\n",
    "            self.input_scaling_values['standardizer_stdev'] = x_stdev\n",
    "        stdz_x = np.divide((x - self.input_scaling_values['standardizer_mean']), self.input_scaling_values['standardizer_stdev'],\n",
    "                           where=self.input_scaling_values['standardizer_stdev']!=0)\n",
    "        return stdz_x\n",
    "    \n",
    "    \n",
    "    def one_hot_encode_labels(self, y):\n",
    "        \"\"\"\n",
    "        Creates one hot encodings for y w.r.t self.y_unique_labels (Supports Multiclass + Multilabel)\n",
    "        Eg: y = np.array([['b'], ['a', 'c', 'd'], ['a'], ['c', 'b']]) and self.y_unique_labels = np.array(['a', 'b', 'c', 'd'])\n",
    "            one_hot_encoded_y = np.array([[0, 1, 0, 0], [1, 0, 1, 1], [1, 0, 0, 0], [0, 1, 1, 0]])\n",
    "        \n",
    "        NOTE: Here I found np.vectorize() to run quicker than a custom matrix function, takes exactly 0.00007(secs) for an input of 6 rows,\n",
    "        Whereas a custom function requires knowing count of multilabels and does padding to create mask which alone takes 0.0008(secs on avg),\n",
    "        let alone additional steps like masking and one hot encoding will add up to time, So vectorize is best for this case\n",
    "        \"\"\"\n",
    "        def one_hot_vector(target):\n",
    "            return np.in1d(self.y_unique_labels, target, assume_unique=\"True\")\n",
    "        one_hot_vectorizer = np.vectorize(one_hot_vector, otypes=[np.ndarray])\n",
    "        one_hot_encoded_y = np.stack(np.hstack(one_hot_vectorizer(y))).astype(int)\n",
    "        return one_hot_encoded_y\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Network configure\n",
    "    '''    \n",
    "     \n",
    "    def add_layer(self, units, activation=\"relu\", input_units=None):\n",
    "        # Validate activation value\n",
    "        self.validate_settings(setting_type=\"activation\", setting_value=activation)\n",
    "        \n",
    "        layers_count = len(self.layers_config)\n",
    "        # check if this is the first layer \n",
    "        if (layers_count == 0):\n",
    "            # Make sure input_dim is specified\n",
    "            if (input_units == None):\n",
    "                # We need input_dim as this is the first hidden layer\n",
    "                raise Exception(\"Please specify parameter value for 'input_units'(eg: input_units=4), as this is the first hidden layer\")\n",
    "            else:\n",
    "                # First create the input layer\n",
    "                self.layers_config.append((layers_count, input_units, None))\n",
    "                # Next add the hidden layer\n",
    "                self.layers_config.append((layers_count+1, units, activation))\n",
    "        else:  \n",
    "            self.layers_config.append((layers_count, units, activation))\n",
    "            \n",
    "            \n",
    "    '''\n",
    "    Network compiler & Parameter initializer\n",
    "    '''\n",
    "    \n",
    "    def validate_settings(self, setting_type, setting_value):\n",
    "        if setting_value not in self.supported_network_settings[setting_type]:\n",
    "            raise Exception(f\"{setting_type}='{setting_value}' is not supported, please specify {setting_type} as any one from the following list {self.supported_network_settings[setting_type]}\")            \n",
    "            \n",
    "    \n",
    "    def compile_network(self, loss, optimizer=None, param_initializer=None):\n",
    "        # Validate loss type value\n",
    "        self.validate_settings(setting_type=\"loss\", setting_value=loss)\n",
    "        # Validate optimizer type value\n",
    "        if optimizer is not None: # If None then use entire dataset as one epoch\n",
    "            self.validate_settings(setting_type=\"optimizer\", setting_value=optimizer)\n",
    "        # Validate initializer type value\n",
    "        if param_initializer is not None:\n",
    "            self.validate_settings(setting_type=\"parameter_initializer\", setting_value=param_initializer)\n",
    "            \n",
    "        # Setup layers\n",
    "        self.network_structure = list(map(tuple, np.array(self.layers_config)[:, :-1])) # Ignore the last columns as it specifies the type of activation function\n",
    "        # Save network settings\n",
    "        self.loss_function = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.parameter_initializer = param_initializer\n",
    "        # initialize and store Weights and biases with according to parameter_initializer\n",
    "        self.layer_weight_matrices = []\n",
    "        self.layer_biases = []\n",
    "        # Initialize optimizer parameters containers, as required by different optimizers\n",
    "        self.past_weight_updates = []\n",
    "        self.past_bias_updates = []\n",
    "        for layers_number in range(1, len(self.network_structure)):\n",
    "            layer_a_units = self.network_structure[layers_number-1][1]\n",
    "            layer_b_units = self.network_structure[layers_number][1]\n",
    "            weight_matrix, bias_matrix = self.generate_intial_parameters(self.parameter_initializer, layer_a_units, layer_b_units)\n",
    "            # append matrix to list layer_weight_matrices and layer_biases lists\n",
    "            self.layer_weight_matrices.append(weight_matrix)\n",
    "            self.layer_biases.append(bias_matrix)\n",
    "            # Initialize initial values into optimizer parameter containers\n",
    "            self.past_weight_updates.append(0)\n",
    "            self.past_bias_updates.append(0)\n",
    "            \n",
    "            \n",
    "    '''\n",
    "    Activation Functions\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_activation(self, z):\n",
    "        # Plain and simple sigmoid function, function returns prob for z(z is weighted sum of input)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def tanh_activation(self, z):\n",
    "        return np.tanh(z) #  (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def relu_activation(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    \n",
    "    def linear_activation(self, z):\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def softmax_activation(self, z_vector):\n",
    "        # Normalizing the inputs to be not too large or too small, to address prob of overshooting when performing exponentiations of large numbers(violates floating point limit)\n",
    "        # Shift the inputs to a range close to zero and less than zero\n",
    "        # large negative exponents \"saturate\" to zero rather than infinity, so we have a better chance of avoiding NaNs.\n",
    "        shiftz = z_vector - np.max(z_vector)\n",
    "        exps = np.exp(shiftz)\n",
    "        return exps / np.sum(exps) \n",
    "    \n",
    "    '''\n",
    "    Derivatives of Activation Functions\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_derivative(self, sig_z): # sig_z = sig(w.a + b) (vectors w,a represent weight,activations)\n",
    "        # Differentiate sigmoid function w.r.t to z to obtain sig_z * (1 - sig_z)\n",
    "        return sig_z * (1.0 - sig_z)\n",
    "    \n",
    "    \n",
    "    def tanh_derivative(self, tanh_z): \n",
    "        return 1.0 - tanh_z**2\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, rel_z):\n",
    "        return 1.0 * (rel_z > 0)\n",
    "    \n",
    "    \n",
    "    def linear_derivative(self, lin_z):\n",
    "        return 1.0\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Loss functions\n",
    "    '''\n",
    "    \n",
    "    def binary_crossentropy(self, y, y_hat):\n",
    "        return np.sum(y*np.log(y_hat + 1e-8) + (1-y)*np.log(1 - y_hat + 1e-8)) * -(1.0/len(y))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_hat):\n",
    "        return -np.mean(np.sum(y*np.log(y_hat + 1e-8), axis=1))\n",
    "    \n",
    "    '''\n",
    "    Derivatives of Loss functions(cross entorpy) w.r.t to logits (dJ/dZ)\n",
    "    '''\n",
    "    \n",
    "    def binary_crossentropy_derivative(self, activation_matrix, y):\n",
    "        return activation_matrix - y\n",
    "    \n",
    "    \n",
    "    def categorical_crossentropy_derivative(self, activation_matrix, y):\n",
    "        return activation_matrix - y\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Function Invokers\n",
    "    '''\n",
    "    \n",
    "    def generate_intial_parameters(self, param_initializer, layer_a_units, layer_b_units):\n",
    "        if param_initializer == \"zeros\":\n",
    "            weight_matrix = np.zeros((layer_a_units, layer_b_units))\n",
    "            bias_matrix = np.zeros((1, layer_b_units))\n",
    "        elif param_initializer == \"xavier\": # Adjust the variance down by multiplying with (1/sqrt(number units in previous layer)) # Sampled from normal distribution\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units) * np.sqrt(1/layer_a_units)\n",
    "            bias_matrix =np.random.randn(1, layer_b_units)\n",
    "        elif param_initializer == \"he\": # Adjust the variance down by multiplying with (2/sqrt(number units in previous layer)) # Sampled from normal distribution\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units) * np.sqrt(2/layer_a_units)\n",
    "            bias_matrix =np.random.randn(1, layer_b_units)\n",
    "        else: # random\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units)\n",
    "            bias_matrix = np.random.randn(1, layer_b_units)\n",
    "        \n",
    "        return weight_matrix, bias_matrix.squeeze()\n",
    "    \n",
    "    \n",
    "    def get_activations(self, activation_type, z_matrix):\n",
    "        if activation_type == \"sigmoid\":\n",
    "            activation_matrix = self.sigmoid_activation(z_matrix)\n",
    "        elif activation_type == \"tanh\":\n",
    "            activation_matrix = self.tanh_activation(z_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            activation_matrix = self.relu_activation(z_matrix)\n",
    "        elif activation_type == \"softmax\":\n",
    "            # For softmax, apply the function over each row rather than element-wise,\n",
    "            # since softmax considers the entire layer to generate a Probability distribution\n",
    "            #print(z_matrix)\n",
    "            activation_matrix = np.apply_along_axis(self.softmax_activation, 1, z_matrix)\n",
    "        else: # Linear\n",
    "            activation_matrix = self.linear_activation(z_matrix)\n",
    "            \n",
    "        return activation_matrix\n",
    "    \n",
    "    \n",
    "    def get_loss(self, loss_type, y, y_hat, weights, l2_regularizer):\n",
    "        if loss_type == \"binary_crossentropy\":\n",
    "            J = self.binary_crossentropy(y, y_hat)\n",
    "        elif loss_type == \"categorical_crossentropy\":\n",
    "            J = self.categorical_crossentropy(y, y_hat)\n",
    "            \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_weights = 0\n",
    "        for weight_matrix in weights: \n",
    "            sum_of_squared_weights += np.sum(weight_matrix.flatten()**2)\n",
    "        loss_l2_regularization_component = (l2_regularizer/(2*self.batch_size)) * sum_of_squared_weights\n",
    "            \n",
    "        J += loss_l2_regularization_component\n",
    "        \n",
    "        return J\n",
    "        \n",
    "    \n",
    "    def get_activation_derivative(self, activation_type, activation_matrix):\n",
    "        # Here Activation is differentiated w.r.t to z (ie: (dA/dZ))\n",
    "        if activation_type == \"sigmoid\":\n",
    "            dA_dZ = self.sigmoid_derivative(activation_matrix)\n",
    "        elif activation_type == \"tanh\":\n",
    "            dA_dZ = self.tanh_derivative(activation_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            dA_dZ = self.relu_derivative(activation_matrix)\n",
    "        else: # Linear\n",
    "            dA_dZ = self.linear_derivative(activation_matrix)\n",
    "            \n",
    "        return dA_dZ\n",
    "    \n",
    "    \n",
    "    def get_loss_derivative(self, loss_type, activation_matrix, y):\n",
    "        # Here loss is differentiated w.r.t preactivations(z) (ie: (dJ/dZ))\n",
    "        if loss_type == \"binary_crossentropy\":\n",
    "            dZ = self.binary_crossentropy_derivative(activation_matrix, y)\n",
    "        elif loss_type == \"categorical_crossentropy\":\n",
    "            dZ = self.categorical_crossentropy_derivative(activation_matrix, y)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Forward Propagation: Forward pass\n",
    "    '''\n",
    "    \n",
    "    def forward_propagation(self, X_batch, weights=None, biases=None):\n",
    "        if (weights is None) & (biases is None):\n",
    "            weights = self.layer_weight_matrices\n",
    "            biases = self.layer_biases\n",
    "        \n",
    "        # Define containers to store activation and z matrices, for use while doing backprop\n",
    "        z_matrices = []\n",
    "        activation_matrices = []\n",
    "        # Append input (matrix; since we have batch) to activation_matrices\n",
    "        activation_matrices.append(X_batch)\n",
    "        # Next iterate over each layer\n",
    "        for layer, weight_matix, bias_vector in zip(self.layers_config[1:], weights, biases): # Ignoring the 0th layer in layers_config since it is an input layer\n",
    "            layer_number = layer[0] # Get layer number\n",
    "            layer_activation_type = layer[-1] # Get activation type for this layer\n",
    "            z_matrix = np.dot(activation_matrices[-1], weight_matix) + bias_vector\n",
    "            activation_matrix = self.get_activations(layer_activation_type, z_matrix) # activation(W.x + b)\n",
    "            # Store activation and z matrix\n",
    "            z_matrices.append(z_matrix)\n",
    "            activation_matrices.append(activation_matrix)\n",
    "        #yhat_matrix = activation_matrices[-1] # Final layer output matrix # Predicted(one hot vector)\n",
    "        #print(activation_matrices)\n",
    "        return weights, biases, z_matrices, activation_matrices\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Backward Propagation: Gradient Computation\n",
    "    '''   \n",
    "    \n",
    "    def backprop_gradient(self, y, weights, biases, z_matrices, activation_matrices):\n",
    "        w_derivatives = []\n",
    "        b_derivatives = []\n",
    "        \n",
    "        # Know the batch size; since we will be averaging our gradients\n",
    "        batch_size = len(activation_matrices[0])\n",
    "        \n",
    "        # Iterate over each layer from back, don't consider input layer\n",
    "        for layer_i in range(len(self.layers_config)-1, 0, -1):\n",
    "            layer_activation_type = self.layers_config[layer_i][-1]\n",
    "            \n",
    "            if (layer_i+1) == len(self.layers_config): # final layer\n",
    "                dZ = self.get_loss_derivative(loss_type=self.loss_function,\n",
    "                                              activation_matrix=activation_matrices[layer_i],\n",
    "                                              y=y)\n",
    "            else: # rest of the layers [note: input layer isn't considered in the loop]\n",
    "                dZ = dA_previous * self.get_activation_derivative(activation_type=layer_activation_type,\n",
    "                                                   activation_matrix=activation_matrices[layer_i])\n",
    "                \n",
    "            dW = np.dot(dZ.T, activation_matrices[layer_i - 1]) / batch_size\n",
    "            db = np.sum(dZ.T, axis=1) / batch_size\n",
    "            dA_previous = np.dot(weights[layer_i-1], dZ.T).T # We subtract 1 from layer_i, because current layer weights are stored on previous index\n",
    "        \n",
    "            # Save dW and db\n",
    "            w_derivatives.append(dW.T)\n",
    "            b_derivatives.append(db)\n",
    "        # Reverse w_derivatives and b_derivatives lists, since iterating backwards added update parameter matrix in backwards order of the network.\n",
    "        # We reverse here simply because it becomes easy to update the existing weights\n",
    "        w_derivatives.reverse()\n",
    "        b_derivatives.reverse()\n",
    "        return w_derivatives, b_derivatives\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Backward Propagation: Optimization and Parameter updates\n",
    "    ''' \n",
    "#     \"default\",\n",
    "#     \"momentum\",\n",
    "#      \"nag\",\n",
    "#      \"adagrad\",\n",
    "#      \"adadelta\",\n",
    "#      \"rmsprop\",\n",
    "#      \"adam\",\n",
    "#      \"adamax\",\n",
    "#      \"nadam\",\n",
    "#      \"amsgrad\"\n",
    "    def default_optimizer(self, batch_i, batch_i_x, batch_i_y, eta, l2_regularizer):\n",
    "        # Perform forward pass and compute change in direction of gradient w.r.t each parameter\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        # Compute loss\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        # Backward pass to perform gradient computation w.r.t each parameter\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        #Update parameters of each layer\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "            self.layer_weight_matrices[i] = derivated_l2_regularization_component * self.layer_weight_matrices[i] - (eta * w_derivatives[i])\n",
    "            self.layer_biases[i] = self.layer_biases[i] - (eta * b_derivatives[i])\n",
    "\n",
    "#         if batch_i == 0:\n",
    "#             print(self.layer_weight_matrices[-1])\n",
    "#             self.a_validator.append(activation_matrices[-1])\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def momentum_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, momentum):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            pass\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def optimize_batch(self, batch_i, optimizer_type, batch_i_x, batch_i_y, eta, l2_regularizer, momentum, beta, beta1, beta2, eps):\n",
    "        if optimizer_type == \"momentum\":\n",
    "            pass\n",
    "            #loss = self.momentum_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, momentum)\n",
    "        else: # Default batch gradient descent\n",
    "            loss = self.default_optimizer(batch_i, batch_i_x, batch_i_y, eta, l2_regularizer)\n",
    "        return loss\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, batch_size=None, eta=0.01, l2_regularizer=0, momentum=0.9, beta=0.9, beta1=0.9, beta2=0.9, eps=1e-8):\n",
    "        #X_bak = X\n",
    "        #y_bak = y\n",
    "        # Feature scaling\n",
    "        X = self.standardizer(X)\n",
    "        \n",
    "        # Save target label values (unique)\n",
    "        self.y_unique_labels = np.unique(np.hstack(y))\n",
    "        self.y_unique_encoding = np.arange(len(self.y_unique_labels))\n",
    "        # One hot encode target (y)\n",
    "        y = self.one_hot_encode_labels(y)\n",
    "        \n",
    "        if not batch_size:\n",
    "            batch_size = len(X)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Get batch indices for creating training data\n",
    "        batch_indices = self.generate_batch_indices(len(X), batch_size)\n",
    "        x_train = X[batch_indices, :]\n",
    "        y_train = y[batch_indices, :]\n",
    "        \n",
    "        # Train\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (batch_i_x, batch_i_y) in enumerate(zip(x_train, y_train)): #zip(x_train[:1], y_train[:1]):\n",
    "#                 if batch_i == 0:\n",
    "#                     print(f\"Batch: {batch_i}\")\n",
    "                loss = self.optimize_batch(batch_i, self.optimizer, batch_i_x, batch_i_y, eta, l2_regularizer, momentum, beta, beta1, beta2, eps)\n",
    "                self.history.append(loss)\n",
    "#                 weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "#                 loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1])\n",
    "#                 w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[0].shape}\")\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[1].shape}\")\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[2].shape}\")\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[3].shape}\")\n",
    "#                 print()\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[0].shape}\")\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[1].shape}\")\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[2].shape}\")\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[3].shape}\")\n",
    "                \n",
    "#                 print(self.layer_weight_matrices[0])\n",
    "#                 print()\n",
    "#                 print(w_derivatives[0])\n",
    "                \n",
    "#                 print(self.layer_biases[0])\n",
    "#                 print()\n",
    "#                 print(b_derivatives[0])\n",
    "        #return X_bak[batch_indices[0], :], y_bak[batch_indices[0], :], batch_i_x, batch_i_y, batch_indices[0]\n",
    "        #return X, batch_indices[0]\n",
    "\n",
    "\n",
    "    '''\n",
    "    Prediction\n",
    "    '''\n",
    "\n",
    "    def predict(self, x): #, ii):\n",
    "        # Feature scaling\n",
    "        x = self.standardizer(x, isTrain=False)\n",
    "        # Perform Forward pass\n",
    "        _, _, _, activation_matrices = self.forward_propagation(x) #[ii, :]) #(x)\n",
    "        y_hat = activation_matrices[-1]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        if self.loss_function == \"binary_crossentropy\":\n",
    "            def binary_match(y_hat_i):\n",
    "                predictions.append(list(self.y_unique_labels[y_hat_i > 0.5])) # Threhold of 0.5, because we use sigmoid function in final layer\n",
    "        \n",
    "            np.apply_along_axis(binary_match, 1, y_hat)\n",
    "            \n",
    "        elif self.loss_function == \"categorical_crossentropy\":\n",
    "            def categorical_match(y_hat_i):\n",
    "                predictions.append(self.y_unique_labels[np.argmax(y_hat_i)]) \n",
    "                \n",
    "            np.apply_along_axis(categorical_match, 1, y_hat)\n",
    "        \n",
    "        return y_hat, predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_arch(sizes, activations):\n",
    "    fig_graph = plt.figure(figsize=(10, 10))\n",
    "    draw_network.draw(fig_graph.gca(), .1, .9, .1, .9, sizes, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc = MutliLayerPerceptronClassifer()\n",
    "mlpc.add_layer(units=20, activation=\"relu\", input_units=4)\n",
    "mlpc.add_layer(units=20, activation=\"relu\")\n",
    "mlpc.add_layer(units=30, activation=\"relu\")\n",
    "mlpc.add_layer(units=3, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc.compile_network(loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], epochs=10, batch_size=5, l2_regularizer=0.3) # xx, yy, xi, yi, ii = #rdp, ii = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cf8adbd160>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlpc.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4, None),\n",
       " (1, 20, 'relu'),\n",
       " (2, 20, 'relu'),\n",
       " (3, 30, 'relu'),\n",
       " (4, 3, 'softmax')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.layers_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 20), (2, 20), (3, 30), (4, 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts = mlpc.network_structure\n",
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_prob, preds = mlpc.predict(clf_X_df.values) #, ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf_y_df.values == np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
