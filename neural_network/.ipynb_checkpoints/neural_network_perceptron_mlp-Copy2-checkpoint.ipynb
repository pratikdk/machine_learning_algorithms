{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch [Multilayer Perceptron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to plot neural network\n",
    "import matplotlib.pyplot as plt\n",
    "import draw_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data [Classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = pd.read_csv(\"Iris.csv\")\n",
    "clf_df = clf_df.drop(\"Id\", axis=1)\n",
    "clf_df = clf_df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_X_df = clf_df.iloc[:, :-1]\n",
    "clf_y_df = clf_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_X_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Iris-setosa\n",
       "1    Iris-setosa\n",
       "2    Iris-setosa\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_y_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "\n",
    "Input: **(Done)**\n",
    "- batch (as seperate argument) full/minibatch **(Done)**\n",
    "- single/batch/minibatch generate indices (random) **(Done)**\n",
    "- sample using indices **(Done)**\n",
    "\n",
    "Output: **(Done)**\n",
    "- output labelencoder **(Done)**\n",
    "- save actual labels **(Done)**\n",
    "- one hot encoding of labels **(Done)**\n",
    "\n",
    "Weight Initializations:\n",
    "- Random **(Done)**\n",
    "- Zeros **(Done)**\n",
    "- Xavier **(Done)**\n",
    "- He **(Done)**\n",
    "\n",
    "Activation functions:\n",
    "- sigmoid **(Done)**\n",
    "- sigmoid derivative **(Done)**\n",
    "- relu **(Done)**\n",
    "- relu derivative **(Done)**\n",
    "- tanh **(Done)**\n",
    "- tanh derivative **(Done)**\n",
    "- softmax **(Done)**\n",
    "- softmax derivative **(Done)**\n",
    "\n",
    "Loss computation:\n",
    "- binary_crossentropy **(Done)**\n",
    "- binary_crossentropy derivative **(Done)**\n",
    "- categorical_crossentropy **(Done)**\n",
    "- categorical_crossentropy derivative **(Done)**\n",
    "\n",
    "Forward propagation:\n",
    "- iterative forward pass **(Done)**\n",
    "\n",
    "Back propagation:\n",
    "- chain builder - Parameter respective,for each training example, negative gradient **(Done)**\n",
    "\n",
    "Parameter update:\n",
    "- average derivative respective to each parameter for all training examples **(Done)**\n",
    "- update old weights by adding **(Done)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliLayerPerceptronClassifer():\n",
    "    def __init__(self, random_state=False):\n",
    "        # RNG seed\n",
    "        if not random_state:\n",
    "            np.random.seed(20)\n",
    "        # network config vars\n",
    "        self.layers_config = []\n",
    "        self.network_structure = None\n",
    "        self.loss_function = None\n",
    "        self.optimizer = \"default\"\n",
    "        self.parameter_initializer = \"random\"\n",
    "        self.history = []\n",
    "        self.input_scaling_values = {}\n",
    "        self.batch_size = 0\n",
    "        \n",
    "        self.supported_network_settings = {\n",
    "            \"parameter_initializer\": [\"random\", \"zeros\", \"xavier\", \"he\"],\n",
    "            \"activation\": [\"sigmoid\", \"tanh\", \"relu\", \"softmax\", \"linear\"],\n",
    "            \"loss\": [\"binary_crossentropy\", \"categorical_crossentropy\"],\n",
    "            \"optimizer\": [\"default\",\n",
    "                         \"momentum\",\n",
    "                         \"nag\",\n",
    "                         \"adagrad\",\n",
    "                         \"adadelta\",\n",
    "                         \"rmsprop\",\n",
    "                         \"adam\",\n",
    "                         \"adamax\",\n",
    "                         \"nadam\",\n",
    "                         \"amsgrad\"]\n",
    "        }\n",
    "         \n",
    "        \n",
    "    '''\n",
    "    Setup training input and output(generate onehot for binary/multiclass/multilabel target)\n",
    "    '''\n",
    "    \n",
    "    def generate_batch_indices(self, input_examples_count, batch_size, final_batch_threshold=0.5, random=True):\n",
    "        row_indices = np.arange(start=0, stop=input_examples_count) # array([0, 1, ....., len(input_examples_count)-1 ])\n",
    "\n",
    "        if random:\n",
    "            np.random.shuffle(row_indices)\n",
    "\n",
    "        # Validate batch sizes\n",
    "        if batch_size: # 'Stochastic | mini-batch' batch sizes\n",
    "            if (batch_size < 1) | (batch_size > len(row_indices)): # Batch size should be between (0, len(row_indices)]\n",
    "                # raise an error \n",
    "                raise Exception(f\"Batch size violated the input size bounds: Batch size should be between (0, {len(data)}]\")\n",
    "        else:\n",
    "            batch_size = len(row_indices) # entire dataset as batch\n",
    "\n",
    "        # Validate if batch_size evenly splits data if not then mark batch_evenly_divides_input as false\n",
    "        batch_evenly_divides_input = False\n",
    "        if (len(row_indices) % batch_size) == 0:\n",
    "            batch_evenly_divides_input = True    \n",
    "        batches_count = int(len(row_indices)/batch_size) # Number of batches \n",
    "        batchable_rows_count = batches_count * batch_size # Number of rows which will be addressed by batches\n",
    "\n",
    "        # Store batch indices\n",
    "        batch_indices = np.stack(np.array_split(row_indices[:batchable_rows_count], batches_count), axis=0).tolist()\n",
    "        \n",
    "        # if some rows remain(batch_size didnt evenly divide len(row_indices)), make one final batch of input using\n",
    "        # remaining rows + randomly sampled rows from row_indices until size of new final batch equals batch_size\n",
    "        if not batch_evenly_divides_input:\n",
    "            remaining_row_indices = row_indices[batchable_rows_count:]\n",
    "            # If remining rows count are less than 50% (final_batch_threshold) of batch_size discard creation of final batch\n",
    "            if len(remaining_row_indices) > (batch_size * final_batch_threshold):\n",
    "                # compute how many more rows are required\n",
    "                required_row_indices_count = batch_size - len(remaining_row_indices)\n",
    "                # randomly sample about 'required_row_indices_count' rows from row_indices without replacement(to avoid selecting a row multiple times)\n",
    "                sampled_rows_indices = row_indices[np.random.choice(len(row_indices), required_row_indices_count, replace=False)]\n",
    "                # Stack remaining rows and sampled_rows within a single batch\n",
    "                final_batch = np.concatenate([remaining_row_indices, sampled_rows_indices]).tolist()\n",
    "                # Finally stack final batch to data_collated\n",
    "                batch_indices.append(final_batch)\n",
    "\n",
    "        return batch_indices\n",
    "    \n",
    "\n",
    "    def standardizer(self, x, isTrain=True): # Normalize with mean 0 and std-dev of 1, also called z-score\n",
    "        if isTrain:\n",
    "            x_mean = np.mean(x, axis=0)\n",
    "            x_stdev = np.std(x, axis=0)\n",
    "            # Save mean and standard deviation\n",
    "            self.input_scaling_values['standardizer_mean'] = x_mean\n",
    "            self.input_scaling_values['standardizer_stdev'] = x_stdev\n",
    "        stdz_x = np.divide((x - self.input_scaling_values['standardizer_mean']), self.input_scaling_values['standardizer_stdev'],\n",
    "                           where=self.input_scaling_values['standardizer_stdev']!=0)\n",
    "        return stdz_x\n",
    "    \n",
    "    \n",
    "    def one_hot_encode_labels(self, y):\n",
    "        \"\"\"\n",
    "        Creates one hot encodings for y w.r.t self.y_unique_labels (Supports Multiclass + Multilabel)\n",
    "        Eg: y = np.array([['b'], ['a', 'c', 'd'], ['a'], ['c', 'b']]) and self.y_unique_labels = np.array(['a', 'b', 'c', 'd'])\n",
    "            one_hot_encoded_y = np.array([[0, 1, 0, 0], [1, 0, 1, 1], [1, 0, 0, 0], [0, 1, 1, 0]])\n",
    "        \n",
    "        NOTE: Here I found np.vectorize() to run quicker than a custom matrix function, takes exactly 0.00007(secs) for an input of 6 rows,\n",
    "        Whereas a custom function requires knowing count of multilabels and does padding to create mask which alone takes 0.0008(secs on avg),\n",
    "        let alone additional steps like masking and one hot encoding will add up to time, So vectorize is best for this case\n",
    "        \"\"\"\n",
    "        def one_hot_vector(target):\n",
    "            return np.in1d(self.y_unique_labels, target, assume_unique=\"True\")\n",
    "        one_hot_vectorizer = np.vectorize(one_hot_vector, otypes=[np.ndarray])\n",
    "        one_hot_encoded_y = np.stack(np.hstack(one_hot_vectorizer(y))).astype(int)\n",
    "        return one_hot_encoded_y\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Network configure\n",
    "    '''    \n",
    "     \n",
    "    def add_layer(self, units, activation=\"relu\", input_units=None):\n",
    "        # Validate activation value\n",
    "        self.validate_settings(setting_type=\"activation\", setting_value=activation)\n",
    "        \n",
    "        layers_count = len(self.layers_config)\n",
    "        # check if this is the first layer \n",
    "        if (layers_count == 0):\n",
    "            # Make sure input_dim is specified\n",
    "            if (input_units == None):\n",
    "                # We need input_dim as this is the first hidden layer\n",
    "                raise Exception(\"Please specify parameter value for 'input_units'(eg: input_units=4), as this is the first hidden layer\")\n",
    "            else:\n",
    "                # First create the input layer\n",
    "                self.layers_config.append((layers_count, input_units, None))\n",
    "                # Next add the hidden layer\n",
    "                self.layers_config.append((layers_count+1, units, activation))\n",
    "        else:  \n",
    "            self.layers_config.append((layers_count, units, activation))\n",
    "            \n",
    "            \n",
    "    '''\n",
    "    Network compiler & Parameter initializer\n",
    "    '''\n",
    "    \n",
    "    def validate_settings(self, setting_type, setting_value):\n",
    "        if setting_value not in self.supported_network_settings[setting_type]:\n",
    "            raise Exception(f\"{setting_type}='{setting_value}' is not supported, please specify {setting_type} as any one from the following list {self.supported_network_settings[setting_type]}\")            \n",
    "            \n",
    "    \n",
    "    def compile_network(self, loss, optimizer=None, param_initializer=None):\n",
    "        # Validate loss type value\n",
    "        self.validate_settings(setting_type=\"loss\", setting_value=loss)\n",
    "        # Validate optimizer type value\n",
    "        if optimizer is not None: # If None then use entire dataset as one epoch\n",
    "            self.validate_settings(setting_type=\"optimizer\", setting_value=optimizer)\n",
    "        # Validate initializer type value\n",
    "        if param_initializer is not None:\n",
    "            self.validate_settings(setting_type=\"parameter_initializer\", setting_value=param_initializer)\n",
    "            \n",
    "        # Setup layers\n",
    "        self.network_structure = list(map(tuple, np.array(self.layers_config)[:, :-1])) # Ignore the last columns as it specifies the type of activation function\n",
    "        # Save network settings\n",
    "        self.loss_function = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.parameter_initializer = param_initializer\n",
    "        # Weight and Bias matrix container\n",
    "        self.layer_weight_matrices = []\n",
    "        self.layer_biases = []\n",
    "        \n",
    "        # Initialize optimizer parameter containers, as required by different optimizers\n",
    "        # timestep: t\n",
    "        self.weights_update = []\n",
    "        self.biases_update = []\n",
    "        # Gradient squared\n",
    "        self.weight_grads_squared = []\n",
    "        self.bias_grads_squared = []\n",
    "        # Start by saving t-1 and t-2 initial updates\n",
    "        self.all_weight_updates = [[], []]\n",
    "        self.all_bias_updates = [[], []]\n",
    "        \n",
    "        for layers_number in range(1, len(self.network_structure)):\n",
    "            layer_a_units = self.network_structure[layers_number-1][1]\n",
    "            layer_b_units = self.network_structure[layers_number][1]\n",
    "            weight_matrix, bias_matrix = self.generate_intial_parameters(self.parameter_initializer, layer_a_units, layer_b_units)\n",
    "            # append matrix to list layer_weight_matrices and layer_biases lists\n",
    "            self.layer_weight_matrices.append(weight_matrix)\n",
    "            self.layer_biases.append(bias_matrix)\n",
    "            # Initialize initial values into optimizer parameter containers\n",
    "            self.weights_update.append(0)\n",
    "            self.biases_update.append(0)\n",
    "            self.weight_grads_squared.append(0)\n",
    "            self.bias_grads_squared.append(0)\n",
    "            self.all_weight_updates[-1].append(0) # t-1\n",
    "            self.all_weight_updates[-2].append(0) # t-2\n",
    "            self.all_bias_updates[-1].append(0) # t-1\n",
    "            self.all_bias_updates[-2].append(0) # t-2\n",
    "            \n",
    "    '''\n",
    "    Activation Functions\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_activation(self, z):\n",
    "        # Plain and simple sigmoid function, function returns prob for z(z is weighted sum of input)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def tanh_activation(self, z):\n",
    "        return np.tanh(z) #  (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def relu_activation(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    \n",
    "    def linear_activation(self, z):\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def softmax_activation(self, z_vector):\n",
    "        # Normalizing the inputs to be not too large or too small, to address prob of overshooting when performing exponentiations of large numbers(violates floating point limit)\n",
    "        # Shift the inputs to a range close to zero and less than zero\n",
    "        # large negative exponents \"saturate\" to zero rather than infinity, so we have a better chance of avoiding NaNs.\n",
    "        shiftz = z_vector - np.max(z_vector)\n",
    "        exps = np.exp(shiftz)\n",
    "        return exps / np.sum(exps) \n",
    "    \n",
    "    '''\n",
    "    Derivatives of Activation Functions\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_derivative(self, sig_z): # sig_z = sig(w.a + b) (vectors w,a represent weight,activations)\n",
    "        # Differentiate sigmoid function w.r.t to z to obtain sig_z * (1 - sig_z)\n",
    "        return sig_z * (1.0 - sig_z)\n",
    "    \n",
    "    \n",
    "    def tanh_derivative(self, tanh_z): \n",
    "        return 1.0 - tanh_z**2\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, rel_z):\n",
    "        return 1.0 * (rel_z > 0)\n",
    "    \n",
    "    \n",
    "    def linear_derivative(self, lin_z):\n",
    "        return 1.0\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Loss functions\n",
    "    '''\n",
    "    \n",
    "    def binary_crossentropy(self, y, y_hat):\n",
    "        return np.sum(y*np.log(y_hat + 1e-8) + (1-y)*np.log(1 - y_hat + 1e-8)) * -(1.0/len(y))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_hat):\n",
    "        return -np.mean(np.sum(y*np.log(y_hat + 1e-8), axis=1))\n",
    "    \n",
    "    '''\n",
    "    Derivatives of Loss functions(cross entorpy) w.r.t to logits (dJ/dZ)\n",
    "    '''\n",
    "    \n",
    "    def binary_crossentropy_derivative(self, activation_matrix, y):\n",
    "        return activation_matrix - y\n",
    "    \n",
    "    \n",
    "    def categorical_crossentropy_derivative(self, activation_matrix, y):\n",
    "        return activation_matrix - y\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Function Invokers\n",
    "    '''\n",
    "    \n",
    "    def generate_intial_parameters(self, param_initializer, layer_a_units, layer_b_units):\n",
    "        if param_initializer == \"zeros\":\n",
    "            weight_matrix = np.zeros((layer_a_units, layer_b_units))\n",
    "            bias_matrix = np.zeros((1, layer_b_units))\n",
    "        elif param_initializer == \"xavier\": # Adjust the variance down by multiplying with (1/sqrt(number units in previous layer)) # Sampled from normal distribution\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units) * np.sqrt(1/layer_a_units)\n",
    "            bias_matrix =np.random.randn(1, layer_b_units)\n",
    "        elif param_initializer == \"he\": # Adjust the variance down by multiplying with (2/sqrt(number units in previous layer)) # Sampled from normal distribution\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units) * np.sqrt(2/layer_a_units)\n",
    "            bias_matrix =np.random.randn(1, layer_b_units)\n",
    "        else: # random\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units)\n",
    "            bias_matrix = np.random.randn(1, layer_b_units)\n",
    "        \n",
    "        weight_matrix = weight_matrix.astype('float64')\n",
    "        bias_matrix = bias_matrix.astype('float64')\n",
    "        return weight_matrix, bias_matrix.squeeze()\n",
    "    \n",
    "    \n",
    "    def get_activations(self, activation_type, z_matrix):\n",
    "        if activation_type == \"sigmoid\":\n",
    "            activation_matrix = self.sigmoid_activation(z_matrix)\n",
    "        elif activation_type == \"tanh\":\n",
    "            activation_matrix = self.tanh_activation(z_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            activation_matrix = self.relu_activation(z_matrix)\n",
    "        elif activation_type == \"softmax\":\n",
    "            # For softmax, apply the function over each row rather than element-wise,\n",
    "            # since softmax considers the entire layer to generate a Probability distribution\n",
    "            #print(z_matrix)\n",
    "            activation_matrix = np.apply_along_axis(self.softmax_activation, 1, z_matrix)\n",
    "        else: # Linear\n",
    "            activation_matrix = self.linear_activation(z_matrix)\n",
    "            \n",
    "        return activation_matrix\n",
    "    \n",
    "    \n",
    "    def get_loss(self, loss_type, y, y_hat, weights, l2_regularizer):\n",
    "        if loss_type == \"binary_crossentropy\":\n",
    "            J = self.binary_crossentropy(y, y_hat)\n",
    "        elif loss_type == \"categorical_crossentropy\":\n",
    "            J = self.categorical_crossentropy(y, y_hat)\n",
    "            \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_weights = 0\n",
    "        for weight_matrix in weights: \n",
    "            sum_of_squared_weights += np.sum(weight_matrix.flatten()**2)\n",
    "        loss_l2_regularization_component = (l2_regularizer/(2*self.batch_size)) * sum_of_squared_weights\n",
    "            \n",
    "        J += loss_l2_regularization_component\n",
    "        \n",
    "        return J\n",
    "        \n",
    "    \n",
    "    def get_activation_derivative(self, activation_type, activation_matrix):\n",
    "        # Here Activation is differentiated w.r.t to z (ie: (dA/dZ))\n",
    "        if activation_type == \"sigmoid\":\n",
    "            dA_dZ = self.sigmoid_derivative(activation_matrix)\n",
    "        elif activation_type == \"tanh\":\n",
    "            dA_dZ = self.tanh_derivative(activation_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            dA_dZ = self.relu_derivative(activation_matrix)\n",
    "        else: # Linear\n",
    "            dA_dZ = self.linear_derivative(activation_matrix)\n",
    "            \n",
    "        return dA_dZ\n",
    "    \n",
    "    \n",
    "    def get_loss_derivative(self, loss_type, activation_matrix, y):\n",
    "        # Here loss is differentiated w.r.t preactivations(z) (ie: (dJ/dZ))\n",
    "        if loss_type == \"binary_crossentropy\":\n",
    "            dZ = self.binary_crossentropy_derivative(activation_matrix, y)\n",
    "        elif loss_type == \"categorical_crossentropy\":\n",
    "            dZ = self.categorical_crossentropy_derivative(activation_matrix, y)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Forward Propagation: Forward pass\n",
    "    '''\n",
    "    \n",
    "    def forward_propagation(self, X_batch, weights=None, biases=None):\n",
    "        if (weights is None) & (biases is None):\n",
    "            weights = self.layer_weight_matrices\n",
    "            biases = self.layer_biases\n",
    "        \n",
    "        # Define containers to store activation and z matrices, for use while doing backprop\n",
    "        z_matrices = []\n",
    "        activation_matrices = []\n",
    "        # Append input (matrix; since we have batch) to activation_matrices\n",
    "        activation_matrices.append(X_batch)\n",
    "        # Next iterate over each layer\n",
    "        for layer, weight_matix, bias_vector in zip(self.layers_config[1:], weights, biases): # Ignoring the 0th layer in layers_config since it is an input layer\n",
    "            layer_number = layer[0] # Get layer number\n",
    "            layer_activation_type = layer[-1] # Get activation type for this layer\n",
    "            z_matrix = np.dot(activation_matrices[-1], weight_matix) + bias_vector\n",
    "            activation_matrix = self.get_activations(layer_activation_type, z_matrix) # activation(W.x + b)\n",
    "            # Store activation and z matrix\n",
    "            z_matrices.append(z_matrix)\n",
    "            activation_matrices.append(activation_matrix)\n",
    "        #yhat_matrix = activation_matrices[-1] # Final layer output matrix # Predicted(one hot vector)\n",
    "        #print(activation_matrices)\n",
    "        return weights, biases, z_matrices, activation_matrices\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Backward Propagation: Gradient Computation\n",
    "    '''   \n",
    "    \n",
    "    def backprop_gradient(self, y, weights, biases, z_matrices, activation_matrices):\n",
    "        w_derivatives = []\n",
    "        b_derivatives = []\n",
    "        \n",
    "        # Know the batch size; since we will be averaging our gradients\n",
    "        batch_size = len(activation_matrices[0])\n",
    "        \n",
    "        # Iterate over each layer from back, don't consider input layer\n",
    "        for layer_i in range(len(self.layers_config)-1, 0, -1):\n",
    "            layer_activation_type = self.layers_config[layer_i][-1]\n",
    "            \n",
    "            if (layer_i+1) == len(self.layers_config): # final layer\n",
    "                dZ = self.get_loss_derivative(loss_type=self.loss_function,\n",
    "                                              activation_matrix=activation_matrices[layer_i],\n",
    "                                              y=y)\n",
    "            else: # rest of the layers [note: input layer isn't considered in the loop]\n",
    "                dZ = dA_previous * self.get_activation_derivative(activation_type=layer_activation_type,\n",
    "                                                   activation_matrix=activation_matrices[layer_i])\n",
    "                \n",
    "            dW = np.dot(dZ.T, activation_matrices[layer_i - 1]) / batch_size\n",
    "            db = np.sum(dZ.T, axis=1) / batch_size\n",
    "            dA_previous = np.dot(weights[layer_i-1], dZ.T).T # We subtract 1 from layer_i, because current layer weights are stored on previous index\n",
    "        \n",
    "            # Save dW and db\n",
    "            w_derivatives.append(dW.T)\n",
    "            b_derivatives.append(db)\n",
    "        # Reverse w_derivatives and b_derivatives lists, since iterating backwards added update parameter matrix in backwards order of the network.\n",
    "        # We reverse here simply because it becomes easy to update the existing weights\n",
    "        w_derivatives.reverse()\n",
    "        b_derivatives.reverse()\n",
    "        return w_derivatives, b_derivatives\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Backward Propagation: Optimization and Parameter updates\n",
    "    ''' \n",
    "#     \"gradient\",\n",
    "#     \"momentum\",\n",
    "#      \"nag\",\n",
    "#      \"adagrad\",\n",
    "#      \"adadelta\",\n",
    "#      \"rmsprop\",\n",
    "#      \"adam\",\n",
    "#      \"adamax\",\n",
    "#      \"nadam\",\n",
    "#      \"amsgrad\"\n",
    "    def gradient_optimizer(self, batch_i, batch_i_x, batch_i_y, eta, l2_regularizer):\n",
    "        # Perform forward pass and compute change in direction of gradient w.r.t each parameter\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        # Compute loss\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        # Backward pass to perform gradient computation w.r.t each parameter\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        # Derivated regularization component\n",
    "        derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "        \n",
    "        #Update parameters of each layer\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            self.layer_weight_matrices[i] = derivated_l2_regularization_component * self.layer_weight_matrices[i] - (eta * w_derivatives[i])\n",
    "            self.layer_biases[i] = self.layer_biases[i] - (eta * b_derivatives[i])\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def momentum_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, momentum):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            # Define momentum based update\n",
    "            self.weights_update[i] = (momentum * self.weights_update[i]) + (eta * w_derivatives[i]) \n",
    "            self.biases_update[i] = (momentum * self.biases_update[i]) + (eta * b_derivatives[i])\n",
    "            # Update parameters using momentum based update\n",
    "            self.layer_weight_matrices[i] = derivated_l2_regularization_component * self.layer_weight_matrices[i] - self.weights_update[i]\n",
    "            self.layer_biases[i] = self.layer_biases[i] - self.biases_update[i]\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "#     def nesterov_accelerated_gradient_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, momentum):\n",
    "#         derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "        \n",
    "#         # First perform temporary update of current parameters in the direction of previous parameter update (Equivalent to first comitting the mistake)\n",
    "#         temporary_layer_weights = []\n",
    "#         temporary_layer_biases = []\n",
    "#         for i in range(len(self.layers_config)-1):\n",
    "#             self.weights_update[i] = momentum * self.previous_weights_update[i]\n",
    "#             self.biases_update[i] = momentum * self.previous_biases_update[i]\n",
    "#             temporary_layer_weights.append(derivated_l2_regularization_component * self.layer_weight_matrices[i] - self.weights_update[i])\n",
    "#             temporary_layer_biases.append(self.layer_biases[i] - self.biases_update[i])\n",
    "            \n",
    "#         # Compute the gradient of updated current parameters,(we obtain a much informed gradient compared to Momentum's(non NAG) gradient, since we are precomputing it from a future position)\n",
    "#         weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x, temporary_layer_weights, temporary_layer_biases)\n",
    "#         loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "#         w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "#         # Finally update current parameters in the direction of previous parameter update, while using the computed gradient which acts as a correction to the mistake(to update in correct direction)\n",
    "#         for i in range(len(self.layers_config)-1):\n",
    "#             self.weights_update[i] = (momentum * self.weights_update[i]) + (eta * w_derivatives[i])\n",
    "#             self.biases_update[i] = (momentum * self.biases_update[i]) + (eta * b_derivatives[i])\n",
    "#             self.layer_weight_matrices[i] = (derivated_l2_regularization_component * self.layer_weight_matrices[i]) - self.weights_update[i]\n",
    "#             self.layer_biases[i] = self.layer_biases[i] - self.biases_update[i]\n",
    "            \n",
    "#         self.previous_weights_update = self.weights_update\n",
    "#         return loss\n",
    "    \n",
    "    def nesterov_accelerated_gradient_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, momentum):\n",
    "        derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "\n",
    "        # First perform temporary update of current parameters in the direction of previous parameter update (Equivalent to first comitting the mistake)\n",
    "        temporary_layer_weights = []\n",
    "        temporary_layer_biases = []\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            weight_momentum_term = momentum * self.all_weight_updates[-1][i]\n",
    "            bias_momentum_term = momentum * self.all_bias_updates[-1][i]\n",
    "            temporary_layer_weights.append(derivated_l2_regularization_component * self.layer_weight_matrices[i] - weight_momentum_term)\n",
    "            temporary_layer_biases.append(self.layer_biases[i] - bias_momentum_term)\n",
    "            \n",
    "        # Compute the gradient of updated current parameters,(we obtain a much informed gradient compared to Momentum's(non NAG) gradient, since we are precomputing it from a future position)\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x, temporary_layer_weights, temporary_layer_biases)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        # Define containers to store parameters\n",
    "        t_weight_container = []\n",
    "        t_bias_container = []\n",
    "        # Finally update current parameters in the direction of previous parameter update, while using the computed gradient which acts as a correction to the mistake(to update in correct direction)\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            t_weight_container.append((momentum * self.all_weight_updates[-1][i]) + (eta * w_derivatives[i]))\n",
    "            t_bias_container.append((momentum * self.all_bias_updates[-1][i]) + (eta * b_derivatives[i]))\n",
    "            self.layer_weight_matrices[i] = self.layer_weight_matrices[i] - t_weight_container[i]\n",
    "            self.layer_biases[i] = self.layer_biases[i] - t_bias_container[i]\n",
    "        \n",
    "        self.all_weight_updates.append(t_weight_container)\n",
    "        self.all_bias_updates.append(t_bias_container)\n",
    "        #self.previous_weights_update = self.weights_update\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def adagrad_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            # Sum of past squared gradients upto current timestep(epoch*batch) t\n",
    "            self.weights_update[i] = self.weights_update[i] + w_derivatives[i]**2\n",
    "            self.biases_update[i] = self.biases_update[i] + b_derivatives[i]**2\n",
    "            self.layer_weight_matrices[i] -= ((eta/(np.sqrt(self.weights_update[i])+eps)) * w_derivatives[i])\n",
    "            self.layer_biases[i] -= ((eta/(np.sqrt(self.biases_update[i])+eps)) * b_derivatives[i])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "#     def adadelta_optimizer(self, batch_i_x, batch_i_y, l2_regularizer, beta, eps):\n",
    "#         weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "#         loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "#         w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "#         #derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "        \n",
    "#         for i in range(len(self.layers_config)-1):\n",
    "#             #w_derivatives[i] = w_derivatives[i].astype('float64') \n",
    "#             #b_derivatives[i] = b_derivatives[i].astype('float64') \n",
    "#             w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "#             # exponentially decaying average of sum of squared parameter updates\n",
    "#             try:\n",
    "#                 self.previous_weights_update[i] = (beta*self.t_2_previous_weights_update[i]) + ((1-beta)*self.previous_weights_update[i]**2)\n",
    "#             except RuntimeWarning:\n",
    "#                 #print(\"HII\")\n",
    "#                 print(self.previous_weights_update[i])\n",
    "#             self.previous_biases_update[i] = (beta*self.t_2_previous_biases_update[i]) + ((1-beta)*self.previous_biases_update[i]**2)\n",
    "            \n",
    "#             #self.previous_weights_update[i] = self.previous_weights_update[i].astype('float64')\n",
    "#             #self.previous_biases_update[i] = self.previous_biases_update[i].astype('float64') \n",
    "            \n",
    "#             # exponentially decaying average of sum of squared gradient\n",
    "#             self.weights_update[i] = (beta*self.weights_update[i]) + ((1-beta)*w_derivatives[i]**2)\n",
    "#             self.biases_update[i] = (beta*self.biases_update[i]) + ((1-beta)*b_derivatives[i]**2)\n",
    "#             # Perform update\n",
    "#             self.layer_weight_matrices[i] = self.layer_weight_matrices[i] - (((np.sqrt(self.previous_weights_update[i])+eps) / (np.sqrt(self.weights_update[i])+eps)) * w_derivatives[i])\n",
    "#             self.layer_biases[i] = self.layer_biases[i] - ( ((np.sqrt(self.previous_biases_update[i])+eps) / (np.sqrt(self.biases_update[i])+eps)) * b_derivatives[i] )\n",
    "        \n",
    "#         self.t_2_previous_weights_update = self.previous_weights_update\n",
    "#         self.t_2_previous_biases_update = self.previous_biases_update\n",
    "#         self.previous_weights_update = self.weights_update\n",
    "#         self.previous_biases_update = self.biases_update\n",
    "#         return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def adadelta_optimizer(self, batch_i_x, batch_i_y, l2_regularizer, beta, eps):\n",
    "#         weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "#         loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "#         w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "#         prev_w_update_save = self.previous_weights_update\n",
    "#         prev_b_update_save = self.previous_biases_update\n",
    "        \n",
    "#         for i in range(len(self.layers_config)-1):\n",
    "#             w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "# #             if self.t_2_previous_weights_update[i] == 0:\n",
    "# #                 print(0)\n",
    "#             # exponentially decaying average of sum of squared parameter updates\n",
    "#             print(\"t_2\")\n",
    "#             print(self.t_2_previous_weights_update[i])\n",
    "#             print(\"Prev\")\n",
    "#             print(self.previous_weights_update[i])\n",
    "#             print(\"What did i save!!\")\n",
    "#             print(prev_w_update_save[i])\n",
    "#             #print(\"What is before self.previous_weights_update[i] here\")\n",
    "#             #print(self.previous_weights_update[i])\n",
    "#             #self.t_2_previous_weights_update[i] = self.previous_weights_update[i]\n",
    "#             #self.t_2_previous_biases_update[i] = self.previous_biases_update[i]\n",
    "            \n",
    "#             self.previous_weights_update[i] = (beta*self.t_2_previous_weights_update[i]) + ((1-beta)*(self.previous_weights_update[i]**2))\n",
    "#             self.previous_biases_update[i] = (beta*self.t_2_previous_biases_update[i]) + ((1-beta)*(self.previous_biases_update[i]**2))\n",
    "\n",
    "#             #print(\"Previous**2\")\n",
    "#             self.weight_grads_squared[i] = (beta*self.weight_grads_squared[i]) + ((1-beta)*w_derivatives[i]**2)\n",
    "#             self.bias_grads_squared[i] = (beta*self.bias_grads_squared[i]) + ((1-beta)*b_derivatives[i]**2)\n",
    "            \n",
    "#             try:\n",
    "#                 self.weights_update[i] = -(((np.sqrt(self.previous_weights_update[i])+eps) / (np.sqrt(self.weight_grads_squared[i]+eps))) * w_derivatives[i])\n",
    "#             except:\n",
    "#                 print(\"BOOM!!!!!\")\n",
    "#                 print(self.previous_weights_update[i])\n",
    "#             self.biases_update[i] = -(((np.sqrt(self.previous_biases_update[i])+eps) / (np.sqrt(self.bias_grads_squared[i]+eps))) * b_derivatives[i])\n",
    "            \n",
    "#             # Perform update\n",
    "#             self.layer_weight_matrices[i] = self.layer_weight_matrices[i] + self.weights_update[i]\n",
    "#             self.layer_biases[i] = self.layer_biases[i] + self.biases_update[i]\n",
    "            \n",
    "#             #self.t_2_previous_weights_update[i] = prev_w_update_save[i]\n",
    "#             #self.t_2_previous_biases_update[i] = prev_b_update_save[i]\n",
    "#             #print(\"What is after self.previous_weights_update[i] here\")\n",
    "#             #print(self.previous_weights_update[i])\n",
    "            \n",
    "#         self.t_2_previous_weights_update = prev_w_update_save\n",
    "#         self.t_2_previous_biases_update = prev_b_update_save\n",
    "#         self.previous_weights_update = self.weights_update\n",
    "#         self.previous_biases_update = self.biases_update\n",
    "\n",
    "# #         print(\"T_2\")\n",
    "# #         print(self.t_2_previous_weights_update)\n",
    "# #         print(\"Prev\")\n",
    "# #         print(self.previous_weights_update)\n",
    "#         print(\"optized\")\n",
    "#         print()\n",
    "#         print()\n",
    "#         print()\n",
    "#         return loss\n",
    "    \n",
    "    def adadelta_optimizer(self, batch_i_x, batch_i_y, l2_regularizer, beta, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        t_weight_container = []\n",
    "        t_bias_container = []\n",
    "\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            running_weight_avg = (beta*self.all_weight_updates[-2][i]**2) + ((1-beta)*(self.all_weight_updates[-1][i]**2))\n",
    "            running_bias_avg = (beta*self.all_bias_updates[-2][i]**2) + ((1-beta)*(self.all_bias_updates[-1][i]**2))\n",
    "\n",
    "            self.weight_grads_squared[i] = (beta*self.weight_grads_squared[i]) + ((1-beta)*w_derivatives[i]**2)\n",
    "            self.bias_grads_squared[i] = (beta*self.bias_grads_squared[i]) + ((1-beta)*b_derivatives[i]**2)\n",
    "            \n",
    "            t_weight_container.append((((np.sqrt(running_weight_avg+eps)) / (np.sqrt(self.weight_grads_squared[i]+eps))) * w_derivatives[i]))\n",
    "            t_bias_container.append((((np.sqrt(running_bias_avg+eps)) / (np.sqrt(self.bias_grads_squared[i]+eps))) * b_derivatives[i]))\n",
    "            \n",
    "            # Perform update\n",
    "            self.layer_weight_matrices[i] = self.layer_weight_matrices[i] - t_weight_container[i]\n",
    "            self.layer_biases[i] = self.layer_biases[i] - t_bias_container[i]\n",
    "            \n",
    "        self.all_weight_updates.append(t_weight_container)\n",
    "        self.all_bias_updates.append(t_bias_container)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def rmsprop_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, beta, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            # Compute exponentially decaying average of sum of squared gradients (Running Average)\n",
    "            self.weights_update[i] = (beta*self.weights_update[i]) + ((1-beta)*w_derivatives[i]**2)\n",
    "            self.biases_update[i] = (beta*self.biases_update[i]) + ((1-beta)*b_derivatives[i]**2)\n",
    "            self.layer_weight_matrices[i] -= ((eta/(np.sqrt(self.weights_update[i])+eps)) * w_derivatives[i])\n",
    "            self.layer_biases[i] -= ((eta/(np.sqrt(self.biases_update[i])+eps)) * b_derivatives[i])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def adam_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, beta, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def optimize_batch(self, epoch_i, batch_i, optimizer_type, batch_i_x, batch_i_y, eta, l2_regularizer, momentum, beta, beta1, beta2, eps):\n",
    "        # Theory: https://ruder.io/optimizing-gradient-descent/\n",
    "        if optimizer_type == \"momentum\":\n",
    "            loss = self.momentum_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, momentum)\n",
    "        elif optimizer_type == \"nag\":\n",
    "            loss = self.nesterov_accelerated_gradient_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, momentum)\n",
    "        elif optimizer_type == \"adagrad\":\n",
    "            loss = self.adagrad_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, eps)\n",
    "        elif optimizer_type == \"adadelta\":\n",
    "            loss = self.adadelta_optimizer(batch_i_x, batch_i_y, l2_regularizer, beta, eps)\n",
    "        elif optimizer_type == \"rmsprop\":\n",
    "            loss = self.rmsprop_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, beta, eps)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            pass\n",
    "            #loss = self.adam_optimizer(epoch_i, batch_i, batch_i_x, batch_i_y, eta, l2_regularizer, beta, eps)\n",
    "        else: # Default: Gradient Descent\n",
    "            loss = self.gradient_optimizer(batch_i, batch_i_x, batch_i_y, eta, l2_regularizer)\n",
    "        return loss\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, batch_size=None, eta=0.01, l2_regularizer=0, momentum=0.9, beta=0.9, beta1=0.9, beta2=0.9, eps=1e-8):\n",
    "        #X_bak = X\n",
    "        #y_bak = y\n",
    "        # Feature scaling\n",
    "        X = self.standardizer(X)\n",
    "        \n",
    "        # Save target label values (unique)\n",
    "        self.y_unique_labels = np.unique(np.hstack(y))\n",
    "        self.y_unique_encoding = np.arange(len(self.y_unique_labels))\n",
    "        # One hot encode target (y)\n",
    "        y = self.one_hot_encode_labels(y)\n",
    "        \n",
    "        if not batch_size:\n",
    "            batch_size = len(X)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Get batch indices for creating training data\n",
    "        batch_indices = self.generate_batch_indices(len(X), batch_size)\n",
    "        x_train = X[batch_indices, :]\n",
    "        y_train = y[batch_indices, :]\n",
    "        \n",
    "        # Train\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (batch_i_x, batch_i_y) in enumerate(zip(x_train, y_train)): #zip(x_train[:1], y_train[:1]):\n",
    "#                 if batch_i == 0:\n",
    "#                     print(f\"Batch: {batch_i}\")\n",
    "                loss = self.optimize_batch(epoch, batch_i, self.optimizer, batch_i_x, batch_i_y, eta, l2_regularizer, momentum, beta, beta1, beta2, eps)\n",
    "                self.history.append(loss)\n",
    "#                 weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "#                 loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1])\n",
    "#                 w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[0].shape}\")\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[1].shape}\")\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[2].shape}\")\n",
    "#                 print(len(w_derivatives), f\"{w_derivatives[3].shape}\")\n",
    "#                 print()\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[0].shape}\")\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[1].shape}\")\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[2].shape}\")\n",
    "#                 print(len(b_derivatives), f\"{b_derivatives[3].shape}\")\n",
    "                \n",
    "#                 print(self.layer_weight_matrices[0])\n",
    "#                 print()\n",
    "#                 print(w_derivatives[0])\n",
    "                \n",
    "#                 print(self.layer_biases[0])\n",
    "#                 print()\n",
    "#                 print(b_derivatives[0])\n",
    "        #return X_bak[batch_indices[0], :], y_bak[batch_indices[0], :], batch_i_x, batch_i_y, batch_indices[0]\n",
    "        #return X, batch_indices[0]\n",
    "\n",
    "\n",
    "    '''\n",
    "    Prediction\n",
    "    '''\n",
    "\n",
    "    def predict(self, x): #, ii):\n",
    "        # Feature scaling\n",
    "        x = self.standardizer(x, isTrain=False)\n",
    "        # Perform Forward pass\n",
    "        _, _, _, activation_matrices = self.forward_propagation(x) #[ii, :]) #(x)\n",
    "        y_hat = activation_matrices[-1]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        if self.loss_function == \"binary_crossentropy\":\n",
    "            def binary_match(y_hat_i):\n",
    "                predictions.append(list(self.y_unique_labels[y_hat_i > 0.5])) # Threhold of 0.5, because we use sigmoid function in final layer\n",
    "        \n",
    "            np.apply_along_axis(binary_match, 1, y_hat)\n",
    "            \n",
    "        elif self.loss_function == \"categorical_crossentropy\":\n",
    "            def categorical_match(y_hat_i):\n",
    "                predictions.append(self.y_unique_labels[np.argmax(y_hat_i)]) \n",
    "                \n",
    "            np.apply_along_axis(categorical_match, 1, y_hat)\n",
    "        \n",
    "        return y_hat, predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_arch(sizes, activations):\n",
    "    fig_graph = plt.figure(figsize=(10, 10))\n",
    "    draw_network.draw(fig_graph.gca(), .1, .9, .1, .9, sizes, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc = MutliLayerPerceptronClassifer()\n",
    "mlpc.add_layer(units=20, activation=\"relu\", input_units=4)\n",
    "mlpc.add_layer(units=20, activation=\"relu\")\n",
    "mlpc.add_layer(units=30, activation=\"relu\")\n",
    "mlpc.add_layer(units=3, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc.compile_network(loss='categorical_crossentropy', optimizer=\"nag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], epochs=30, batch_size=10, momentum=0.9, l2_regularizer=0.8) # xx, yy, xi, yi, ii = #rdp, ii = \n",
    "#mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], epochs=200, batch_size=5, momentum=0.9, l2_regularizer=0.5)\n",
    "mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], epochs=20, batch_size=5, momentum=0.5, l2_regularizer=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2bf48570ba8>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlpc.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4, None),\n",
       " (1, 20, 'relu'),\n",
       " (2, 20, 'relu'),\n",
       " (3, 30, 'relu'),\n",
       " (4, 3, 'softmax')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.layers_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 20), (2, 20), (3, 30), (4, 3)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts = mlpc.network_structure\n",
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_prob, preds = mlpc.predict(clf_X_df.values) #, ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf_y_df.values == np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-0159691d5d4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mNone\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "None**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float64(1.0) / 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(np.float64(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghx = np.array([[1.83661532e+105, 5.45826738e+132, 1.04858320e+138, 1.57300189e+146,\n",
    "  1.35772872e+119, 1.56109596e+151, 2.81178673e+107, 1.70623671e+107,\n",
    "  2.28839398e-002, 2.94107717e+094, 1.17908247e+153, 4.09924097e+141,\n",
    "  1.00501527e+116, 1.35424213e+070, 2.67714496e+070, 7.91625977e+114,\n",
    "  8.23533630e+112, 2.67680822e+089, 1.14065651e+047, 1.49742073e+088],\n",
    " [2.67995372e+112, 5.33072708e+123, 8.46262814e+127, 1.89892514e+136,\n",
    "  9.48660481e+112, 1.65151397e+141, 3.35420574e+113, 2.89659044e+109,\n",
    "  3.08342455e-003, 6.10019732e+095, 1.06057283e+143, 2.54218596e+132,\n",
    "  1.46401028e+123, 9.45639763e+063, 1.96353106e+066, 7.27253700e+104,\n",
    "  8.98542653e+117, 5.55777979e+090, 8.55358237e+045, 2.43776378e+055],\n",
    " [5.70207526e+101, 2.87937985e+138, 4.41692659e+141, 9.83213685e+149,\n",
    "  1.68077660e+123, 7.22786114e+154, 7.14511779e+098, 3.72214753e+101,\n",
    "  1.05446573e-002, 1.73976540e+086, 5.58275062e+156, 2.42589539e+146,\n",
    "  3.09150907e+112, 1.67586228e+074, 4.18967211e+075, 3.61034625e+118,\n",
    "  9.33004998e+095, 1.58525608e+081, 4.09778195e+043, 7.55544351e+086],\n",
    " [5.91729989e+092, 6.21315727e+139, 9.69051631e+140, 3.21931940e+149,\n",
    "  4.63257116e+122, 1.68493419e+154, 3.35079156e+091, 5.95092023e+079,\n",
    "  3.93764104e-003, 8.88198472e+088, 1.39017005e+156, 6.66883680e+146,\n",
    "  2.40518215e+103, 4.61748822e+073, 8.88238450e+074, 8.37481014e+117,\n",
    "  5.13447801e+098, 8.08625670e+083, 6.18030356e+041, 7.12991874e+090]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeWarning",
     "evalue": "overflow encountered in square",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d2084afb4a1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mghx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in square"
     ]
    }
   ],
   "source": [
    "ghx**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.474334945943572e+292"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.57300189e+146**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "(34, 'Result too large')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-707eadc6c7d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;36m1.39017005e+154\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m: (34, 'Result too large')"
     ]
    }
   ],
   "source": [
    "1.39017005e+154**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeWarning",
     "evalue": "overflow encountered in square",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6736d7c2a2b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mghx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeWarning\u001b[0m: overflow encountered in square"
     ]
    }
   ],
   "source": [
    "ghx.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0, 0]\n",
    "b = [0, 0]\n",
    "a = b\n",
    "b = [np.array([1, 2]), np.array([3, 4])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg = np.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = np.zeros(hg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz / hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 / hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
