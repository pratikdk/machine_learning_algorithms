{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch [Multilayer Perceptron]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to plot neural network\n",
    "import matplotlib.pyplot as plt\n",
    "import draw_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data [Classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df = pd.read_csv(\"Iris.csv\")\n",
    "clf_df = clf_df.drop(\"Id\", axis=1)\n",
    "clf_df = clf_df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_X_df = clf_df.iloc[:, :-1]\n",
    "clf_y_df = clf_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_X_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Iris-setosa\n",
       "1    Iris-setosa\n",
       "2    Iris-setosa\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_y_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: \n",
    "\n",
    "Input: **(Done)**\n",
    "- batch (as seperate argument) full/minibatch **(Done)**\n",
    "- single/batch/minibatch generate indices (random) **(Done)**\n",
    "- sample using indices **(Done)**\n",
    "\n",
    "Output: **(Done)**\n",
    "- output labelencoder **(Done)**\n",
    "- save actual labels **(Done)**\n",
    "- one hot encoding of labels **(Done)**\n",
    "\n",
    "Weight Initializations:\n",
    "- Random **(Done)**\n",
    "- Zeros **(Done)**\n",
    "- Xavier **(Done)**\n",
    "- He **(Done)**\n",
    "\n",
    "Activation functions:\n",
    "- sigmoid **(Done)**\n",
    "- sigmoid derivative **(Done)**\n",
    "- relu **(Done)**\n",
    "- relu derivative **(Done)**\n",
    "- tanh **(Done)**\n",
    "- tanh derivative **(Done)**\n",
    "- softmax **(Done)**\n",
    "- softmax derivative **(Done)**\n",
    "\n",
    "Loss computation:\n",
    "- binary_crossentropy **(Done)**\n",
    "- binary_crossentropy derivative **(Done)**\n",
    "- categorical_crossentropy **(Done)**\n",
    "- categorical_crossentropy derivative **(Done)**\n",
    "\n",
    "Forward propagation:\n",
    "- iterative forward pass **(Done)**\n",
    "\n",
    "Back propagation:\n",
    "- chain builder - Parameter respective,for each training example, negative gradient **(Done)**\n",
    "\n",
    "Parameter update:\n",
    "- average derivative respective to each parameter for all training examples **(Done)**\n",
    "- update old weights by adding **(Done)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutliLayerPerceptronClassifer():\n",
    "    def __init__(self, random_state=False):\n",
    "        # RNG seed\n",
    "        if not random_state:\n",
    "            np.random.seed(20)\n",
    "        # network config vars\n",
    "        self.layers_config = []\n",
    "        self.network_structure = None\n",
    "        self.loss_function = None\n",
    "        self.optimizer = \"default\"\n",
    "        self.parameter_initializer = \"random\"\n",
    "        self.history = []\n",
    "        self.input_scaling_values = {}\n",
    "        self.batch_size = 0\n",
    "        \n",
    "        self.supported_network_settings = {\n",
    "            \"parameter_initializer\": [\"random\", \"zeros\", \"xavier\", \"he\"],\n",
    "            \"activation\": [\"sigmoid\", \"tanh\", \"relu\", \"softmax\", \"linear\"],\n",
    "            \"loss\": [\"binary_crossentropy\", \"categorical_crossentropy\"],\n",
    "            \"optimizer\": [\"default\",\n",
    "                         \"momentum\",\n",
    "                         \"nag\",\n",
    "                         \"adagrad\",\n",
    "                         \"adadelta\",\n",
    "                         \"rmsprop\",\n",
    "                         \"adam\",\n",
    "                         \"adamax\",\n",
    "                         \"nadam\",\n",
    "                         \"amsgrad\"]\n",
    "        }\n",
    "         \n",
    "        \n",
    "    '''\n",
    "    Setup training input and output(generate onehot for binary/multiclass/multilabel target)\n",
    "    '''\n",
    "    \n",
    "    def generate_batch_indices(self, input_examples_count, batch_size, final_batch_threshold=0.5, random=True):\n",
    "        row_indices = np.arange(start=0, stop=input_examples_count) # array([0, 1, ....., len(input_examples_count)-1 ])\n",
    "\n",
    "        if random:\n",
    "            np.random.shuffle(row_indices)\n",
    "\n",
    "        # Validate batch sizes\n",
    "        if batch_size: # 'Stochastic | mini-batch' batch sizes\n",
    "            if (batch_size < 1) | (batch_size > len(row_indices)): # Batch size should be between (0, len(row_indices)]\n",
    "                # raise an error \n",
    "                raise Exception(f\"Batch size violated the input size bounds: Batch size should be between (0, {len(data)}]\")\n",
    "        else:\n",
    "            batch_size = len(row_indices) # entire dataset as batch\n",
    "\n",
    "        # Validate if batch_size evenly splits data if not then mark batch_evenly_divides_input as false\n",
    "        batch_evenly_divides_input = False\n",
    "        if (len(row_indices) % batch_size) == 0:\n",
    "            batch_evenly_divides_input = True    \n",
    "        batches_count = int(len(row_indices)/batch_size) # Number of batches \n",
    "        batchable_rows_count = batches_count * batch_size # Number of rows which will be addressed by batches\n",
    "\n",
    "        # Store batch indices\n",
    "        batch_indices = np.stack(np.array_split(row_indices[:batchable_rows_count], batches_count), axis=0).tolist()\n",
    "        \n",
    "        # if some rows remain(batch_size didnt evenly divide len(row_indices)), make one final batch of input using\n",
    "        # remaining rows + randomly sampled rows from row_indices until size of new final batch equals batch_size\n",
    "        if not batch_evenly_divides_input:\n",
    "            remaining_row_indices = row_indices[batchable_rows_count:]\n",
    "            # If remining rows count are less than 50% (final_batch_threshold) of batch_size discard creation of final batch\n",
    "            if len(remaining_row_indices) > (batch_size * final_batch_threshold):\n",
    "                # compute how many more rows are required\n",
    "                required_row_indices_count = batch_size - len(remaining_row_indices)\n",
    "                # randomly sample about 'required_row_indices_count' rows from row_indices without replacement(to avoid selecting a row multiple times)\n",
    "                sampled_rows_indices = row_indices[np.random.choice(len(row_indices), required_row_indices_count, replace=False)]\n",
    "                # Stack remaining rows and sampled_rows within a single batch\n",
    "                final_batch = np.concatenate([remaining_row_indices, sampled_rows_indices]).tolist()\n",
    "                # Finally stack final batch to data_collated\n",
    "                batch_indices.append(final_batch)\n",
    "\n",
    "        return batch_indices\n",
    "    \n",
    "\n",
    "    def standardizer(self, x, isTrain=True): # Normalize with mean 0 and std-dev of 1, also called z-score\n",
    "        if isTrain:\n",
    "            x_mean = np.mean(x, axis=0)\n",
    "            x_stdev = np.std(x, axis=0)\n",
    "            # Save mean and standard deviation\n",
    "            self.input_scaling_values['standardizer_mean'] = x_mean\n",
    "            self.input_scaling_values['standardizer_stdev'] = x_stdev\n",
    "        stdz_x = np.divide((x - self.input_scaling_values['standardizer_mean']), self.input_scaling_values['standardizer_stdev'],\n",
    "                           where=self.input_scaling_values['standardizer_stdev']!=0)\n",
    "        return stdz_x\n",
    "    \n",
    "    \n",
    "    def one_hot_encode_labels(self, y):\n",
    "        \"\"\"\n",
    "        Creates one hot encodings for y w.r.t self.y_unique_labels (Supports Multiclass + Multilabel)\n",
    "        Eg: y = np.array([['b'], ['a', 'c', 'd'], ['a'], ['c', 'b']]) and self.y_unique_labels = np.array(['a', 'b', 'c', 'd'])\n",
    "            one_hot_encoded_y = np.array([[0, 1, 0, 0], [1, 0, 1, 1], [1, 0, 0, 0], [0, 1, 1, 0]])\n",
    "        \n",
    "        NOTE: Here I found np.vectorize() to run quicker than a custom matrix function, takes exactly 0.00007(secs) for an input of 6 rows,\n",
    "        Whereas a custom function requires knowing count of multilabels and does padding to create mask which alone takes 0.0008(secs on avg),\n",
    "        let alone additional steps like masking and one hot encoding will add up to time, So vectorize is best for this case\n",
    "        \"\"\"\n",
    "        def one_hot_vector(target):\n",
    "            return np.in1d(self.y_unique_labels, target, assume_unique=\"True\")\n",
    "        one_hot_vectorizer = np.vectorize(one_hot_vector, otypes=[np.ndarray])\n",
    "        one_hot_encoded_y = np.stack(np.hstack(one_hot_vectorizer(y))).astype(int)\n",
    "        return one_hot_encoded_y\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    Network configure\n",
    "    '''    \n",
    "     \n",
    "    def add_layer(self, units, activation=\"relu\", input_units=None):\n",
    "        # Validate activation value\n",
    "        self.validate_settings(setting_type=\"activation\", setting_value=activation)\n",
    "        \n",
    "        layers_count = len(self.layers_config)\n",
    "        # check if this is the first layer \n",
    "        if (layers_count == 0):\n",
    "            # Make sure input_dim is specified\n",
    "            if (input_units == None):\n",
    "                # We need input_dim as this is the first hidden layer\n",
    "                raise Exception(\"Please specify parameter value for 'input_units'(eg: input_units=4), as this is the first hidden layer\")\n",
    "            else:\n",
    "                # First create the input layer\n",
    "                self.layers_config.append((layers_count, input_units, None))\n",
    "                # Next add the hidden layer\n",
    "                self.layers_config.append((layers_count+1, units, activation))\n",
    "        else:  \n",
    "            self.layers_config.append((layers_count, units, activation))\n",
    "            \n",
    "            \n",
    "    '''\n",
    "    Network compiler & Parameter initializer\n",
    "    '''\n",
    "    \n",
    "    def validate_settings(self, setting_type, setting_value):\n",
    "        if setting_value not in self.supported_network_settings[setting_type]:\n",
    "            raise Exception(f\"{setting_type}='{setting_value}' is not supported, please specify {setting_type} as any one from the following list {self.supported_network_settings[setting_type]}\")            \n",
    "            \n",
    "    \n",
    "    def compile_network(self, loss, optimizer=None, param_initializer=None):\n",
    "        # Validate loss type value\n",
    "        self.validate_settings(setting_type=\"loss\", setting_value=loss)\n",
    "        # Validate optimizer type value\n",
    "        if optimizer is not None: # If None then use entire dataset as one epoch\n",
    "            self.validate_settings(setting_type=\"optimizer\", setting_value=optimizer)\n",
    "        # Validate initializer type value\n",
    "        if param_initializer is not None:\n",
    "            self.validate_settings(setting_type=\"parameter_initializer\", setting_value=param_initializer)\n",
    "            \n",
    "        # Setup layers\n",
    "        self.network_structure = list(map(tuple, np.array(self.layers_config)[:, :-1])) # Ignore the last columns as it specifies the type of activation function\n",
    "        # Save network settings\n",
    "        self.loss_function = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.parameter_initializer = param_initializer\n",
    "        # Weight and Bias matrix container\n",
    "        self.layer_weight_matrices = []\n",
    "        self.layer_biases = []\n",
    "        \n",
    "        # Define optimizer parameter containers, as required by different optimizers\n",
    "        # timestep: t\n",
    "        self.weights_update = []\n",
    "        self.biases_update = []\n",
    "        # Gradient\n",
    "        self.weight_grads = []\n",
    "        self.bias_grads = []\n",
    "        # Gradient squared\n",
    "        self.weight_grads_squared = []\n",
    "        self.bias_grads_squared = []\n",
    "        # Start by saving t-1 and t-2 initial updates\n",
    "        self.all_weight_updates = [[], []]\n",
    "        self.all_bias_updates = [[], []]\n",
    "        # Known Max/min/etc of second moment of gradient v_t\n",
    "        self.criterion_weight_v_t = []\n",
    "        self.criterion_bias_v_t = []\n",
    "        \n",
    "        for layers_number in range(1, len(self.network_structure)):\n",
    "            layer_a_units = self.network_structure[layers_number-1][1]\n",
    "            layer_b_units = self.network_structure[layers_number][1]\n",
    "            weight_matrix, bias_matrix = self.generate_intial_parameters(self.parameter_initializer, layer_a_units, layer_b_units)\n",
    "            # append matrix to list layer_weight_matrices and layer_biases lists\n",
    "            self.layer_weight_matrices.append(weight_matrix)\n",
    "            self.layer_biases.append(bias_matrix)\n",
    "            \n",
    "            # Optimizer essentials: initialize containers\n",
    "            # Initialize parameter update\n",
    "            self.weights_update.append(0)\n",
    "            self.biases_update.append(0)\n",
    "            # Intialize gradient containers\n",
    "            self.weight_grads.append(0)\n",
    "            self.bias_grads.append(0)\n",
    "            # Intialize gradient squared containers\n",
    "            self.weight_grads_squared.append(0)\n",
    "            self.bias_grads_squared.append(0)\n",
    "            # Intiliaze all_weight_updates with initial t-1 and t-2\n",
    "            self.all_weight_updates[-1].append(0) # t-1\n",
    "            self.all_weight_updates[-2].append(0) # t-2\n",
    "            self.all_bias_updates[-1].append(0) # t-1\n",
    "            self.all_bias_updates[-2].append(0) # t-2\n",
    "            # Initializer criteria based last known second moment of gradient v_t\n",
    "            self.criterion_weight_v_t.append(0)\n",
    "            self.criterion_bias_v_t.append(0)\n",
    "    '''\n",
    "    Activation Functions\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_activation(self, z):\n",
    "        # Plain and simple sigmoid function, function returns prob for z(z is weighted sum of input)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def tanh_activation(self, z):\n",
    "        return np.tanh(z) #  (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    def relu_activation(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    \n",
    "    def linear_activation(self, z):\n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def softmax_activation(self, z_vector):\n",
    "        # Normalizing the inputs to be not too large or too small, to address prob of overshooting when performing exponentiations of large numbers(violates floating point limit)\n",
    "        # Shift the inputs to a range close to zero and less than zero\n",
    "        # large negative exponents \"saturate\" to zero rather than infinity, so we have a better chance of avoiding NaNs.\n",
    "        shiftz = z_vector - np.max(z_vector)\n",
    "        exps = np.exp(shiftz)\n",
    "        return exps / np.sum(exps) \n",
    "    \n",
    "    '''\n",
    "    Derivatives of Activation Functions\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_derivative(self, sig_z): # sig_z = sig(w.a + b) (vectors w,a represent weight,activations)\n",
    "        # Differentiate sigmoid function w.r.t to z to obtain sig_z * (1 - sig_z)\n",
    "        return sig_z * (1.0 - sig_z)\n",
    "    \n",
    "    \n",
    "    def tanh_derivative(self, tanh_z): \n",
    "        return 1.0 - tanh_z**2\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, rel_z):\n",
    "        return 1.0 * (rel_z > 0)\n",
    "    \n",
    "    \n",
    "    def linear_derivative(self, lin_z):\n",
    "        return 1.0\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Loss functions\n",
    "    '''\n",
    "    \n",
    "    def binary_crossentropy(self, y, y_hat):\n",
    "        return np.sum(y*np.log(y_hat + 1e-8) + (1-y)*np.log(1 - y_hat + 1e-8)) * -(1.0/len(y))\n",
    "    \n",
    "    def categorical_crossentropy(self, y, y_hat):\n",
    "        return -np.mean(np.sum(y*np.log(y_hat + 1e-8), axis=1))\n",
    "    \n",
    "    '''\n",
    "    Derivatives of Loss functions(cross entorpy) w.r.t to logits (dJ/dZ)\n",
    "    '''\n",
    "    \n",
    "    def binary_crossentropy_derivative(self, activation_matrix, y):\n",
    "        return activation_matrix - y\n",
    "    \n",
    "    \n",
    "    def categorical_crossentropy_derivative(self, activation_matrix, y):\n",
    "        return activation_matrix - y\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Function Invokers\n",
    "    '''\n",
    "    \n",
    "    def generate_intial_parameters(self, param_initializer, layer_a_units, layer_b_units):\n",
    "        if param_initializer == \"zeros\":\n",
    "            weight_matrix = np.zeros((layer_a_units, layer_b_units))\n",
    "            bias_matrix = np.zeros((1, layer_b_units))\n",
    "        elif param_initializer == \"xavier\": # Adjust the variance down by multiplying with (1/sqrt(number units in previous layer)) # Sampled from normal distribution\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units) * np.sqrt(1/layer_a_units)\n",
    "            bias_matrix =np.random.randn(1, layer_b_units)\n",
    "        elif param_initializer == \"he\": # Adjust the variance down by multiplying with (2/sqrt(number units in previous layer)) # Sampled from normal distribution\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units) * np.sqrt(2/layer_a_units)\n",
    "            bias_matrix =np.random.randn(1, layer_b_units)\n",
    "        else: # random\n",
    "            weight_matrix = np.random.randn(layer_a_units, layer_b_units)\n",
    "            bias_matrix = np.random.randn(1, layer_b_units)\n",
    "        \n",
    "        weight_matrix = weight_matrix.astype('float64')\n",
    "        bias_matrix = bias_matrix.astype('float64')\n",
    "        return weight_matrix, bias_matrix.squeeze()\n",
    "    \n",
    "    \n",
    "    def get_activations(self, activation_type, z_matrix):\n",
    "        if activation_type == \"sigmoid\":\n",
    "            activation_matrix = self.sigmoid_activation(z_matrix)\n",
    "        elif activation_type == \"tanh\":\n",
    "            activation_matrix = self.tanh_activation(z_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            activation_matrix = self.relu_activation(z_matrix)\n",
    "        elif activation_type == \"softmax\":\n",
    "            # For softmax, apply the function over each row rather than element-wise,\n",
    "            # since softmax considers the entire layer to generate a Probability distribution\n",
    "            #print(z_matrix)\n",
    "            activation_matrix = np.apply_along_axis(self.softmax_activation, 1, z_matrix)\n",
    "        else: # Linear\n",
    "            activation_matrix = self.linear_activation(z_matrix)\n",
    "            \n",
    "        return activation_matrix\n",
    "    \n",
    "    \n",
    "    def get_loss(self, loss_type, y, y_hat, weights, l2_regularizer):\n",
    "        if loss_type == \"binary_crossentropy\":\n",
    "            J = self.binary_crossentropy(y, y_hat)\n",
    "        elif loss_type == \"categorical_crossentropy\":\n",
    "            J = self.categorical_crossentropy(y, y_hat)\n",
    "            \n",
    "        # L2 Regularization\n",
    "        sum_of_squared_weights = 0\n",
    "        for weight_matrix in weights: \n",
    "            sum_of_squared_weights += np.sum(weight_matrix.flatten()**2)\n",
    "        loss_l2_regularization_component = (l2_regularizer/(2*self.batch_size)) * sum_of_squared_weights\n",
    "            \n",
    "        J += loss_l2_regularization_component\n",
    "        \n",
    "        return J\n",
    "        \n",
    "    \n",
    "    def get_activation_derivative(self, activation_type, activation_matrix):\n",
    "        # Here Activation is differentiated w.r.t to z (ie: (dA/dZ))\n",
    "        if activation_type == \"sigmoid\":\n",
    "            dA_dZ = self.sigmoid_derivative(activation_matrix)\n",
    "        elif activation_type == \"tanh\":\n",
    "            dA_dZ = self.tanh_derivative(activation_matrix)\n",
    "        elif activation_type == \"relu\":\n",
    "            dA_dZ = self.relu_derivative(activation_matrix)\n",
    "        else: # Linear\n",
    "            dA_dZ = self.linear_derivative(activation_matrix)\n",
    "            \n",
    "        return dA_dZ\n",
    "    \n",
    "    \n",
    "    def get_loss_derivative(self, loss_type, activation_matrix, y):\n",
    "        # Here loss is differentiated w.r.t preactivations(z) (ie: (dJ/dZ))\n",
    "        if loss_type == \"binary_crossentropy\":\n",
    "            dZ = self.binary_crossentropy_derivative(activation_matrix, y)\n",
    "        elif loss_type == \"categorical_crossentropy\":\n",
    "            dZ = self.categorical_crossentropy_derivative(activation_matrix, y)\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Forward Propagation: Forward pass\n",
    "    '''\n",
    "    \n",
    "    def forward_propagation(self, X_batch, weights=None, biases=None):\n",
    "        if (weights is None) & (biases is None):\n",
    "            weights = self.layer_weight_matrices\n",
    "            biases = self.layer_biases\n",
    "        \n",
    "        # Define containers to store activation and z matrices, for use while doing backprop\n",
    "        z_matrices = []\n",
    "        activation_matrices = []\n",
    "        # Append input (matrix; since we have batch) to activation_matrices\n",
    "        activation_matrices.append(X_batch)\n",
    "        # Next iterate over each layer\n",
    "        for layer, weight_matix, bias_vector in zip(self.layers_config[1:], weights, biases): # Ignoring the 0th layer in layers_config since it is an input layer\n",
    "            layer_number = layer[0] # Get layer number\n",
    "            layer_activation_type = layer[-1] # Get activation type for this layer\n",
    "            z_matrix = np.dot(activation_matrices[-1], weight_matix) + bias_vector\n",
    "            activation_matrix = self.get_activations(layer_activation_type, z_matrix) # activation(W.x + b)\n",
    "            # Store activation and z matrix\n",
    "            z_matrices.append(z_matrix)\n",
    "            activation_matrices.append(activation_matrix)\n",
    "        #yhat_matrix = activation_matrices[-1] # Final layer output matrix # Predicted(one hot vector)\n",
    "        #print(activation_matrices)\n",
    "        return weights, biases, z_matrices, activation_matrices\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Backward Propagation: Gradient Computation\n",
    "    '''   \n",
    "    \n",
    "    def backprop_gradient(self, y, weights, biases, z_matrices, activation_matrices):\n",
    "        w_derivatives = []\n",
    "        b_derivatives = []\n",
    "        \n",
    "        # Know the batch size; since we will be averaging our gradients\n",
    "        batch_size = len(activation_matrices[0])\n",
    "        \n",
    "        # Iterate over each layer from back, don't consider input layer\n",
    "        for layer_i in range(len(self.layers_config)-1, 0, -1):\n",
    "            layer_activation_type = self.layers_config[layer_i][-1]\n",
    "            \n",
    "            if (layer_i+1) == len(self.layers_config): # final layer\n",
    "                dZ = self.get_loss_derivative(loss_type=self.loss_function,\n",
    "                                              activation_matrix=activation_matrices[layer_i],\n",
    "                                              y=y)\n",
    "            else: # rest of the layers [note: input layer isn't considered in the loop]\n",
    "                dZ = dA_previous * self.get_activation_derivative(activation_type=layer_activation_type,\n",
    "                                                   activation_matrix=activation_matrices[layer_i])\n",
    "                \n",
    "            dW = np.dot(dZ.T, activation_matrices[layer_i - 1]) / batch_size\n",
    "            db = np.sum(dZ.T, axis=1) / batch_size\n",
    "            dA_previous = np.dot(weights[layer_i-1], dZ.T).T # We subtract 1 from layer_i, because current layer weights are stored on previous index\n",
    "        \n",
    "            # Save dW and db\n",
    "            w_derivatives.append(dW.T)\n",
    "            b_derivatives.append(db)\n",
    "        # Reverse w_derivatives and b_derivatives lists, since iterating backwards added update parameter matrix in backwards order of the network.\n",
    "        # We reverse here simply because it becomes easy to update the existing weights\n",
    "        w_derivatives.reverse()\n",
    "        b_derivatives.reverse()\n",
    "        return w_derivatives, b_derivatives\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Backward Propagation: Optimization and Parameter updates\n",
    "    \n",
    "    My optimizer implementaions are based on theory provided by:\n",
    "    https://ruder.io/optimizing-gradient-descent/\n",
    "    ''' \n",
    "#     \"gradient\",\n",
    "#     \"momentum\",\n",
    "#      \"nag\",\n",
    "#      \"adagrad\",\n",
    "#      \"adadelta\",\n",
    "#      \"rmsprop\",\n",
    "#      \"adam\",\n",
    "#      \"adamax\",\n",
    "#      \"nadam\",\n",
    "#      \"amsgrad\"\n",
    "    def gradient_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer):\n",
    "        # Perform forward pass and compute change in direction of gradient w.r.t each parameter\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        # Compute loss\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        # Backward pass to perform gradient computation w.r.t each parameter\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        # Derivated regularization component\n",
    "        derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "        \n",
    "        #Update parameters of each layer\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            self.layer_weight_matrices[i] = derivated_l2_regularization_component * self.layer_weight_matrices[i] - (eta * w_derivatives[i])\n",
    "            self.layer_biases[i] = self.layer_biases[i] - (eta * b_derivatives[i])\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def momentum_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, momentum):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            # Define momentum based update\n",
    "            self.weights_update[i] = (momentum * self.weights_update[i]) + (eta * w_derivatives[i]) \n",
    "            self.biases_update[i] = (momentum * self.biases_update[i]) + (eta * b_derivatives[i])\n",
    "            # Update parameters using momentum based update\n",
    "            self.layer_weight_matrices[i] = derivated_l2_regularization_component * self.layer_weight_matrices[i] - self.weights_update[i]\n",
    "            self.layer_biases[i] = self.layer_biases[i] - self.biases_update[i]\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def nesterov_accelerated_gradient_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, momentum):\n",
    "        derivated_l2_regularization_component = 1 - (eta*l2_regularizer)/len(batch_i_x)\n",
    "\n",
    "        # First perform temporary update of current parameters in the direction of previous parameter update (Equivalent to first comitting the mistake)\n",
    "        temporary_layer_weights = []\n",
    "        temporary_layer_biases = []\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            weight_momentum_term = momentum * self.all_weight_updates[-1][i]\n",
    "            bias_momentum_term = momentum * self.all_bias_updates[-1][i]\n",
    "            temporary_layer_weights.append(derivated_l2_regularization_component * self.layer_weight_matrices[i] - weight_momentum_term)\n",
    "            temporary_layer_biases.append(self.layer_biases[i] - bias_momentum_term)\n",
    "            \n",
    "        # Compute the gradient of updated current parameters,(we obtain a much informed gradient compared to Momentum's(non NAG) gradient, since we are precomputing it from a future position)\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x, temporary_layer_weights, temporary_layer_biases)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        # Define containers to store parameters\n",
    "        t_weight_container = []\n",
    "        t_bias_container = []\n",
    "        # Finally update current parameters in the direction of previous parameter update, while using the computed gradient which acts as a correction to the mistake(to update in correct direction)\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            t_weight_container.append((momentum * self.all_weight_updates[-1][i]) + (eta * w_derivatives[i]))\n",
    "            t_bias_container.append((momentum * self.all_bias_updates[-1][i]) + (eta * b_derivatives[i]))\n",
    "            self.layer_weight_matrices[i] = self.layer_weight_matrices[i] - t_weight_container[i]\n",
    "            self.layer_biases[i] = self.layer_biases[i] - t_bias_container[i]\n",
    "        \n",
    "        self.all_weight_updates.append(t_weight_container)\n",
    "        self.all_bias_updates.append(t_bias_container)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def adagrad_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            # Sum of past squared gradients upto current timestep(epoch*batch) t\n",
    "            self.weights_update[i] = self.weights_update[i] + w_derivatives[i]**2\n",
    "            self.biases_update[i] = self.biases_update[i] + b_derivatives[i]**2\n",
    "            self.layer_weight_matrices[i] -= ((eta/(np.sqrt(self.weights_update[i])+eps)) * w_derivatives[i])\n",
    "            self.layer_biases[i] -= ((eta/(np.sqrt(self.biases_update[i])+eps)) * b_derivatives[i])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def adadelta_optimizer(self, batch_i_x, batch_i_y, l2_regularizer, beta, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        t_weight_container = []\n",
    "        t_bias_container = []\n",
    "\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            running_weight_avg = (beta*self.all_weight_updates[-2][i]**2) + ((1-beta)*(self.all_weight_updates[-1][i]**2))\n",
    "            running_bias_avg = (beta*self.all_bias_updates[-2][i]**2) + ((1-beta)*(self.all_bias_updates[-1][i]**2))\n",
    "\n",
    "            self.weight_grads_squared[i] = (beta*self.weight_grads_squared[i]) + ((1-beta)*w_derivatives[i]**2)\n",
    "            self.bias_grads_squared[i] = (beta*self.bias_grads_squared[i]) + ((1-beta)*b_derivatives[i]**2)\n",
    "            \n",
    "            t_weight_container.append((((np.sqrt(running_weight_avg+eps)) / (np.sqrt(self.weight_grads_squared[i]+eps))) * w_derivatives[i]))\n",
    "            t_bias_container.append((((np.sqrt(running_bias_avg+eps)) / (np.sqrt(self.bias_grads_squared[i]+eps))) * b_derivatives[i]))\n",
    "            \n",
    "            # Perform update\n",
    "            self.layer_weight_matrices[i] = self.layer_weight_matrices[i] - t_weight_container[i]\n",
    "            self.layer_biases[i] = self.layer_biases[i] - t_bias_container[i]\n",
    "            \n",
    "        self.all_weight_updates.append(t_weight_container)\n",
    "        self.all_bias_updates.append(t_bias_container)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def rmsprop_optimizer(self, batch_i_x, batch_i_y, eta, l2_regularizer, beta, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            # Compute exponentially decaying average of squared gradients (Running Average)\n",
    "            self.weights_update[i] = (beta*self.weights_update[i]) + ((1-beta)*w_derivatives[i]**2)\n",
    "            self.biases_update[i] = (beta*self.biases_update[i]) + ((1-beta)*b_derivatives[i]**2)\n",
    "            self.layer_weight_matrices[i] -= ((eta/(np.sqrt(self.weights_update[i]+eps))) * w_derivatives[i])\n",
    "            self.layer_biases[i] -= ((eta/(np.sqrt(self.biases_update[i]+eps))) * b_derivatives[i])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def adam_optimizer(self, t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            # Exponentially decaying average of past gradients i.e: m_t or first moment (the mean) of gradient\n",
    "            self.weight_grads[i] = (beta1*self.weight_grads[i]) + ((1-beta1)*w_derivatives[i])\n",
    "            self.bias_grads[i] = (beta1*self.bias_grads[i]) + ((1-beta1)*b_derivatives[i])\n",
    "            \n",
    "            # Exponentially decaying average of past gradient squared i.e: v_t or second moment (the uncentered variance) of gradient\n",
    "            self.weight_grads_squared[i] = (beta2*self.weight_grads_squared[i]) + ((1-beta2)*w_derivatives[i]**2)\n",
    "            self.bias_grads_squared[i] = (beta2*self.bias_grads_squared[i]) + ((1-beta2)*b_derivatives[i]**2)\n",
    "            \n",
    "            # Fix: Bias-corrected first and second moment estimates\n",
    "            weight_m_t_hat = self.weight_grads[i] / (1-np.power(beta1, t_timestep))\n",
    "            bias_m_t_hat = self.bias_grads[i] / (1-np.power(beta1, t_timestep))\n",
    "            \n",
    "            weight_v_t_hat = self.weight_grads_squared[i] / (1-np.power(beta2, t_timestep))\n",
    "            bias_v_t_hat = self.bias_grads_squared[i] / (1-np.power(beta2, t_timestep))\n",
    "            \n",
    "            # Update weight and bias parameters\n",
    "            self.layer_weight_matrices[i] -= (eta/(np.sqrt(weight_v_t_hat+eps))) * weight_m_t_hat\n",
    "            self.layer_biases[i] -= (eta/(np.sqrt(bias_v_t_hat+eps))) * bias_m_t_hat\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def adamax_optimizer(self, t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "\n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            # Exponentially decaying average of past gradients i.e: m_t or first moment (the mean) of gradient\n",
    "            self.weight_grads[i] = (beta1*self.weight_grads[i]) + ((1-beta1)*w_derivatives[i])\n",
    "            self.bias_grads[i] = (beta1*self.bias_grads[i]) + ((1-beta1)*b_derivatives[i])\n",
    "            \n",
    "            # Fix: Bias-corrected first and second moment estimates\n",
    "            weight_m_t_hat = self.weight_grads[i] / (1-np.power(beta1, t_timestep))\n",
    "            bias_m_t_hat = self.bias_grads[i] / (1-np.power(beta1, t_timestep))\n",
    "            \n",
    "            # Not really grads_squared but a max operation\n",
    "            self.weight_grads_squared[i] = np.maximum(beta2 * self.weight_grads_squared[i], np.abs(w_derivatives[i]))\n",
    "            self.bias_grads_squared[i] = np.maximum(beta2 * self.bias_grads_squared[i], np.abs(b_derivatives[i]))\n",
    "            \n",
    "            # Update weight and bias parameters\n",
    "            self.layer_weight_matrices[i] -= (eta/(self.weight_grads_squared[i]+eps)) * weight_m_t_hat\n",
    "            self.layer_biases[i] -= (eta/(self.bias_grads_squared[i]+eps)) * bias_m_t_hat\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        \n",
    "    def nadam_optimizer(self, t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            # Exponentially decaying average of past gradients i.e: m_t or first moment (the mean) of gradient\n",
    "            self.weight_grads[i] = (beta1*self.weight_grads[i]) + ((1-beta1)*w_derivatives[i])\n",
    "            self.bias_grads[i] = (beta1*self.bias_grads[i]) + ((1-beta1)*b_derivatives[i])\n",
    "            \n",
    "            # Exponentially decaying average of past gradient squared i.e: v_t or second moment (the uncentered variance) of gradient\n",
    "            self.weight_grads_squared[i] = (beta2*self.weight_grads_squared[i]) + ((1-beta2)*w_derivatives[i]**2)\n",
    "            self.bias_grads_squared[i] = (beta2*self.bias_grads_squared[i]) + ((1-beta2)*b_derivatives[i]**2)\n",
    "            \n",
    "            # Fix: Bias-corrected first and second moment estimates\n",
    "            weight_m_t_hat = self.weight_grads[i] / (1-np.power(beta1, t_timestep))\n",
    "            bias_m_t_hat = self.bias_grads[i] / (1-np.power(beta1, t_timestep))\n",
    "            \n",
    "            weight_v_t_hat = self.weight_grads_squared[i] / (1-np.power(beta2, t_timestep))\n",
    "            bias_v_t_hat = self.bias_grads_squared[i] / (1-np.power(beta2, t_timestep))\n",
    "            \n",
    "            # Update weight and bias parameters\n",
    "            self.layer_weight_matrices[i] -= (eta/np.sqrt(self.weight_grads_squared[i]+eps)) * ( (beta1*weight_m_t_hat) + (((1-beta1)*w_derivatives[i])/(1-np.power(beta1, t_timestep))) )\n",
    "            self.layer_biases[i] -= (eta/(self.bias_grads_squared[i]+eps)) * ( (beta1*bias_m_t_hat) + (((1-beta1)*b_derivatives[i])/(1-np.power(beta1, t_timestep))) )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def amsgrad_optimizer(self, t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps):\n",
    "        weights, biases, z_matrices, activation_matrices = self.forward_propagation(batch_i_x)\n",
    "        loss = self.get_loss(self.loss_function, batch_i_y, activation_matrices[-1], weights, l2_regularizer)\n",
    "        w_derivatives, b_derivatives = self.backprop_gradient(batch_i_y, weights, biases, z_matrices, activation_matrices)\n",
    "        \n",
    "        for i in range(len(self.layers_config)-1):\n",
    "            w_derivatives[i] += (l2_regularizer*self.layer_weight_matrices[i])/len(batch_i_x)\n",
    "            \n",
    "            # Exponentially decaying average of past gradients i.e: m_t or first moment (the mean) of gradient\n",
    "            self.weight_grads[i] = (beta1*self.weight_grads[i]) + ((1-beta1)*w_derivatives[i])\n",
    "            self.bias_grads[i] = (beta1*self.bias_grads[i]) + ((1-beta1)*b_derivatives[i])\n",
    "            \n",
    "            # Exponentially decaying average of past gradient squared i.e: v_t or second moment (the uncentered variance) of gradient\n",
    "            self.weight_grads_squared[i] = (beta2*self.weight_grads_squared[i]) + ((1-beta2)*w_derivatives[i]**2)\n",
    "            self.bias_grads_squared[i] = (beta2*self.bias_grads_squared[i]) + ((1-beta2)*b_derivatives[i]**2)\n",
    "            \n",
    "            # Consider only maximum of gradients, to remove dependency on short-term memory of the past gradients\n",
    "            self.criterion_weight_v_t[i] = np.maximum(self.criterion_weight_v_t[i], self.weight_grads_squared[i])\n",
    "            self.criterion_bias_v_t[i] = np.maximum(self.criterion_bias_v_t[i], self.bias_grads_squared[i])\n",
    "            \n",
    "            # Update weight and bias parameters\n",
    "            self.layer_weight_matrices[i] -= (eta/np.sqrt(self.criterion_weight_v_t[i]+eps)) * self.weight_grads[i]\n",
    "            self.layer_biases[i] -= (eta/np.sqrt(self.criterion_bias_v_t[i]+eps)) * self.bias_grads[i]\n",
    "            \n",
    "        return loss\n",
    "            \n",
    "    \n",
    "\n",
    "    def optimize_batch(self, t_timestep, optimizer_type, batch_i_x, batch_i_y, eta, l2_regularizer, momentum, beta, beta1, beta2, eps):\n",
    "        if optimizer_type == \"momentum\":\n",
    "            loss = self.momentum_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, momentum)\n",
    "        elif optimizer_type == \"nag\":\n",
    "            loss = self.nesterov_accelerated_gradient_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, momentum)\n",
    "        elif optimizer_type == \"adagrad\":\n",
    "            loss = self.adagrad_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, eps)\n",
    "        elif optimizer_type == \"adadelta\":\n",
    "            loss = self.adadelta_optimizer(batch_i_x, batch_i_y, l2_regularizer, beta, eps)\n",
    "        elif optimizer_type == \"rmsprop\":\n",
    "            loss = self.rmsprop_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer, beta, eps)\n",
    "        elif optimizer_type == \"adam\":\n",
    "            loss = self.adam_optimizer(t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps)\n",
    "        elif optimizer_type == \"adamax\":\n",
    "            loss = self.adamax_optimizer(t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps)\n",
    "        elif optimizer_type == \"nadam\":\n",
    "            loss = self.nadam_optimizer(t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps)\n",
    "        elif optimizer_type == \"amsgrad\":\n",
    "            loss = self.amsgrad_optimizer(t_timestep, batch_i_x, batch_i_y, eta, l2_regularizer, beta1, beta2, eps)\n",
    "        else: # Default: Gradient Descent\n",
    "            loss = self.gradient_optimizer(batch_i_x, batch_i_y, eta, l2_regularizer)\n",
    "        return loss\n",
    "            \n",
    "    \n",
    "    '''\n",
    "    Training\n",
    "    '''\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, batch_size=None, eta=0.01, l2_regularizer=0, momentum=0.9, beta=0.9, beta1=0.9, beta2=0.9, eps=1e-8):\n",
    "        # Feature scaling\n",
    "        X = self.standardizer(X)\n",
    "        \n",
    "        # Save target label values (unique)\n",
    "        self.y_unique_labels = np.unique(np.hstack(y))\n",
    "        self.y_unique_encoding = np.arange(len(self.y_unique_labels))\n",
    "        # One hot encode target (y)\n",
    "        y = self.one_hot_encode_labels(y)\n",
    "        \n",
    "        if not batch_size:\n",
    "            batch_size = len(X)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Get batch indices for creating training data\n",
    "        batch_indices = self.generate_batch_indices(len(X), batch_size)\n",
    "        x_train = X[batch_indices, :]\n",
    "        y_train = y[batch_indices, :]\n",
    "        \n",
    "        t_timestep = 0\n",
    "        # Train\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (batch_i_x, batch_i_y) in enumerate(zip(x_train, y_train)):\n",
    "                t_timestep += 1\n",
    "                loss = self.optimize_batch(t_timestep, self.optimizer, batch_i_x, batch_i_y, eta, l2_regularizer, momentum, beta, beta1, beta2, eps)\n",
    "                self.history.append(loss)\n",
    "\n",
    "\n",
    "    '''\n",
    "    Prediction\n",
    "    '''\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Feature scaling\n",
    "        x = self.standardizer(x, isTrain=False)\n",
    "        # Perform Forward pass\n",
    "        _, _, _, activation_matrices = self.forward_propagation(x)\n",
    "        y_hat = activation_matrices[-1]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        if self.loss_function == \"binary_crossentropy\":\n",
    "            def binary_match(y_hat_i):\n",
    "                predictions.append(list(self.y_unique_labels[y_hat_i > 0.5])) # Threhold of 0.5, because we use sigmoid function in final layer\n",
    "        \n",
    "            np.apply_along_axis(binary_match, 1, y_hat)\n",
    "            \n",
    "        elif self.loss_function == \"categorical_crossentropy\":\n",
    "            def categorical_match(y_hat_i):\n",
    "                predictions.append(self.y_unique_labels[np.argmax(y_hat_i)]) \n",
    "                \n",
    "            np.apply_along_axis(categorical_match, 1, y_hat)\n",
    "        \n",
    "        return y_hat, predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_arch(sizes, activations):\n",
    "    fig_graph = plt.figure(figsize=(10, 10))\n",
    "    draw_network.draw(fig_graph.gca(), .1, .9, .1, .9, sizes, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc = MutliLayerPerceptronClassifer()\n",
    "mlpc.add_layer(units=20, activation=\"relu\", input_units=4)\n",
    "mlpc.add_layer(units=20, activation=\"relu\")\n",
    "mlpc.add_layer(units=30, activation=\"relu\")\n",
    "mlpc.add_layer(units=3, activation=\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc.compile_network(loss='categorical_crossentropy', optimizer=\"amsgrad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], epochs=30, batch_size=10, momentum=0.9, l2_regularizer=0.8) # xx, yy, xi, yi, ii = #rdp, ii = \n",
    "#mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], epochs=200, batch_size=5, momentum=0.9, l2_regularizer=0.5)\n",
    "mlpc.fit(clf_X_df.values, clf_y_df.values[:, np.newaxis], eta=0.001, epochs=80, batch_size=5, momentum=0.5, l2_regularizer=0, beta1=0.9, beta2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20b8abefa90>]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlpc.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4, None),\n",
       " (1, 20, 'relu'),\n",
       " (2, 20, 'relu'),\n",
       " (3, 30, 'relu'),\n",
       " (4, 3, 'softmax')]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.layers_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 20), (2, 20), (3, 30), (4, 3)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wts = mlpc.network_structure\n",
    "wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "sof_prob, preds = mlpc.predict(clf_X_df.values) #, ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clf_y_df.values == np.array(preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
